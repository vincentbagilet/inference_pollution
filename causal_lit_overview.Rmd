---
title: "Retrospective Power Analysis of the Causal Inference Literature"
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache.path = "cache/",
  cache = FALSE,
  fig.path = "images/causal_lit_overview/",
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  out.width = "85%",
  dpi = 300,
  fig.align = "center"
)
```

In this document, we carry out a retrospective power analysis of the causal inference literature on the acute health effects of air pollution. 


We used an extensive search strategy on [Google Scholar](https://scholar.google.com/), [PubMed](https://pubmed.ncbi.nlm.nih.gov/), and [IDEAS](https://ideas.repec.org/) to retrieve studies that (i) focus on the short-health effects of air pollution on mortality and morbidity outcomes, and (ii) rely on a causal inference method. We exclude the very recent literature on the effects of air pollution on COVID-19 health outcomes. We found a corpus of 36 relevant articles. For each study, we retrieved the method used by the authors, which health outcome and air pollutant they consider, the point estimate and the standard error of results. We coded all main results but also those on heterogeneous effects by age categories and control outcomes ("placebo tests").

Our document is organized as follows:

* In the first section, we describe the coded variables and explain how we standardize effect sizes.

* In the second section, we provide two pieces of evidence for publication bias in the literature.

* In the third section, we compute the statistical power, the type M error and the probability to make a type S error for each paper using different guesses of true effect sizes. For this task, we rely on the very convenient [retrodesign](https://cran.r-project.org/web/packages/retrodesign/vignettes/Intro_To_retrodesign.html) package.

# Set-Up

### Packages and Data

We load the packages:

```{r load_packages, echo=TRUE}
library("groundhog")
packages <- c(
  "here",
  "tidyverse", 
  "knitr",
  "retrodesign", 
  "Cairo",
  "kableExtra",
  "patchwork",
  "vincentbagilet/mediocrethemes"
)

groundhog.library(packages, "2022-11-28")

set_mediocre_all(
  base_size = if (.Platform$OS.type == "windows") 22 else 12, 
  pal = "leo"
)
```

We load the literature review data:

```{r load_data, echo=TRUE}
# load literature review data
data_literature <-
  readRDS(here::here(
    "data",
    "literature_review_causal_inference",
    "data_literature_review.rds"
  ))
```

### Description of Coded Variables

We retrieved data `r nrow(data_literature)` estimates from `r data_literature %>% select(paper_label) %>% distinct() %>% nrow()` articles. For each paper, we coded 25 variables:

* `paper_label`: the first author and publication date of the article. This is the identifier of a study.
* `paper_estimate_id`: this is the unique identifier of a result. The median number of results reported by studies is `r median(data_literature %>% group_by(paper_label) %>% summarise(n = n()) %>% pull(n))`.
* `url`: the internet link of the publication.
* `publication_year`: year of publication. Papers we consider were published between `r min(data_literature$publication_year)` and `r max(data_literature$publication_year)`.
* `field`: whether the variable was published in economics or in epidemiology. `r data_literature %>% select(paper_label, field) %>% distinct() %>% summarize(round(mean(field=="economics")*100,0))` of studies are published in economics journals, the rest being published in epidemiology/public health journals.
* `context`: the location of the study.
* `empirical_strategy`: the empirical strategy used by researchers ("conventional time series model", "reduced-form", "difference-in-differences, "instrumental variable", etc...). We display below the 5 most used empirical strategies for our `r nrow(data_literature)` estimates:

```{r table_empirical_strategy}
data_literature %>%
  group_by(empirical_strategy) %>%
  summarise(n = n()) %>%
  mutate(proportion = n / sum(n) * 100,
         proportion = round(proportion, 0)) %>%
  select(empirical_strategy, proportion) %>%
  arrange(-proportion) %>%
  slice(1:5) %>%
  rename("Empirical Strategy" = empirical_strategy, "Proportion (%)" = proportion) %>%
  kable(., align = c("l", "c")) %>%
  kable_styling(position = "center")
```

* `outcome`: the health outcome studied. 
* `outcome_type`: whether the health outcome is a mortality or morbidity outcome. `r round(sum(data_literature$outcome_type=="mortality")/nrow(data_literature)*100, 0)`% of outcomes are related to mortality.
* `outcome_subgroup`: indicates which age group is studied.
* `control_outcome`: dummy equals to 1 if the health outcome is an outcome that should not be affected by the treatment of interest.
* `independent_variable`: the treatment of interest.
* `temporal_scale`: indicates at which temporal scale data are recorded (e.g., at the daily level).
* `sample_size`: number of observations.
* `increase_independent_variable`: the increase in the independent variable considered (e.g., a 1 $\mu g/m^3$ increase in an air pollutant concentration).
* `independent_variable_dummy`: indicates whether the independent variable is binary. `r round(sum(data_literature$independent_variable_dummy==1)/nrow(data_literature)*100, 0)`% of estimates are for a binary treatment.
* `standardized_effect`: indicates whether the estimates have already been standardized. `r round(sum(data_literature$standardized_effect==1)/nrow(data_literature)*100, 0)`% of estimates are already standardized.
* `log_outcome`: is the estimate expressed in relative term. `r round(sum(data_literature$log_outcome==1)/nrow(data_literature)*100, 0)`% of estimates represent relative increase in the health outcome.
* `mean_outcome` and `sd_outcome`: the average and the standard deviation of the outcome.
* `mean_independant_variable` and `sd_independent_variable`: the average and standard deviation of the treatment of interest.
* `estimate` and `standard_error`: the values of the estimate and its standard error.
* `first_stage_statistic`: the first stage $F$-statistic.
* `source_summary_stats`: where we retrieve in the paper the figures on the average and standard deviation of variables.
* `source_results`: where we retrieve in the paper the figures on the estimates and standard errors.
* `remarks`: remarks on whether we have to do some computations on our own to retrieve the relevant information. For instance, we had to infer for very few papers the mean and standard deviation of a pollutant or an health outcome with statistics such as the median and the quartiles. We use the formula found [here](https://stats.stackexchange.com/questions/256456/how-to-calculate-mean-and-standard-deviation-from-median-and-quartiles).
* `open_data`: whether the data are available to reproduce the findings of the study. Only `r round(sum(data_literature$open_data==1)/nrow(data_literature)*100, 0)`% of results are directly reproducible.

### Computing Standardized Effect Sizes

We standardize the effect sizes using the standard deviations of the independent and outcome variables:

* If we denote $\beta_{unstandardized}$ the unstandardized estimate, SD$_{X}$ the standard deviation of the treatment and SD$_{Y}$  the standard deviation of the health outcome, the standardized estimate is equal to $\beta_{standardized} = \beta \times \frac{SD_{X}}{SD_{Y}}$.
* The standardized standard error SE$_{standardized}$ is then equal to $SE_{standardized} = SE_{unstandardized} \times \frac{\beta_{standardized}}{\beta_{unstandardized}}$.

In the case where authors used linear regression models with log-transformed variables, we rely on the formulas provided by [Rodr√≠guez-Barranco et al. (2017)](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0322-8#Sec8) to standardize the effect size.

```{r standardizing_effects}
# standardize log models
data_literature_logs <- data_literature %>%
  filter(log_outcome == 1) %>%
  select(
    paper_estimate_id,
    standardized_effect,
    independent_variable_dummy,
    mean_outcome,
    sd_outcome,
    sd_independent_variable,
    estimate,
    standard_error
  ) %>%
  mutate(
    estimate_unlog = (exp(estimate) - 1) * mean_outcome,
    ci_upper_95_unlog = (exp(estimate + 1.96 * standard_error) - 1) * mean_outcome,
    standard_error_unlog = (ci_upper_95_unlog - estimate_unlog) / 1.96
  ) %>%
  mutate(
    standardized_estimate = case_when(
      standardized_effect == 0 &
        independent_variable_dummy == 0 ~ estimate_unlog * sd_independent_variable / sd_outcome,
      standardized_effect == 0 &
        independent_variable_dummy == 1 ~ estimate_unlog / sd_outcome,
      standardized_effect == 1 ~ estimate_unlog
    ),
    standardized_standard_error = case_when(
      standardized_effect == 0 ~ standard_error_unlog * standardized_estimate / estimate_unlog,
      standardized_effect == 1 ~ standard_error_unlog
    )
  ) %>%
  select(paper_estimate_id,
         standardized_estimate,
         standardized_standard_error)

# standardize other models
data_literature_not_logs <- data_literature %>%
  filter(log_outcome != 1) %>%
  select(
    paper_estimate_id,
    standardized_effect,
    independent_variable_dummy,
    mean_outcome,
    sd_outcome,
    sd_independent_variable,
    estimate,
    standard_error
  ) %>%
  mutate(
    standardized_estimate = case_when(
      standardized_effect == 0 &
        independent_variable_dummy == 0 ~ estimate * sd_independent_variable / sd_outcome,
      standardized_effect == 0 &
        independent_variable_dummy == 1 ~ estimate / sd_outcome,
      standardized_effect == 1 ~ estimate
    ),
    standardized_standard_error = case_when(
      standardized_effect == 0 ~ standard_error * standardized_estimate / estimate,
      standardized_effect == 1 ~ standard_error
    )
  ) %>%
  select(paper_estimate_id,
         standardized_estimate,
         standardized_standard_error)

# merge the two datasets
data_literature <-
  bind_rows(data_literature_logs, data_literature_not_logs) %>%
  left_join(data_literature, ., by = "paper_estimate_id") %>%
  select(
    paper_label:standard_error,
    standardized_estimate,
    standardized_standard_error,
    first_stage_statistic:open_data
  )
```

We are able to standardize the effects of `r round(sum(!is.na(data_literature$standardized_estimate))/nrow(data_literature)*100, 0)`% of all estimates. We display below summary statistics on the distribution of the standardized effect sizes of causal inference methods:

```{r summary_standardized_effects}
# display sample sizes
data_literature %>%
  filter(!(empirical_strategy %in% c("conventional time series", "conventional time series - suggestive evidence"))) %>%
  mutate(standardized_estimate = abs(standardized_estimate)) %>%
  summarise(
    "Min" = min(standardized_estimate, na.rm = TRUE),
    "First Quartile" = quantile(standardized_estimate, na.rm = TRUE)[2],
    "Mean" = mean(standardized_estimate, na.rm = TRUE),
    "Median" = median(standardized_estimate, na.rm = TRUE),
    "Third Quartile" = quantile(standardized_estimate, na.rm = TRUE)[4],
    "Maximum" = max(standardized_estimate, na.rm = TRUE)
  ) %>%
  kable(., align = rep("c", 6)) %>%
  kable_styling(position = "center")
```

We see that half of the studies estimated effect sizes below `r round(median(data_literature$standardized_estimate, na.rm = TRUE), 2)` standard deviation.

We plot below the ratio of 2SLS estimates over OLS estimates:

```{r graph_ratio_ols_iv, fig.asp=0.5}
# compute ratio 2sls/ols
data_ratio_ols_iv <- data_literature %>%
    select(paper_label, empirical_strategy, outcome, independent_variable, estimate) %>%
    filter(empirical_strategy %in% c("conventional time series", "instrumental variable")) %>%
    arrange(paper_label, outcome, independent_variable) %>%
    group_by(paper_label, outcome, independent_variable) %>%
    summarise(ratio = estimate[2]/estimate[1]) %>%
    drop_na(ratio)

# plot the distribution
graph_ratio_ols_iv <- data_ratio_ols_iv %>% 
  filter(ratio>-20 & ratio <20) %>%
  ggplot() +
  geom_linerange(aes(x = ratio, ymin = -0.5, ymax = 0.5), colour = "#0081a7", alpha = 0.8) +
  geom_linerange(aes(x = 1, ymin = -0.5, ymax = 0.5), linewidth = 1.1, colour = "black") +
  geom_linerange(aes(x = median(data_ratio_ols_iv$ratio), ymin = -0.5, ymax = 0.5), linewidth = 1.1, colour = "#f07167") +
  # annotate(geom = "label", x = -2, y = 0.4, label = "OLS = 2SLS", colour = "black", fill="white", label.size = NA, label.r=unit(0, "cm"), size = 6) +
  # geom_curve(aes(x = -2, y = 0.30,
  #            xend = 0.9, yend = 0),
  #            curvature = 0.3,
  #            arrow = arrow(length = unit(0.42, "cm")),
  #            colour = "black", 
  #            lineend = "round") +
  # annotate(geom = "label", x = 6, y = 0.4, label = "Median of Ratios", colour = "#f07167", fill="white", label.size = NA, label.r=unit(0, "cm"), size = 6) +
  # geom_curve(aes(x = 6, y = 0.30,
  #            xend = 4, yend = 0),
  #            curvature = -0.3,
  #            arrow = arrow(length = unit(0.42, "cm")),
  #            colour = "#f07167", 
  #            lineend = "round") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 21)) +
  xlab("Ratio 2SLS Estimate/OLS Estimate") + ylab(NULL) +
  ggtitle("Distribution of the Ratios of 2SLS over OLS Estimates", subtitle = "2SLS estimates are often much larger than OLS estimates.") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major.y = element_blank()) 

# display the graph
graph_ratio_ols_iv
```

For half of the studies reporting both an OLS estimate and an IV estimate, the ratio is superior to `r round(median(data_ratio_ols_iv$ratio), 1)`.

# Evidence of Publication Bias

### Distribution of t-statistics

We plot the distribution of weighted t-statistics by following [Brodeur et al. (2020)](https://www.aeaweb.org/articles?id=10.1257/aer.20190687) where the weights are equal to the inverse of the number of tests presented in the same table multiplied by the inverse of the number of tables in the article. 


```{r graph_t_statistics}
# male graph of the distribution of t-statistics
graph_distribution_t <- data_literature %>%
  mutate(table = stringr::word(source_results, 1, sep = "-")) %>%
  group_by(paper_label, table) %>%
  mutate(n_tests = 1/n()) %>%
  group_by(paper_label) %>%
  mutate(n_tables = 1/length(unique(table))) %>%
  ungroup() %>%
  mutate(weight = n_tests*n_tables) %>%
  ggplot(., aes(x = abs(estimate/standard_error), y = ..density.., weight = weight)) + 
  geom_histogram(bins = 60, colour = "white", alpha = 0.6) +
  geom_density(bw = 0.35, fill = NA, size = 0.9) +
  geom_vline(xintercept = 1.96, size = 1.1, colour = "#f07167") +
  # geom_curve(aes(x = 5, y = 0.37,
  #            xend = 2.1, yend = 0.27),
  #            curvature = -0.3,
  #            arrow = arrow(length = unit(0.42, "cm")),
  #            colour = "#f07167",
  #            lineend = "round") +
  # annotate("text", x = 5.5, y = 0.38, label = "5% Significance Threshold", colour = "#f07167", size = 6) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("t-statistic") + ylab("Density") +
  ggtitle("Weighted Distribution of the t-Statistics", subtitle = "More mass around the 1.96 threshold.")
  
# display the graph
graph_distribution_t
```

### Estimated Effect Sizes versus Precision

We plot below the relationship between the absolute values of standardized estimates and the inverse of their standard errors. We do not include control outcomes ("placebo" tests) and conventional time series estimates.
  
```{r graph_effect_size_vs_precision}
# make the graph of effect sizes versus precision
graph_effect_precision <- data_literature %>%
  mutate(inverse_se = 1 / standardized_standard_error) %>%
  # drop control outcomes
  filter(control_outcome==0) %>%
  # drop conventional time series models
  filter(!(empirical_strategy %in% c("conventional time series", "conventional time series - suggestive evidence"))) %>%
  ggplot(.,
         aes(x = inverse_se, y = abs(standardized_estimate))) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    size = 1.1, 
    colour = "#f07167",
    linetype = "dashed"
  ) +
  geom_point(shape = 16, colour = "#0081a7", fill = NA, size = 1.8, alpha = 0.6) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x)
      10 ^ x),
    labels = scales::trans_format("log10", scales::math_format(10 ^
                                                                 .x))
  ) +
  scale_y_log10(
    breaks = scales::trans_breaks("log10", function(x)
      10 ^ x),
    labels = scales::trans_format("log10", scales::math_format(10 ^
                                                                 .x))
  ) +
  xlab("Precision (Inverse of Standard Errors)") + ylab("Standardized Effect Sizes") +
  ggtitle("Standardized Estimates vs. Precision", subtitle = "Less precise studies find larger standardized effect sizes.") 

# display the graph
graph_effect_precision
```


```{r graph_intro_paper}
# gather the three graphs in a single graph for the introduction of the paper
graph_intro_paper <-
  graph_ratio_ols_iv / (graph_effect_precision + graph_distribution_t) +
    plot_layout(heights  = c(1, 2.5))

ggsave(
  graph_intro_paper,
  plot_annotation +
    theme(plot.title = element_blank(),
          plot.subtitle = element_blank()),
  filename = here::here(
    "images",
    "graph_introduction.pdf"
  ),
  width = 38,
  height = 30,
  units = "cm"
  # device = cairo_pdf
)
```

# Computing Statistical Power, Type M and S Errors

In this section, we compute the statistical power, the exaggeration factor (Type M error) and the probability to make a type S error for each study. We rely on the [retrodesign](https://cran.r-project.org/web/packages/retrodesign/vignettes/Intro_To_retrodesign.html) package.

To compute the three metrics, we need to make an assumption about the true effect size of each study:

* First, we assess whether the design of each study would be robust enough to detect an effect size that is a bit lower than the observed estimate.
* Second, for instrumental variable strategy, we take the OLS estimate as the true value of the 2SLS causal estimand. 

### Overview

We define the true effect sizes as a decreasing fraction of the estimates. We want to see how the overall distribution of the three metrics evolve with as we decrease the hypothesized true effect size.

```{r data_for_retrodesign, echo = TRUE}
# test type-m and type-s errors
data_retrodesign <- data_literature %>%
  # drop control outcomes
  filter(control_outcome == 0) %>%
  # drop conventional time series models
  filter(!(
    empirical_strategy %in% c(
      "conventional time series",
      "conventional time series - suggestive evidence"
    )
  )) %>%
  select(paper_label, paper_estimate_id, estimate, standard_error) %>%
  # select statistical significant estimates at the 5% level
  filter(abs(estimate / standard_error) >= 1.96)
```

For each study, we compute the statistical power, the exaggeration factor and the probability to make a type S error by defining their true effect sizes as decreasing fraction of the estimates. 

```{r run_retrodesign, echo = TRUE}
# run retrospective power analysis for decreasing effect sizes
data_retrodesign_fraction <- data_retrodesign %>%
  crossing(percentage = seq(from = 50, to = 100) / 100) %>%
  mutate(hypothetical_effect_size = percentage * estimate) %>%
  mutate(
    power = map2(
      hypothetical_effect_size,
      standard_error,
      ~ retro_design(.x, .y)$power * 100
    ),
    type_s = map2(
      hypothetical_effect_size,
      standard_error,
      ~ retro_design(.x, .y)$typeS * 100
    ),
    type_m = map2(
      hypothetical_effect_size,
      standard_error,
      ~ retro_design(.x, .y)$typeM
    )
  ) %>%
  unnest(cols = c(power, type_s, type_m)) %>%
  pivot_longer(
    cols = c(power, type_m, type_s),
    names_to = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = case_when(
      metric == "power" ~ "Statistical Power (%)",
      metric == "type_m" ~ "Exaggeration Ratio",
      metric == "type_s" ~ "Type S Error (%)"
    )
  )

# compute mean values of metrics for the entire literature
data_retrodesign_fraction_mean <- data_retrodesign_fraction %>%
  group_by(metric, percentage) %>%
  summarise(median_value = median(value))
```

We plot below the power and the exaggeration ratio metrics for the different scenarios (Type S error do not occur):
  
```{r graph_retro_power_analysis}
# make the graph of the retrospective power analysis
graph_retrodesign_causal_inf <- data_retrodesign_fraction %>%
  filter(metric != "Type S Error (%)") %>%
  ggplot(., aes(
    x = percentage * 100,
    y = value,
    group = interaction(paper_estimate_id)
  )) +
  geom_line(colour = "#adb5bd", alpha = 0.2) +
  geom_line(
    data = data_retrodesign_fraction_mean %>% filter(metric != "Type S Error (%)"),
    aes(
      x = percentage * 100,
      y = median_value,
      group = "l"
    ),
    colour = "#0081a7",
    size = 1.5
  ) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  facet_wrap( ~ fct_rev(metric), scales = "free_y") +
  xlab("True Effect Size as Fraction of Observed Estimate (%)") + ylab(NULL) +
  theme(panel.grid.major.y = element_blank(),
        # axis ticks
        axis.ticks.x = element_line(size = 0.5, color = "black"),
        axis.ticks.y = element_line(size = 0.5, color = "black"),
        axis.ticks.length = unit(0.15, "cm"))

# display the graph
graph_retrodesign_causal_inf

# save the graph
ggsave(
  graph_retrodesign_causal_inf,
  plot_annotation +
    theme(plot.title = element_blank(),
          plot.subtitle = element_blank()),
  filename = here::here(
    "images",
    "graph_retrodesign_causal_inf.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm"
  # device = cairo_pdf
)
```

We display below summary statistics for the scenario where true effect sizes are equal to the observed estimates reduced by 25%:
  
```{r}
data_retrodesign_fraction %>%
  filter(percentage == 0.75) %>%
  group_by(metric) %>%
  summarise(
    "Min" = min(value, na.rm = TRUE),
    "First Quartile" = quantile(value, na.rm = TRUE)[2],
    "Mean" = mean(value, na.rm = TRUE),
    "Median" = median(value, na.rm = TRUE),
    "Third Quartile" = quantile(value, na.rm = TRUE)[4],
    "Maximum" = max(value, na.rm = TRUE)
  ) %>%
  mutate_at(vars(-metric), ~ round(., 1)) %>%
  rename(Metric = metric) %>%
  kable(., align = c("l", rep("c", 5))) %>%
  kable_styling(position = "center")
```

And here when estimates are divided by two:

```{r}
data_retrodesign_fraction %>%
  filter(percentage == 0.5) %>%
  group_by(metric) %>%
  summarise(
    "Min" = min(value, na.rm = TRUE),
    "First Quartile" = quantile(value, na.rm = TRUE)[2],
    "Mean" = mean(value, na.rm = TRUE),
    "Median" = median(value, na.rm = TRUE),
    "Third Quartile" = quantile(value, na.rm = TRUE)[4],
    "Maximum" = max(value, na.rm = TRUE)
  ) %>%
  mutate_at(vars(-metric), ~ round(., 1)) %>%
  rename(Metric = metric) %>%
  kable(., align = c("l", rep("c", 5))) %>%
  kable_styling(position = "center")
```


### OLS Estimates as True Effect Sizes

For statistically significant 2SLS estimates, we define the true values of effect size as the corresponding OLS estimates. We assume that (i) the causal estimand targeted by the naive and instrumental variable strategy is the same (i.e., we are in the case of homogeneous constant treatment effects), (ii) that there are no omitted variables and (iii) no classical measurement errors in the air pollution exposure.


We retrieve 98 2SLS estimates that were statistically significant at the 5% level and had

```{r graph_distrib_ols_as_true, fig.asp=0.7}
# retrieve iv data
data_iv <- data_literature %>%
  select(
    paper_label,
    empirical_strategy,
    outcome,
    independent_variable,
    estimate,
    standard_error
  ) %>%
  filter(empirical_strategy == "instrumental variable") %>%
  filter(abs(estimate / standard_error) >= 1.96) %>%
  select(-empirical_strategy)

# retrieve ols data
data_ols <- data_literature %>%
  select(paper_label,
         empirical_strategy,
         outcome,
         independent_variable,
         estimate) %>%
  filter(empirical_strategy == "conventional time series") %>%
  select(-empirical_strategy) %>%
  rename(estimate_ols = estimate)

# merge the two datasets
data_retro_iv <-
  left_join(data_iv,
            data_ols,
            by = c("paper_label", "outcome", "independent_variable")) %>%
  drop_na(estimate_ols)

# compute power, type m and s errors
data_retro_iv <- data_retro_iv %>%
  mutate(
    power = map2(estimate_ols,
                 standard_error,
                 ~ retro_design(.x, .y)$power * 100),
    type_m = map2(estimate_ols,
                  standard_error,
                  ~ retro_design(.x, .y)$typeM)
  ) %>%
  unnest(cols = c(power, type_m)) %>%
  mutate_at(vars(power, type_m), ~ round(., 1)) %>%
  select(-estimate,-standard_error,-estimate_ols) %>%
  pivot_longer(
    cols = c("power", "type_m"),
    names_to = "metric",
    values_to = "value"
  ) %>%
  mutate(metric = ifelse(
    metric == "power",
    "Statistical Power (%)",
    "Exaggeration Ratio"
  ))

# compute summary stats
summary_retro_iv <- data_retro_iv %>%
  group_by(metric) %>%
  summarise(median = median(value))

# graph power
graph_retro_iv_power <- data_retro_iv %>%
  filter(metric == "Statistical Power (%)") %>%
  ggplot() +
  geom_linerange(aes(x = value, ymin = -0.5, ymax = 0.5),
                 colour = "#0081a7",
                 alpha = 0.6) +
  geom_linerange(
    data = summary_retro_iv,
    aes(x = median[2], ymin = -0.5, ymax = 0.5),
    size = 1.1,
    colour = "#f07167"
  ) +
  # annotate(geom = "label", x = 13.5, y = 0.27, label = "Median", colour = "#f07167", fill="white", label.size = NA, label.r=unit(0, "cm"), size = 6) +
  # geom_curve(aes(x = 14.5, y = 0.10,
  #            xend = 8.8, yend = -0.2),
  #            curvature = -0.3,
  #            arrow = arrow(length = unit(0.42, "cm")),
  #            colour = "#f07167", 
  #            lineend = "round") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  expand_limits(x = 0, y = 0) +
  labs(x = "Power (%)", y = NULL, title = "Distribution of Statistical Power") +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    panel.grid.major.y = element_blank()
  )

# graph type m error
graph_retro_iv_type_m <- data_retro_iv %>%
  filter(metric == "Exaggeration Ratio") %>%
  filter(value < 42) %>%
  ggplot() +
  geom_linerange(aes(x = value, ymin = -0.5, ymax = 0.5),
                 colour = "#0081a7",
                 alpha = 0.8) +
  geom_linerange(
    data = summary_retro_iv,
    aes(x = median[1], ymin = -0.5, ymax = 0.5),
    size = 1.1,
    colour = "#f07167"
  ) +
  expand_limits(x = 1, y = 0) +
  labs(x = "Exaggeration Factor", y = NULL, title = "Distribution of Exaggeration Factor") +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    panel.grid.major.y = element_blank()
  )

# display the graph
graph_retro_iv <- graph_retro_iv_power / graph_retro_iv_type_m
graph_retro_iv

# save graph
ggsave(
  graph_retro_iv,
  plot_annotation +
    theme(plot.title = element_blank(),
          plot.subtitle = element_blank()),
  filename = here::here(
    "images",
    "graph_retro_iv.pdf"
  ),
  width = 30,
  height = 15,
  units = "cm"
  # device = cairo_pdf
)
```

The median statistical power is `r summary_retro_iv$median[1]`% and the median type M error is `r summary_retro_iv$median[2]`.









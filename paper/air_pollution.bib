@book{_statistical_2008,
  title = {Statistical {{Methods}} for {{Environmental Epidemiology}} with {{R}}},
  year = 2008,
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-78167-9},
  urldate = {2021-06-10},
  isbn = {978-0-387-78166-2 978-0-387-78167-9},
  langid = {english}
}

@book{_statistical_2008a,
  title = {Statistical {{Methods}} for {{Environmental Epidemiology}} with {{R}}},
  year = 2008,
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-78167-9},
  urldate = {2021-06-10},
  isbn = {978-0-387-78166-2 978-0-387-78167-9},
  langid = {english}
}

@article{abadie_statistical_2020,
  title = {Statistical {{Nonsignificance}} in {{Empirical Economics}}},
  author = {Abadie, Alberto},
  year = 2020,
  month = jun,
  journal = {American Economic Review: Insights},
  volume = {2},
  number = {2},
  pages = {193--208},
  issn = {2640-205X, 2640-2068},
  doi = {10.1257/aeri.20190252},
  abstract = {Statistical significance is often interpreted as providing greater information than nonsignificance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are common in economics, where data-sets are large and there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. Therefore, we advocate visible reporting and discussion of nonsignificant results. (JEL C12, C90)},
  langid = {english},
  keywords = {Example,NHST,Publication bias}
}

@article{abadie_statistical_2020,
  title = {Statistical {{Nonsignificance}} in {{Empirical Economics}}},
  author = {Abadie, Alberto},
  year = 2020,
  month = jun,
  journal = {American Economic Review: Insights},
  volume = {2},
  number = {2},
  pages = {193--208},
  issn = {2640-205X, 2640-2068},
  doi = {10.1257/aeri.20190252},
  urldate = {2021-06-29},
  abstract = {Statistical significance is often interpreted as providing greater information than nonsignificance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are common in economics, where data-sets are large and there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. Therefore, we advocate visible reporting and discussion of nonsignificant results. (JEL C12, C90)},
  langid = {english},
  keywords = {Example,NHST,Publication bias}
}

@article{adhvaryu2022management,
  title = {Management and Shocks to Worker Productivity},
  author = {Adhvaryu, Achyuta and Kala, Namrata and Nyshadham, Anant},
  year = 2022,
  journal = {Journal of Political Economy},
  volume = {130},
  number = {1},
  pages = {1--47},
  publisher = {The University of Chicago Press Chicago, IL}
}

@book{agresti_statistical_2018,
  title = {Statistical Methods for the Social Sciences},
  author = {Agresti, Alan},
  year = 2018,
  edition = {Fifth edition},
  publisher = {Pearson},
  address = {Boston},
  isbn = {978-0-13-450710-1},
  langid = {english},
  lccn = {QA276.12 .A34 2018},
  keywords = {Statistical power (Leo)}
}

@article{aguilar2020adaptation,
  title = {Adaptation and Mitigation of Pollution: Evidence from Air Quality Warnings},
  author = {{Aguilar-Gomez}, Sandra},
  year = 2020,
  journal = {Unpublished manuscript}
}

@misc{allaire2018distill,
  title = {Distill for {{R}} Markdown},
  author = {Allaire, {\relax JJ} and Iannone, Rich and Presmanes Hill, Alison and Xie, Yihui},
  year = 2018
}

@article{altoe_enhancing_2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Alto{\`e}, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagn{\`i}, Antonio and Finos, Livio and Pastore, Massimiliano},
  year = 2020,
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2893},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02893},
  urldate = {2021-06-10},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed ``prospective and retrospective design analysis.'' Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers' awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen's d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.},
  langid = {english}
}

@article{altoe_enhancing_2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Alto{\`e}, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagn{\`i}, Antonio and Finos, Livio and Pastore, Massimiliano},
  year = 2020,
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {2893},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02893},
  urldate = {2022-11-17},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed ``prospective and retrospective design analysis.'' Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers' awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen's d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.},
  langid = {english}
}

@article{amrhein_inferential_2019,
  title = {Inferential {{Statistics}} as {{Descriptive Statistics}}: {{There Is No Replication Crisis}} If {{We Don}}'t {{Expect Replication}}},
  shorttitle = {Inferential {{Statistics}} as {{Descriptive Statistics}}},
  author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
  year = 2019,
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {262--270},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1543137},
  abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a ``replication crisis'' may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
  langid = {english}
}

@article{amrhein_inferential_2019,
  title = {Inferential {{Statistics}} as {{Descriptive Statistics}}: {{There Is No Replication Crisis}} If {{We Don}}'t {{Expect Replication}}},
  shorttitle = {Inferential {{Statistics}} as {{Descriptive Statistics}}},
  author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
  year = 2019,
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {262--270},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1543137},
  urldate = {2022-11-17},
  abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a ``replication crisis'' may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
  langid = {english}
}

@article{anderson-cook_experimental_2005,
  title = {Experimental and {{Quasi-Experimental Designs}} for {{Generalized Causal Inference}}},
  author = {{Anderson-Cook}, Christine M},
  year = 2005,
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {708--708},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2005.s22},
  langid = {english}
}

@techreport{anderson2022bounds,
  title = {Bounds, Benefits, and Bad Air: {{Welfare}} Impacts of Pollution Alerts},
  author = {Anderson, Michael L and Hyun, Minwoo and Lee, Jaecheol},
  year = 2022,
  institution = {National Bureau of Economic Research}
}

@article{andre_carbon_2010,
  title = {Carbon {{Monoxide Pollution Promotes Cardiac Remodeling}} and {{Ventricular Arrhythmia}} in {{Healthy Rats}}},
  author = {Andre, Lucas and Boissi{\`e}re, Julien and Reboul, Cyril and Perrier, Romain and Zalvidea, Santiago and Meyer, Gregory and Thireau, J{\'e}r{\^o}me and Tanguy, St{\'e}phane and Bideaux, Patrice and Hayot, Maurice and Boucher, Fran{\c c}ois and Obert, Philippe and Cazorla, Olivier and Richard, Sylvain},
  year = 2010,
  month = mar,
  journal = {American Journal of Respiratory and Critical Care Medicine},
  volume = {181},
  number = {6},
  pages = {587--595},
  issn = {1073-449X},
  doi = {10.1164/rccm.200905-0794OC},
  urldate = {2020-03-12},
  abstract = {Rationale: Epidemiologic studies associate atmospheric carbon monoxide (CO) pollution with adverse cardiovascular outcomes and increased cardiac mortality risk. However, there is a lack of data regarding cellular mechanisms in healthy individuals.Objectives: To investigate the chronic effects of environmentally relevant CO levels on cardiac function in a well-standardized healthy animal model.Methods : Wistar rats were exposed for 4 weeks to filtered air (CO {$<$} 1 ppm) or air enriched with CO (30 ppm with five peaks of 100 ppm per 24-h period), consistent with urban pollution. Myocardial function was assessed by echocardiography and analysis of surface ECG and in vitro by measuring the excitation-contraction coupling of single left ventricular cardiomyocytes.Measurements and Main Results: Chronic CO pollution promoted left ventricular interstitial and perivascular fibrosis, with no change in cardiomyocyte size, and had weak, yet significant, effects on in vivo cardiac function. However, both contraction and relaxation of single cardiomyocytes were markedly altered. Several changes occurred, including decreased Ca2+ transient amplitude and Ca2+ sensitivity of myofilaments and increased diastolic intracellular Ca2+ subsequent to decreased SERCA-2a expression and impaired Ca2+ reuptake. CO pollution increased the number of arrhythmic events. Hyperphosphorylation of Ca2+-handling and sarcomeric proteins, and reduced responses to {$\beta$}-adrenergic challenge were obtained, suggestive of moderate CO-induced hyperadrenergic state.Conclusions: Chronic CO exposure promotes a pathological phenotype of cardiomyocytes in the absence of underlying cardiomyopathy. The less severe phenotype in vivo suggests a role for compensatory mechanisms. Arrhythmia propensity may derive from intracellular Ca2+ overload.}
}

@article{andrews_identification_2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  year = 2019,
  month = aug,
  journal = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  urldate = {2021-04-21},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {Estimation: General Design of Experiments: General Higher Education,Media,Research Institutions Labor Demand Wages Compensation and Labor Costs: Public Policy Entertainment}
}

@article{andrews_identification_2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  year = 2019,
  month = aug,
  journal = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {and Labor Costs: Public Policy,Compensation,Design of Experiments: General,Entertainment,Estimation: General,Higher Education,Labor Demand,Media,Publication bias,Research Institutions,Wages}
}

@article{andrews_identification_2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  year = 2019,
  month = aug,
  journal = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {and Labor Costs: Public Policy,Compensation,Design of Experiments: General,Entertainment,Estimation: General,Higher Education,Labor Demand,Media,Research Institutions,Wages}
}

@article{angrist_credibility_2010,
  title = {The {{Credibility Revolution}} in {{Empirical Economics}}: {{How Better Research Design Is Taking}} the {{Con}} out of {{Econometrics}}},
  shorttitle = {The {{Credibility Revolution}} in {{Empirical Economics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = 2010,
  month = jun,
  journal = {Journal of Economic Perspectives},
  volume = {24},
  number = {2},
  pages = {3--30},
  issn = {0895-3309},
  doi = {10.1257/jep.24.2.3},
  abstract = {Since Edward Leamer's memorable 1983 paper, "Let's Take the Con out of Econometrics," empirical microeconomics has experienced a credibility revolution. While Leamer's suggested remedy, sensitivity analysis, has played a role in this, we argue that the primary engine driving improvement has been a focus on the quality of empirical research designs. The advantages of a good research design are perhaps most easily apparent in research using random assignment. We begin with an overview of Leamer's 1983 critique and his proposed remedies. We then turn to the key factors we see contributing to improved empirical work, including the availability of more and better data, along with advances in theoretical econometric understanding, but especially the fact that research design has moved front and center in much of empirical micro. We offer a brief digression into macroeconomics and industrial organization, where progress -- by our lights -- is less dramatic, although there is work in both fields that we find encouraging. Finally, we discuss the view that the design pendulum has swung too far. Critics of design-driven studies argue that in pursuit of clean and credible research designs, researchers seek good answers instead of good questions. We briefly respond to this concern, which worries us little.},
  langid = {english},
  keywords = {Econometrics,Economic Methodology}
}

@book{angrist_mastering_2014,
  title = {Mastering '{{Metrics}}: {{The Path}} from {{Cause}} to {{Effect}}},
  shorttitle = {Mastering '{{Metrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = 2014,
  month = dec,
  publisher = {Princeton University Press},
  abstract = {From Joshua Angrist, winner of the Nobel Prize in Economics, and J\"orn-Steffen Pischke, an accessible and fun guide to the essential tools of econometric researchApplied econometrics, known to aficionados as 'metrics, is the original data science. 'Metrics encompasses the statistical methods economists use to untangle cause and effect in human affairs. Through accessible discussion and with a dose of kung fu--themed humor, Mastering 'Metrics presents the essential tools of econometric research and demonstrates why econometrics is exciting and useful.The five most valuable econometric methods, or what the authors call the Furious Five--random assignment, regression, instrumental variables, regression discontinuity designs, and differences in differences--are illustrated through well-crafted real-world examples (vetted for awesomeness by Kung Fu Panda's Jade Palace). Does health insurance make you healthier? Randomized experiments provide answers. Are expensive private colleges and selective public high schools better than more pedestrian institutions? Regression analysis and a regression discontinuity design reveal the surprising truth. When private banks teeter, and depositors take their money and run, should central banks step in to save them? Differences-in-differences analysis of a Depression-era banking crisis offers a response. Could arresting O. J. Simpson have saved his ex-wife's life? Instrumental variables methods instruct law enforcement authorities in how best to respond to domestic abuse.Wielding econometric tools with skill and confidence, Mastering 'Metrics uses data and statistics to illuminate the path from cause to effect.Shows why econometrics is importantExplains econometric research through humorous and accessible discussionOutlines empirical methods central to modern econometric practiceWorks through interesting and relevant real-world examples},
  isbn = {978-1-4008-5238-3},
  langid = {english},
  keywords = {Business & Economics / Econometrics}
}

@book{angrist_mostly_2009,
  title = {Mostly {{Harmless Econometrics}}: {{An Empiricist}}'s {{Companion}}},
  shorttitle = {Mostly {{Harmless Econometrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = 2009,
  month = jan,
  edition = {1 edition},
  publisher = {Princeton University Press},
  address = {Princeton},
  abstract = {The core methods in today's econometric toolkit are linear regression for statistical control, instrumental variables methods for the analysis of natural experiments, and differences-in-differences methods that exploit policy changes. In the modern experimentalist paradigm, these techniques address clear causal questions such as: Do smaller classes increase learning? Should wife batterers be arrested? How much does education raise wages? Mostly Harmless Econometrics shows how the basic tools of applied econometrics allow the data to speak. In addition to econometric essentials, Mostly Harmless Econometrics covers important new extensions--regression-discontinuity designs and quantile regression--as well as how to get standard errors right. Joshua Angrist and J\"orn-Steffen Pischke explain why fancier econometric techniques are typically unnecessary and even dangerous. The applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science. An irreverent review of econometric essentials A focus on tools that applied researchers use most Chapters on regression-discontinuity designs, quantile regression, and standard errors Many empirical examples A clear and concise resource with wide applications},
  isbn = {978-0-691-12035-5},
  langid = {english},
  keywords = {DiD,Econometrics,Handbook,IV,Methods,RDD,Theory}
}

@article{angrist_two-stage_1995,
  title = {Two-{{Stage Least Squares Estimation}} of {{Average Causal Effects}} in {{Models}} with {{Variable Treatment Intensity}}},
  author = {Angrist, Joshua D. and Imbens, Guido W.},
  year = 1995,
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {431--442},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1995.10476535},
  urldate = {2022-11-17},
  langid = {english}
}

@article{arceo_does_2016,
  title = {Does the {{Effect}} of {{Pollution}} on {{Infant Mortality Differ Between Developing}} and {{Developed Countries}}? {{Evidence}} from {{Mexico City}}},
  shorttitle = {Does the {{Effect}} of {{Pollution}} on {{Infant Mortality Differ Between Developing}} and {{Developed Countries}}?},
  author = {Arceo, Eva and Hanna, Rema and Oliva, Paulina},
  year = 2016,
  month = mar,
  journal = {The Economic Journal},
  volume = {126},
  number = {591},
  pages = {257--280},
  issn = {00130133},
  doi = {10.1111/ecoj.12273},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{arceo_does_2016,
  title = {Does the {{Effect}} of {{Pollution}} on {{Infant Mortality Differ Between Developing}} and {{Developed Countries}}? {{Evidence}} from {{Mexico City}}},
  shorttitle = {Does the {{Effect}} of {{Pollution}} on {{Infant Mortality Differ Between Developing}} and {{Developed Countries}}?},
  author = {Arceo, Eva and Hanna, Rema and Oliva, Paulina},
  year = 2016,
  month = mar,
  journal = {The Economic Journal},
  volume = {126},
  number = {591},
  pages = {257--280},
  issn = {00130133},
  doi = {10.1111/ecoj.12273},
  urldate = {2022-11-17},
  langid = {english}
}

@techreport{arel_bundock_quantitative_2022,
  type = {Preprint},
  title = {Quantitative {{Political Science Research}} Is {{Greatly Underpowered}}},
  author = {{Arel-Bundock}, Vincent and Briggs, Ryan C and Doucouliagos, Hristos and Mendoza Avi{\~n}a, Marco and Stanley, T.D.},
  year = 2022,
  month = jul,
  institution = {Open Science Framework},
  doi = {10.31219/osf.io/7vy2f},
  urldate = {2022-11-17},
  abstract = {The social sciences face a replicability crisis. A key determinant of replication success is statistical power. We assess the power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles. Using generous assumptions, we find that the median analysis has about 10\% power and that only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. We also find substantial heterogeneity in tests across research areas, with some being characterized by high power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  langid = {english}
}

@article{arel-bundock_quantitative_2024,
  title = {Quantitative {{Political Science Research Is Greatly Underpowered}}},
  author = {{Arel-Bundock}, Vincent and Briggs, Ryan C. and Doucouliagos, Hristos and Avi{\~n}a, Marco M. and Stanley, T. D.},
  year = 2024,
  month = dec,
  journal = {The Journal of Politics},
  volume = {88},
  number = {1},
  pages = {36--46},
  publisher = {The University of Chicago Press},
  issn = {0022-3816},
  doi = {10.1086/734279},
  urldate = {2025-11-13},
  abstract = {The social sciences face a replicability crisis. A key determinant of replication success is statistical power. We assess the power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles in 46 areas of the discipline. Under generous assumptions, we show that quantitative research in political science is greatly underpowered: the median analysis has about 10\% power, and only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. We also find substantial heterogeneity in tests across research areas, with some being characterized by high power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  keywords = {meta-analysis,methodology,notion,p-hacking,publication bias,sample size,statistical power}
}

@techreport{aronow_does_2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {Does {{Regression Produce Representative Estimates}} of {{Causal Effects}}?},
  author = {Aronow, Peter M. and Samii, Cyrus},
  year = 2015,
  month = feb,
  number = {ID 2224964},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest. What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample. Causal effects estimated via multiple regression differentially weight each unit's contribution. The "effective sample'' that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce. There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods. We show how to estimate the "multiple regression weights'' that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects. The requisite conditions cannot always be met.},
  langid = {english},
  keywords = {Solutions,Variations}
}

@article{athey_econometrics_2016,
  title = {The {{Econometrics}} of {{Randomized Experiments}}},
  author = {Athey, Susan and Imbens, Guido},
  year = 2016,
  month = jul,
  journal = {arXiv:1607.00698 [econ, stat]},
  eprint = {1607.00698},
  primaryclass = {econ, stat},
  abstract = {In this chapter, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as noncompliance. In the presence of non-compliance we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider in detail estimation and inference for heterogenous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62K99,Economics - Econometrics,G.3,Statistics - Methodology}
}

@article{austin_cesifo_nodate,
  title = {{{CESifo Working Paper}} No. 8609},
  author = {Austin, Wes and Carattini, Stefano and Mahecha, John Gomez and Pesko, Michael},
  pages = {40},
  abstract = {We examine the relationship between contemporaneous fine particulate matter exposure and COVID-19 morbidity and mortality using an instrumental variable approach based on wind direction. Harnessing daily changes in county-level wind direction, we show that arguably exogenous fluctuations in local air quality impact the rate of confirmed cases and deaths from COVID-19. In our preferred high dimensional fixed effects specification with state-level policy and social distancing controls, we find that a one {$\mu$}g/m3 increase in PM 2.5 increases the number of confirmed cases by roughly 2\% from the mean case rate in a county. These effects tend to increase in magnitude over longer time horizons, being twice as large over a 3-day period. Meanwhile, a one {$\mu$}g/m3 increase in PM 2.5 increases the same-day death rate by 3\% from the mean. Our estimates are robust to a host of sensitivity tests. These results suggest that air pollution plays an important role in mediating the severity of respiratory syndromes such as COVID-19, for which progressive respiratory failure is the primary cause of death, and that policy levers to improve air quality may lead to improvements in COVID-19 outcomes.},
  langid = {english}
}

@article{baccini_assessing_2017,
  title = {Assessing the Short Term Impact of Air Pollution on Mortality: A Matching Approach},
  shorttitle = {Assessing the Short Term Impact of Air Pollution on Mortality},
  author = {Baccini, Michela and Mattei, Alessandra and Mealli, Fabrizia and Bertazzi, Pier Alberto and Carugno, Michele},
  year = 2017,
  month = dec,
  journal = {Environmental Health},
  volume = {16},
  number = {1},
  pages = {7},
  issn = {1476-069X},
  doi = {10.1186/s12940-017-0215-7},
  urldate = {2021-06-10},
  abstract = {Background: The opportunity to assess short term impact of air pollution relies on the causal interpretation of the exposure-response association. However, up to now few studies explicitly faced this issue within a causal inference framework. In this paper, we reformulated the problem of assessing the short term impact of air pollution on health using the potential outcome approach to causal inference. We considered the impact of high daily levels of particulate matter {$\leq$}10 {$\mu$}m in diameter (PM10) on mortality within two days from the exposure in the metropolitan area of Milan (Italy), during the period 2003--2006. Our research focus was the causal impact of a hypothetical intervention setting daily air pollution levels under a pre-fixed threshold. Methods: We applied a matching procedure based on propensity score to estimate the total number of attributable deaths (AD) during the study period. After defining the number of attributable deaths in terms of difference between potential outcomes, we used the estimated propensity score to match each high exposure day, namely each day with a level of exposure higher than 40 {$\mu$}g/m3, with a day with similar background characteristics but a level of exposure lower than 40 {$\mu$}g/m3. Then, we estimated the impact by comparing mortality between matched days. Results: During the study period daily exposures larger than 40 {$\mu$}g/m3 were responsible for 1079 deaths (90\% CI: 116; 2042). The impact was more evident among the elderly than in the younger age classes. Exposures {$\geq$} 40 {$\mu$}g/m3 were responsible, among the elderly, for 1102 deaths (90\% CI: 388, 1816), of which 797 from cardiovascular causes and 243 from respiratory causes. Clear evidence of an impact on respiratory mortality was found also in the age class 65--74, with 87 AD (90\% CI: 11, 163). Conclusions: The propensity score matching turned out to be an appealing method to assess historical impacts in this field, which guarantees that the estimated total number of AD can be derived directly as sum of either age-specific or cause-specific AD, unlike the standard model-based procedure. For this reason, it is a promising approach to perform surveillance focusing on very specific causes of death or diseases, or on susceptible subpopulations. Finally, the propensity score matching is free from issues concerning the exposure-confounders-mortality modeling and does not involve extrapolation. On the one hand this enhances the internal validity of our results; on the other, it makes the approach scarcely appropriate for estimating future impacts.},
  langid = {english}
}

@article{baccini_assessing_2017,
  title = {Assessing the Short Term Impact of Air Pollution on Mortality: A Matching Approach},
  shorttitle = {Assessing the Short Term Impact of Air Pollution on Mortality},
  author = {Baccini, Michela and Mattei, Alessandra and Mealli, Fabrizia and Bertazzi, Pier Alberto and Carugno, Michele},
  year = 2017,
  month = dec,
  journal = {Environmental Health},
  volume = {16},
  number = {1},
  pages = {7},
  issn = {1476-069X},
  doi = {10.1186/s12940-017-0215-7},
  urldate = {2022-11-17},
  abstract = {Background: The opportunity to assess short term impact of air pollution relies on the causal interpretation of the exposure-response association. However, up to now few studies explicitly faced this issue within a causal inference framework. In this paper, we reformulated the problem of assessing the short term impact of air pollution on health using the potential outcome approach to causal inference. We considered the impact of high daily levels of particulate matter {$\leq$}10 {$\mu$}m in diameter (PM10) on mortality within two days from the exposure in the metropolitan area of Milan (Italy), during the period 2003--2006. Our research focus was the causal impact of a hypothetical intervention setting daily air pollution levels under a pre-fixed threshold. Methods: We applied a matching procedure based on propensity score to estimate the total number of attributable deaths (AD) during the study period. After defining the number of attributable deaths in terms of difference between potential outcomes, we used the estimated propensity score to match each high exposure day, namely each day with a level of exposure higher than 40 {$\mu$}g/m3, with a day with similar background characteristics but a level of exposure lower than 40 {$\mu$}g/m3. Then, we estimated the impact by comparing mortality between matched days. Results: During the study period daily exposures larger than 40 {$\mu$}g/m3 were responsible for 1079 deaths (90\% CI: 116; 2042). The impact was more evident among the elderly than in the younger age classes. Exposures {$\geq$} 40 {$\mu$}g/m3 were responsible, among the elderly, for 1102 deaths (90\% CI: 388, 1816), of which 797 from cardiovascular causes and 243 from respiratory causes. Clear evidence of an impact on respiratory mortality was found also in the age class 65--74, with 87 AD (90\% CI: 11, 163). Conclusions: The propensity score matching turned out to be an appealing method to assess historical impacts in this field, which guarantees that the estimated total number of AD can be derived directly as sum of either age-specific or cause-specific AD, unlike the standard model-based procedure. For this reason, it is a promising approach to perform surveillance focusing on very specific causes of death or diseases, or on susceptible subpopulations. Finally, the propensity score matching is free from issues concerning the exposure-confounders-mortality modeling and does not involve extrapolation. On the one hand this enhances the internal validity of our results; on the other, it makes the approach scarcely appropriate for estimating future impacts.},
  langid = {english}
}

@misc{bagilet_causal_2023,
  title = {Causal {{Exaggeration}}: {{Unconfounded}} but {{Inflated Causal Estimates}}},
  author = {Bagilet, Vincent},
  year = 2023,
  month = nov,
  langid = {english}
}

@misc{bagilet_causal_2026,
  type = {{{SSRN Scholarly Paper}}},
  title = {Causal {{Exaggeration}}: {{Unconfounded}} but {{Inflated Causal Estimates}}},
  shorttitle = {Causal {{Exaggeration}}},
  author = {Bagilet, Vincent},
  year = 2026,
  month = jan,
  number = {6071752},
  eprint = {6071752},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.6071752},
  urldate = {2026-02-02},
  abstract = {The credibility revolution has made causal inference methods ubiquitous in economics. Yet there is widespread evidence of selection on statistical significance and associated biases in the literature. I show that these two phenomena interact to reduce the reliability of published estimates: while causal identification strategies alleviate bias from confounders, by restricting the variation used for identification they reduce statistical power and can exacerbate another bias-exaggeration. I characterize this confounding-exaggeration trade-off theoretically and through realistic Monte Carlo simulations, and explore its prevalence in the literature. In realistic settings, exaggeration can exceed the confounding bias these methods aim to eliminate. Finally, I propose practical solutions to navigate this trade-off, including a versatile tool to identify the variation and observations actually driving identification in applied causal studies.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Applied Microeconomics,Causal inference,Exaggeration,Package,Simulations,Statistical Power},
  file = {/Users/vincentbagilet/Zotero/storage/BCF3N88X/Bagilet - 2026 - Causal Exaggeration Unconfounded but Inflated Causal Estimates.pdf}
}

@article{bagilet_unconfounded_nodate,
  title = {Unconfounded but {{Inflated Causal Estimates}}},
  author = {Bagilet, Vincent and {Zabrocki-Hallak}, L{\'e}o},
  pages = {24},
  abstract = {Convincing research designs make empirical economics credible. To avoid confounding, quasi-experimental studies focus on specific sources of variation. This could lead to a reduction in statistical power. Yet, published estimates can overestimate true effects sizes when power is low. Using fake data simulations, we show that for all causal inference methods, there could be a trade-off between confounding and exaggerating true effect sizes due to a loss in power. We then discuss how reporting power calculations could help address this issue.},
  langid = {english}
}

@misc{baker_difference--differences_2019,
  title = {Difference-in-{{Differences Methodology}}},
  author = {Baker, Andrew C.},
  year = 2019,
  month = sep,
  abstract = {Introduction In this methodological section I will explain the issues with difference-in-differences (DiD) designs when there are multiple units and more than two time periods, and also the particular issues that arise when the treatment is conducted at staggered periods in time. In the canonical DiD set-up (e.g. the Card and Kreuger minimum wage study comparing New Jersey and Pennsylvania) there are two units and two time periods, with one of the units being treated in the second period.},
  langid = {american}
}

@article{barnett_examination_2019,
  title = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019: An Observational Study},
  shorttitle = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019},
  author = {Barnett, Adrian Gerard and Wren, Jonathan D.},
  year = 2019,
  month = nov,
  journal = {BMJ Open},
  volume = {9},
  number = {11},
  pages = {e032506},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2019-032506},
  urldate = {2022-04-12},
  abstract = {Objectives Previous research has shown clear biases in the distribution of published p values, with an excess below the 0.05 threshold due to a combination of p-hacking and publication bias. We aimed to examine the bias for statistical significance using published confidence intervals. Design Observational study. Setting Papers published in Medline since 1976. Participants Over 968 000 confidence intervals extracted from abstracts and over 350 000 intervals extracted from the full-text. Outcome measures Cumulative distributions of lower and upper confidence interval limits for ratio estimates. Results We found an excess of statistically significant results with a glut of lower intervals just above one and upper intervals just below 1. These excesses have not improved in recent years. The excesses did not appear in a set of over 100 000 confidence intervals that were not subject to p-hacking or publication bias. Conclusions The huge excesses of published confidence intervals that are just below the statistically significant threshold are not statistically plausible. Large improvements in research practice are needed to provide more results that better reflect the truth.},
  chapter = {Evidence based practice},
  copyright = {\copyright{} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. Published by BMJ.. This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See:~https://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {31753893},
  keywords = {confidence intervals,p-hacking,p-values,publication bias,statistical significance}
}

@techreport{barwick_morbidity_2018,
  title = {The {{Morbidity Cost}} of {{Air Pollution}}: {{Evidence}} from {{Consumer Spending}} in {{China}}},
  shorttitle = {The {{Morbidity Cost}} of {{Air Pollution}}},
  author = {Barwick, Panle Jia and Li, Shanjun and Rao, Deyu and Zahur, Nahim Bin},
  year = 2018,
  month = jun,
  number = {w24688},
  pages = {w24688},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w24688},
  urldate = {2020-11-05},
  abstract = {Developing and fast-growing economies have some of the worse air pollution in the world, but there is a lack of systematic evidence on the health especially morbidity impact of air pollution in these countries. Based on the universe of credit and debit card transactions in China from 2013 to 2015, this paper provides to our knowledge the first analysis of the morbidity cost of PM2.5 for the entire population of a developing country. To address potential endogeneity in pollution exposure, we construct an instrumental variable by modeling the spatial spillovers of PM2.5 due to long-range transport. We propose a flexible distributed-lag model that incorporates the IV approach to capture the dynamic response to past pollution exposure. Our analysis shows that PM2.5 has a significant impact on healthcare spending in both the short and medium terms that survives an array of robustness checks. The annual reduction in national healthcare spending from complying with the World Health Organization's annual standard of 10 \textmu g/m3 would amount to \$42 billion, or nearly 7\% of China's total healthcare spending in 2015. In contrast to the common perception that the morbidity impact is modest relative to the mortality impact, our estimated morbidity cost of air pollution is about two-thirds of the mortality cost from the recent literature.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@techreport{barwick_morbidity_2018,
  title = {The {{Morbidity Cost}} of {{Air Pollution}}: {{Evidence}} from {{Consumer Spending}} in {{China}}},
  shorttitle = {The {{Morbidity Cost}} of {{Air Pollution}}},
  author = {Barwick, Panle Jia and Li, Shanjun and Rao, Deyu and Zahur, Nahim Bin},
  year = 2018,
  month = jun,
  number = {w24688},
  pages = {w24688},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w24688},
  abstract = {Developing and fast-growing economies have some of the worse air pollution in the world, but there is a lack of systematic evidence on the health especially morbidity impact of air pollution in these countries. Based on the universe of credit and debit card transactions in China from 2013 to 2015, this paper provides to our knowledge the first analysis of the morbidity cost of PM2.5 for the entire population of a developing country. To address potential endogeneity in pollution exposure, we construct an instrumental variable by modeling the spatial spillovers of PM2.5 due to long-range transport. We propose a flexible distributed-lag model that incorporates the IV approach to capture the dynamic response to past pollution exposure. Our analysis shows that PM2.5 has a significant impact on healthcare spending in both the short and medium terms that survives an array of robustness checks. The annual reduction in national healthcare spending from complying with the World Health Organization's annual standard of 10 \textmu g/m3 would amount to \$42 billion, or nearly 7\% of China's total healthcare spending in 2015. In contrast to the common perception that the morbidity impact is modest relative to the mortality impact, our estimated morbidity cost of air pollution is about two-thirds of the mortality cost from the recent literature.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{bauernschuster_when_2017,
  title = {When {{Labor Disputes Bring Cities}} to a {{Standstill}}: {{The Impact}} of {{Public Transit Strikes}} on {{Traffic}}, {{Accidents}}, {{Air Pollution}}, and {{Health}}},
  shorttitle = {When {{Labor Disputes Bring Cities}} to a {{Standstill}}},
  author = {Bauernschuster, Stefan and Hener, Timo and Rainer, Helmut},
  year = 2017,
  month = feb,
  journal = {American Economic Journal: Economic Policy},
  volume = {9},
  number = {1},
  pages = {1--37},
  issn = {1945-7731, 1945-774X},
  doi = {10.1257/pol.20150414},
  urldate = {2020-11-05},
  abstract = {Many governments have banned strikes in public transportation. Whether this can be justified depends on whether strikes endanger public safety or health. We use time-series and cross-sectional variation in powerful registry data to quantify the effects of public transit strikes on urban populations in Germany. Due to higher traffic volumes and longer travel times, total car hours operated increase by 11 to 13 percent during strikes. This effect is accompanied by a 14 percent increase in vehicle crashes, a 20 percent increase in accident-related injuries, a 14 percent increase in particle pollution, and an 11 percent increase in hospital admissions for respiratory diseases among young children. (JEL I12, J45, J52, L91, Q53, R41)},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{bauernschuster_when_2017,
  title = {When {{Labor Disputes Bring Cities}} to a {{Standstill}}: {{The Impact}} of {{Public Transit Strikes}} on {{Traffic}}, {{Accidents}}, {{Air Pollution}}, and {{Health}}},
  shorttitle = {When {{Labor Disputes Bring Cities}} to a {{Standstill}}},
  author = {Bauernschuster, Stefan and Hener, Timo and Rainer, Helmut},
  year = 2017,
  month = feb,
  journal = {American Economic Journal: Economic Policy},
  volume = {9},
  number = {1},
  pages = {1--37},
  issn = {1945-7731, 1945-774X},
  doi = {10.1257/pol.20150414},
  urldate = {2022-11-17},
  abstract = {Many governments have banned strikes in public transportation. Whether this can be justified depends on whether strikes endanger public safety or health. We use time-series and cross-sectional variation in powerful registry data to quantify the effects of public transit strikes on urban populations in Germany. Due to higher traffic volumes and longer travel times, total car hours operated increase by 11 to 13 percent during strikes. This effect is accompanied by a 14 percent increase in vehicle crashes, a 20 percent increase in accident-related injuries, a 14 percent increase in particle pollution, and an 11 percent increase in hospital admissions for respiratory diseases among young children. (JEL I12, J45, J52, L91, Q53, R41)},
  langid = {english}
}

@article{beard_winter_2012,
  title = {Winter {{Temperature Inversions}} and {{Emergency Department Visits}} for {{Asthma}} in {{Salt Lake County}}, {{Utah}}, 2003--2008},
  author = {Beard, John D. and Beck, Celeste and Graham, Randall and Packham, Steven C. and Traphagan, Monica and Giles, Rebecca T. and Morgan, John G.},
  year = 2012,
  month = oct,
  journal = {Environmental Health Perspectives},
  volume = {120},
  number = {10},
  pages = {1385--1390},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/ehp.1104349},
  urldate = {2020-11-05},
  abstract = {Background: Winter temperature inversions---layers of air in which temperature increases with altitude---trap air pollutants and lead to higher pollutant concentrations. Previous studies have evaluated associations between pollutants and emergency department (ED) visits for asthma, but none have considered inversions as independent risk factors for ED visits for asthma. Objective: We aimed to assess associations between winter inversions and ED visits for asthma in Salt Lake County, Utah. Methods: We obtained electronic records of ED visits for asthma and data on inversions, weather, and air pollutants for Salt Lake County, Utah, during the winters of 2003 through 2004 to 2007 through 2008. We identified 3,425 ED visits using a primary diagnosis of asthma. We used a timestratified case-crossover design, and conditional logistic regression models to calculate odds ratios (ORs) and 95\% confidence intervals (CIs) to estimate rate ratios of ED visits for asthma in relation to inversions during a 4-day lag period and prolonged inversions. We evaluated interactions between inversions and weather and pollutants. Results: After adjusting for dew point and mean temperatures, the OR for ED visits for asthma associated with inversions 0--3 days before the visit compared with no inversions during the lag period was 1.14 (95\% CI: 1.00, 1.30). The OR for each 1-day increase in the number of inversion days during the lag period was 1.03 (95\% CI: 1.00, 1.07). Associations were only apparent when PM10 and maximum and mean temperatures were above median levels. Conclusions: Our results provide evidence that winter inversions are associated with increased rates of ED visits for asthma.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{bell_time-series_2004,
  title = {Time-{{Series Studies}} of {{Particulate Matter}}},
  author = {Bell, Michelle L. and Samet, Jonathan M. and Dominici, Francesca},
  year = 2004,
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {25},
  number = {1},
  pages = {247--280},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.25.102802.124329},
  urldate = {2021-09-08},
  langid = {english}
}

@article{bell_time-series_2004,
  title = {Time-{{Series Studies}} of {{Particulate Matter}}},
  author = {Bell, Michelle L. and Samet, Jonathan M. and Dominici, Francesca},
  year = 2004,
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {25},
  number = {1},
  pages = {247--280},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.25.102802.124329},
  urldate = {2022-11-17},
  abstract = {Studies of air pollution and human health have evolved from descriptive studies of the early phenomena of large increases in adverse health effects following extreme air pollution episodes to time-series analyses based on the use of sophisticated regression models. In fact, advanced statistical methods are necessary to address the challenges inherent in the detection of a relatively small pollution risk in the presence of potential confounders. This paper reviews the history, methods, and findings of the time-series studies estimating health risks associated with short-term exposure to particulate matter (PM), though much of the discussion is applicable to epidemiological studies of air pollution in general. We review the critical role of epidemiological studies in setting regulatory standards and the history of PM epidemiology and time-series analysis. We also summarize recent time-series results and conclude with a discussion of current and future directions of time-series analysis of particulates, including research on mortality displacement and the resolution of results from cohort and time-series studies.},
  langid = {english}
}

@article{bell2004time,
  title = {Time-Series Studies of Particulate Matter},
  author = {Bell, Michelle L and Samet, Jonathan M and Dominici, Francesca},
  year = 2004,
  journal = {Annual Review of Public Health},
  volume = {25},
  pages = {247--280},
  publisher = {Annual Reviews}
}

@article{benmarhnia_les_2017,
  title = {{Les approches quasi exp\'erimentales d'une nouvelle g\'en\'eration d'\'etudes pour \'evaluer les effets des politiques de lutte contre la pollution atmosph\'erique}},
  author = {Benmarhnia, Tarik},
  year = 2017,
  journal = {Pollution atmosph\'erique},
  number = {N{$^\circ$}235 Juillet - Septembre 2017},
  issn = {2268-3798, 0032-3632},
  doi = {10.4267/pollution-atmospherique.6356},
  urldate = {2020-11-05},
  abstract = {Impact evaluation aims at providing evidence about the potential effectiveness of a public policy that can be applied in the context or air pollution reduction and related health benefits. This article aims at introducing the notion of causal inference and the counterfactual framework in the context of public health policy for outdoor air quality. We introduce the notion of quasi experiment which is rarely used in this context. We thus highlight the use of instrumental variables and regression discontinuity methods in order to encourage further applications of these methods for a new generation of impact evaluation studies in the context of public health policy for outdoor air quality.},
  langid = {french},
  keywords = {Lit review air pollution health effects,Statistical power (Leo)}
}

@article{benmarhnia_using_nodate,
  title = {Using {{Instrumental Variables}} under {{Partial Observability}} of {{Endogenous Variables}} for {{Assessing Effects}} of {{Air Pollution}} on {{Health}}},
  author = {Benmarhnia, Tarik and Bharadwaj, Prashant and Romero, Mauricio},
  pages = {20},
  langid = {english}
}

@article{bennett_how_2020,
  title = {How {{Far}} Is {{Too Far}}? {{Estimation}} of an {{Interval}} for {{Generalization}} of a {{Regression Discontinuity Design Away}} from the {{Cutoff}}},
  author = {Bennett, Magdalena},
  year = 2020,
  month = mar,
  pages = {50},
  abstract = {Regression discontinuity designs are a commonly used approach for causal inference in observational studies. Under mild continuity assumptions, the method provides a robust estimate of the average treatment effect for observations directly at the threshold of assignment. However, it has limited external validity for populations away from the cutoff. This article proposes a strategy to overcome this limitation by identifying a wider interval around the cutoff for estimation using a Generalization of a Regression Discontinuity Design (GRD). In this interval, predictive covariates are used to explain away the relationship between the assignment score and the outcome of interest for the pre-intervention period. Under the partially-testable assumption of conditional timeinvariance in absence of the treatment, the generalization bandwidth can be applied to the post-intervention period, allowing for the estimation of average treatment effects for populations away from the cutoff. To illustrate this method, GRD is applied in the context of free higher education in Chile to estimate effects for vulnerable students.},
  langid = {english},
  keywords = {RDD}
}

@article{beverland_comparison_2012,
  title = {Comparison of Models for Estimation of Long-Term Exposure to Air Pollution in Cohort Studies},
  author = {Beverland, I. J. and Robertson, C. and Yap, C. and Heal, M. R. and Cohen, G. R. and Henderson, D. E. J. and Hart, C. L. and Agius, R. M.},
  year = 2012,
  month = dec,
  journal = {Atmospheric Environment},
  volume = {62},
  pages = {530--539},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2012.08.001},
  urldate = {2020-08-03},
  abstract = {This study compared three spatio-temporal models for estimation of exposure to air pollution throughout the central part of Scotland during 1970--79 for approximately 21,600 individuals in 2 closely-related prospective cohort studies. Although 181 black smoke (BS) monitoring sites operated in this region at some point during 1970--79, a substantial amount of BS exposure data was missing at many sites. The three exposure estimation methods were: (i) area-based regression models to impute missing data followed by assignment of exposure by inverse distance weighting of observed BS at nearby monitoring sites (IDWBS); (ii) area-based regression models to impute missing data followed by a spatial regression additive model using four local air quality predictors (LAQP): altitude; distance to the nearest major road; household density within a 250~m buffer zone; and distance to the edge of urban boundary (AMBS); (iii) a multilevel spatio-temporal model using LAQP (MultiBS). The three methods were evaluated using maps of predicted BS, and cross validation using monitored and imputed BS at sites with {$\geq$}80\% data. The use of LAQP in the AMBS and MultiBS exposure models provided spatial patterns in BS consistent with known sources of BS associated with major roads and the centre of urban areas. Cross-validation analyses demonstrated that the MultiBS model provided more precise predictions (R2~=~60\%) of decadal geometric mean BS concentrations at monitoring sites compared with the IDWBS and AMBS models (R2 of 19\% and 20\%, respectively).},
  langid = {english},
  keywords = {Black smoke,Cohort,Exposure estimation,Multi-level model}
}

@article{bind_causal_2019,
  title = {Causal {{Modeling}} in {{Environmental Health}}},
  author = {Bind, Marie-Ab{\`e}le},
  year = 2019,
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {40},
  number = {1},
  pages = {23--43},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-040218-044048},
  urldate = {2021-06-10},
  abstract = {The field of environmental health has been dominated by modeling associations, especially by regressing an observed outcome on a linear or nonlinear function of observed covariates. Readers interested in advances in policies for improving environmental health are, however, expecting to be informed about health effects resulting from, or more explicitly caused by, environmental exposures. The quantification of health impacts resulting from the removal of environmental exposures involves causal statements. Therefore, when possible, causal inference frameworks should be considered for analyzing the effects of environmental exposures on health outcomes.},
  langid = {english}
}

@article{bind_causal_2019,
  title = {Causal {{Modeling}} in {{Environmental Health}}},
  author = {Bind, Marie-Ab{\`e}le},
  year = 2019,
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {40},
  number = {1},
  pages = {23--43},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-040218-044048},
  urldate = {2022-11-17},
  abstract = {The field of environmental health has been dominated by modeling associations, especially by regressing an observed outcome on a linear or nonlinear function of observed covariates. Readers interested in advances in policies for improving environmental health are, however, expecting to be informed about health effects resulting from, or more explicitly caused by, environmental exposures. The quantification of health impacts resulting from the removal of environmental exposures involves causal statements. Therefore, when possible, causal inference frameworks should be considered for analyzing the effects of environmental exposures on health outcomes.},
  langid = {english}
}

@techreport{black_effect_2019,
  title = {The {{Effect}} of {{Health Insurance}} on {{Mortality}}: {{Power Analysis}} and {{What We Can Learn}} from the {{Affordable Care Act Coverage Expansions}}},
  shorttitle = {The {{Effect}} of {{Health Insurance}} on {{Mortality}}},
  author = {Black, Bernard and Hollingsworth, Alex and Nunes, Leticia and Simon, Kosali},
  year = 2019,
  month = feb,
  number = {w25568},
  pages = {w25568},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w25568},
  urldate = {2020-11-05},
  abstract = {A large literature examines the effect of health insurance on mortality. We contribute by emphasizing two challenges in using the Affordable Care Act (ACA)'s quasi-experimental variation to study mortality. The first is non-parallel pretreatment trends. Rising mortality in Medicaid non-expansion relative to expansion states prior to Medicaid expansion makes it difficult to estimate the effect of insurance using difference-in-differences (DD). We use various DD, triple difference, age-discontinuity and synthetic control approaches, but are unable to satisfactorily address this concern. Our estimates are not statistically significant, but are imprecise enough to be consistent with both no effect and a large effect of insurance on amenable mortality over the first three post-ACA years. Thus, our results should not be interpreted as evidence that health insurance has no effect on mortality for this age group, especially in light of the literature documenting greater health care use as a result of the ACA. Second, we provide a simulationbased power analysis, showing that even the nationwide natural experiment provided by the ACA is underpowered to detect plausibly sized mortality effects in available datasets, and discuss data needs for the literature to advance. Our simulated pseudo-shocks power analysis approach is broadly applicable to other natural-experiment studies.},
  langid = {english},
  keywords = {Statistical power (Leo)}
}

@techreport{black_simulated_2019,
  title = {Simulated {{Power Analyses}} for {{Observational Studies}}: {{An Application}} to the {{Affordable Care Act Medicaid Expansion}}},
  shorttitle = {Simulated {{Power Analyses}} for {{Observational Studies}}},
  author = {Black, Bernard and Hollingsworth, Alex and Nunes, Leticia and Simon, Kosali},
  year = 2019,
  month = feb,
  number = {w25568},
  pages = {w25568},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w25568},
  urldate = {2021-10-01},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate. An analysis with low power is unlikely to produce convincing evidence of a treatment effect even when one exists. Of greater concern, a statistically significant estimate from a low-powered analysis is likely to overstate the magnitude of the true effect size, often finding estimates of the wrong sign or that are several times too large. Yet statistical power is rarely reported in published economics work. This is in part because modern research designs are complex enough that power cannot always be easily ascertained using simple formulae. Power can also be difficult to estimate in observational settings where researchers may not know---and have no ability to manipulate---the true treatment effect or other parameters of interest. Using an ap-plied example---the link between gaining health insurance and mortality---we conduct a simulated power analysis to outline the importance of power and ways to estimate power in complex research settings. We find that standard difference-in-differences and triple differences analyses of Medicaid expansions using county or state mortality data would need to induce reductions in population mortality of at least 2\% to be well powered. While there is no single, correct method for conducting a simulated power analysis, our manuscript outlines decisions relevant for applied researchers interested in conducting simulations appropriate to other settings.},
  langid = {english},
  keywords = {Power,Simulations,To read}
}

@techreport{black_simulated_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {Simulated {{Power Analyses}} for {{Observational Studies}}: {{An Application}} to the {{Affordable Care Act Medicaid Expansion}}},
  shorttitle = {Simulated {{Power Analyses}} for {{Observational Studies}}},
  author = {Black, Bernard S. and Hollingsworth, Alex and Nunes, Leticia and Simon, Kosali Ilayperuma},
  year = 2021,
  month = mar,
  number = {ID 3368187},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.3368187},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate.An analysis with low power is unlikely to produce convincing evidence of atreatment effect even when one exists. Of greater concern, a statistically significantestimate from a low-powered analysis is likely to overstate the magnitude of the trueeffect size, often finding estimates of the wrong sign or that are several times toolarge. Yet statistical power is rarely reported in published economics work. This is inpart because modern research designs are complex enough that power cannot alwaysbe easily ascertained using simple formulae. Power can also be difficult to estimatein observational settings where researchers may not know---and have no ability tomanipulate---the true treatment effect or other parameters of interest. Using an appliedexample---the link between gaining health insurance and mortality---we conducta simulated power analysis to outline the importance of power and ways to estimatepower in complex research settings. We find that standard difference-in-differencesand triple differences analyses of Medicaid expansions using county or state mortalitydata would need to induce reductions in population mortality of at least 2\% to be wellpowered. While there is no single, correct method for conducting a simulated poweranalysis, our manuscript outlines decisions relevant for applied researchers interestedin conducting simulations appropriate to other settings.},
  langid = {english},
  keywords = {Event study}
}

@article{black_simulated_2022,
  title = {Simulated Power Analyses for Observational Studies: {{An}} Application to the {{Affordable Care Act Medicaid}} Expansion},
  shorttitle = {Simulated Power Analyses for Observational Studies},
  author = {Black, Bernard and Hollingsworth, Alex and Nunes, Let{\'i}cia and Simon, Kosali},
  year = 2022,
  month = sep,
  journal = {Journal of Public Economics},
  volume = {213},
  pages = {104713},
  issn = {0047-2727},
  doi = {10.1016/j.jpubeco.2022.104713},
  urldate = {2023-04-13},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate. An analysis with low power is unlikely to produce convincing evidence of a treatment effect even when one exists. Of greater concern, a statistically significant estimate from a low-powered analysis is likely to misstate the true effect size, including finding estimates of the wrong sign or that are several times too large. Yet statistical power is rarely reported in published economics work. This is in part because many modern research designs are complex enough that power cannot be easily ascertained using simple formulae. Power can also be difficult to estimate in observational settings. Using an applied example--the link between gaining health insurance and mortality--we conduct a simulated power analysis to outline the importance of power and ways to estimate power in complex research settings. We find that standard difference-in-differences and triple differences analyses of Medicaid expansions using county or state mortality data would need to induce reductions in population mortality of at least 2\% to be well powered. While there is no single, correct method for conducting a simulated power analysis, our manuscript outlines how applied researchers can conduct simulations appropriate to their settings.},
  langid = {english},
  keywords = {Health insurance,Medicaid expansion,Mortality,Simulated power analysis}
}

@article{black_simulated_2022,
  title = {Simulated Power Analyses for Observational Studies: {{An}} Application to the {{Affordable Care Act Medicaid}} Expansion},
  shorttitle = {Simulated Power Analyses for Observational Studies},
  author = {Black, Bernard and Hollingsworth, Alex and Nunes, Let{\'i}cia and Simon, Kosali},
  year = 2022,
  month = sep,
  journal = {Journal of Public Economics},
  volume = {213},
  pages = {104713},
  issn = {00472727},
  doi = {10.1016/j.jpubeco.2022.104713},
  urldate = {2022-11-17},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate. An analysis with low power is unlikely to produce convincing evidence of a treatment effect even when one exists. Of greater concern, a statistically significant estimate from a low-powered analysis is likely to misstate the true effect size, including finding estimates of the wrong sign or that are several times too large. Yet statistical power is rarely reported in published economics work. This is in part because many modern research designs are complex enough that power cannot be easily ascertained using simple formulae. Power can also be difficult to estimate in observational settings. Using an applied example--the link between gaining health insurance and mortality--we conduct a simulated power analysis to outline the importance of power and ways to estimate power in complex research settings. We find that standard difference-indifferences and triple differences analyses of Medicaid expansions using county or state mortality data would need to induce reductions in population mortality of at least 2\% to be well powered. While there is no single, correct method for conducting a simulated power analysis, our manuscript outlines how applied researchers can conduct simulations appropriate to their settings.},
  langid = {english}
}

@article{blumberg_causal_2016,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}: {{Book Reviews}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Blumberg, Carol Joyce},
  year = 2016,
  month = apr,
  journal = {International Statistical Review},
  volume = {84},
  number = {1},
  pages = {159--159},
  issn = {03067734},
  doi = {10.1111/insr.12170},
  langid = {english}
}

@article{boaz_multivariate_2019,
  title = {Multivariate Air Pollution Prediction Modeling with Partial Missingness},
  author = {Boaz, R. M. and Lawson, A. B. and Pearce, J. L.},
  year = 2019,
  journal = {Environmetrics},
  volume = {30},
  number = {7},
  pages = {e2592},
  issn = {1099-095X},
  doi = {10.1002/env.2592},
  urldate = {2020-10-09},
  abstract = {Missing observations from air pollution monitoring networks has posed a longstanding problem for health investigators of air pollution. Growing interest in mixtures of air pollutants has further complicated this problem, as many new challenges have arisen that require development of novel methods. The objective of this study is to develop a methodology for multivariate prediction of air pollution. We focus specifically on tackling different forms of missing data such as spatial (sparse sites), outcome (pollutants not measured at some sites), and temporal (varieties of interrupted time series). To address these challenges, we develop a novel multivariate fusion framework, which leverages the observed interpollutant correlation structure to reduce error in the simultaneous prediction of multiple air pollutants. Our joint fusion model employs predictions from the Environmental Protection Agency's Community Multiscale Air Quality model along with spatiotemporal error terms. We have implemented our models on both simulated data and a case study in South Carolina for eight pollutants over a 28-day period in June 2006. We found that our model, which uses a multivariate correlated error in a Bayesian framework, showed promising predictive accuracy particularly for gaseous pollutants.},
  copyright = {\copyright{} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Air pollution,Health,Imputation,Imputation method,Missing data,US}
}

@article{boogaard_impact_2012,
  title = {Impact of Low Emission Zones and Local Traffic Policies on Ambient Air Pollution Concentrations},
  author = {Boogaard, Hanna and Janssen, Nicole A.H. and Fischer, Paul H. and Kos, Gerard P.A. and Weijers, Ernie P. and Cassee, Flemming R. and {van der Zee}, Saskia C. and {de Hartog}, Jeroen J. and Meliefste, Kees and Wang, Meng and Brunekreef, Bert and Hoek, Gerard},
  year = 2012,
  month = oct,
  journal = {Science of The Total Environment},
  volume = {435--436},
  pages = {132--140},
  issn = {00489697},
  doi = {10.1016/j.scitotenv.2012.06.089},
  urldate = {2020-06-03},
  abstract = {Background: Evaluations of the effectiveness of air pollution policy interventions are scarce. This study investigated air pollution at street level before and after implementation of local traffic policies including low emission zones (LEZ) directed at heavy duty vehicles (trucks) in five Dutch cities. Methods: Measurements of PM10, PM2.5, `soot', NO2, NOx, and elemental composition of PM10 and PM2.5 were conducted simultaneously at eight streets, six urban background locations and four suburban background locations before (2008) and two years after implementation of the policies (2010). The four suburban locations were selected as control locations to account for generic air pollution trends and weather differences. Results: All pollutant concentrations were lower in 2010 than in 2008. For traffic-related pollutants including `soot' and NOx and elemental composition (Cr, Cu, Fe) the decrease did not differ significantly between the intervention locations and the suburban control locations. Only for PM2.5 reductions were considerably larger at urban streets (30\%) and urban background locations (27\%) than at the matching suburban control locations (20\%). In one urban street where traffic intensity was reduced with 50\%, `soot', NOx and NO2 concentrations were reduced substantially more (41, 36 and 25\%) than at the corresponding suburban control location (22, 14 and 7\%). Conclusion: With the exception of one urban street where traffic flows were drastically reduced, the local traffic policies including LEZ were too modest to produce significant decreases in traffic-related air pollution concentrations.},
  langid = {english}
}

@article{bowers_randomization_2010,
  title = {The Randomization Mode of Statistical Inference.},
  author = {Bowers, Jake and Panagopoulos, Costas},
  year = 2010,
  month = aug,
  pages = {27},
  abstract = {How should one estimate and test comparative effects from a field experiment of only 8 units? What does statistical inference mean in this context? In a randomized experiment the most basic and important inference is between the treatments: after all, the point of randomizing is to allow us to say how the treatment group would have behaved had treatment been withheld.},
  langid = {english}
}

@book{box_time_2015,
  title = {Time {{Series Analysis}}: {{Forecasting}} and {{Control}}},
  shorttitle = {Time {{Series Analysis}}},
  author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
  year = 2015,
  month = may,
  publisher = {John Wiley \& Sons},
  abstract = {Praise for the Fourth Edition  "The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series, and by developing a complete approach to model building, estimation, forecasting and control."---Mathematical Reviews Bridging classical models and modern topics, the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing the latest developments that have occurred in the field over the past decade through applications from areas such as business, finance, and engineering, the Fifth Edition continues to serve as one of the most influential and prominent works on the subject. Time Series Analysis: Forecasting and Control, Fifth Edition provides a clearly written exploration of the key methods for building, classifying, testing, and analyzing stochastic models for time series and describes their use in five important areas of application: forecasting; determining the transfer function of a system; modeling the effects of intervention events; developing multivariate dynamic models; and designing simple control schemes. Along with these classical uses, the new edition covers modern topics with new features that include:  A redesigned chapter on multivariate time series analysis with an expanded treatment of Vector Autoregressive, or VAR models, along with a discussion of the analytical tools needed for modeling vector time series An expanded chapter on special topics covering unit root testing, time-varying volatility models such as ARCH and GARCH, nonlinear time series models, and long memory models Numerous examples drawn from finance, economics, engineering, and other related fields The use of the publicly available R software for graphical illustrations and numerical calculations along with scripts that demonstrate the use of R for model building and forecasting Updates to literature references throughout and new end-of-chapter exercises Streamlined chapter introductions and revisions that update and enhance the exposition  Time Series Analysis: Forecasting and Control, Fifth Edition is a valuable real-world reference for researchers and practitioners in time series analysis, econometrics, finance, and related fields. The book is also an excellent textbook for beginning graduate-level courses in advanced statistics, mathematics, economics, finance, engineering, and physics.},
  googlebooks = {rNt5CgAAQBAJ},
  isbn = {978-1-118-67492-5},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{brewer_inference_2018,
  title = {Inference with {{Difference-in-Differences Revisited}}},
  author = {Brewer, Mike and Crossley, Thomas F. and Joyce, Robert},
  year = 2018,
  month = jan,
  journal = {Journal of Econometric Methods},
  volume = {7},
  number = {1},
  publisher = {De Gruyter},
  issn = {2156-6674},
  doi = {10.1515/jem-2017-0005},
  abstract = {A growing literature on inference in difference-in-differences (DiD) designs has been pessimistic about obtaining hypothesis tests of the correct size, particularly with few groups. We provide Monte Carlo evidence for four points: (i) it is possible to obtain tests of the correct size even with few groups, and in many settings very straightforward methods will achieve this; (ii) the main problem in DiD designs with grouped errors is instead low power to detect real effects; (iii) feasible GLS estimation combined with robust inference can increase power considerably whilst maintaining correct test size -- again, even with few groups, and (iv) using OLS with robust inference can lead to a perverse relationship between power and panel length.},
  langid = {english},
  keywords = {DID}
}

@article{broderick_automatic_2021,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  year = 2021,
  month = nov,
  journal = {arXiv:2011.14999 [econ, stat]},
  eprint = {2011.14999},
  primaryclass = {econ, stat},
  abstract = {We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1\% of the sample.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology}
}

@misc{brodeur_mass_2024,
  type = {{{SSRN Scholarly Paper}}},
  title = {Mass {{Reproducibility}} and {{Replicability}}: {{A New Hope}}},
  shorttitle = {Mass {{Reproducibility}} and {{Replicability}}},
  author = {Brodeur, Abel and Mikola, Derek and Cook, Nikolai},
  year = 2024,
  month = apr,
  number = {4790780},
  eprint = {4790780},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2026-01-29},
  abstract = {This study pushes our understanding of research reliability by reproducing and replicating claims from 110 papers in leading economic and political science journals. The analysis involves computational reproducibility checks and robustness assessments. It reveals several patterns. First, we uncover a high rate of fully computationally reproducible results (over 85\%). Second, excluding minor issues like missing packages or broken pathways, we uncover coding errors for about 25\% of studies, with some studies containing multiple errors. Third, we test the robustness of the results to 5,511 re-analyses. We find a robustness reproducibility of about 70\%. Robustness reproducibility rates are relatively higher for re-analyses that introduce new data and lower for re-analyses that change the sample or the definition of the dependent variable. Fourth, 52\% of re-analysis effect size estimates are smaller than the original published estimates and the average statistical significance of a re-analysis is 77\% of the original. Lastly, we rely on six teams of researchers working independently to answer eight additional research questions on the determinants of robustness reproducibility. Most teams find a negative relationship between replicators' experience and reproducibility, while finding no relationship between reproducibility and the provision of intermediate or even raw data combined with the necessary cleaning codes.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {economics,open science,political science,replication,reproduction,research transparency}
}

@article{brodeur_methods_2020,
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = 2020,
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  urldate = {2021-01-19},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal "revise and resubmit" process mitigates the problem; (iii) things are improving through time.},
  langid = {english},
  keywords = {and Selection,Hypothesis Testing: General,Model Evaluation,Sociology of Economics,Validation}
}

@article{brodeur_methods_2020,
  title = {Methods {{Matter}}: {{P-Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = 2020,
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal "revise and resubmit" process mitigates the problem; (iii) things are improving through time.},
  langid = {english},
  keywords = {and Selection,Hypothesis Testing: General,Model Evaluation,Sociology of Economics,Validation}
}

@article{brodeur_methods_2020,
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = 2020,
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  urldate = {2022-11-17},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal ``revise and resubmit'' process mitigates the problem; (iii) things are improving through time. (JEL A14, C12, C52)},
  langid = {english}
}

@article{brodeur_star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = 2016,
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  urldate = {2023-11-14},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  langid = {english},
  keywords = {Market for Economists Estimation: General,Role of Economics,Role of Economists}
}

@article{brodeur_star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = 2016,
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782, 1945-7790},
  doi = {10.1257/app.20150044},
  urldate = {2022-11-17},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10--20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing ``significant'' specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  langid = {english}
}

@article{bunnenberg_size_nodate,
  title = {Size and Power of Difference-in-Differences Studies in Financial Economics: {{An}} Approximate Permutation Test},
  author = {Bunnenberg, Sebastian and Meyer, Steffen},
  pages = {26},
  abstract = {Researchers use difference-in-differences models to evaluate the causal effects of policy changes. As the empirical correlation across firms and time is usually unknown, estimating consistent standard errors is difficult and statistical inferences may be biased. We suggest an approximate permutation test using simulated interventions to reveal the empirical error distribution of estimated policy effects. In contrast to existing econometric corrections, such as single- or double-clustering, our approach does not impose any parametric form on the data. In comparison to alternative parametric tests, our procedure maintains correct size with simulated and real-world interventions. Simultaneously, it improves power.},
  langid = {english}
}

@article{button_power_2013,
  title = {Power Failure: {{Why}} Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = 2013,
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Molecular neuroscience}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = 2013,
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2022-11-17},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  langid = {english}
}

@article{button_power_2013a,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = 2013,
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2020-10-27},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{camerer_evaluating_2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = 2016,
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf0918},
  keywords = {Experiments,Replications}
}

@article{camerer_evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = 2018,
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2022-11-17},
  langid = {english}
}

@techreport{card_using_1993,
  type = {Working {{Paper}}},
  title = {Using {{Geographic Variation}} in {{College Proximity}} to {{Estimate}} the {{Return}} to {{Schooling}}},
  author = {Card, David},
  year = 1993,
  month = oct,
  series = {Working {{Paper Series}}},
  number = {4483},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w4483},
  abstract = {A convincing analysis of the causal link between schooling and earnings requires an exogenous source of variation in education outcomes. This paper explores the use of college proximity as an exogenous determinant of schooling. Analysis of the NLS Young Men Cohort reveals that men who grew up in local labor markets with a nearby college have significantly higher education and earnings than other men. The education and earnings gains are concentrated among men with poorly-educated parents -- men who would otherwise stop schooling at relatively low levels. When college proximity is taken as an exogenous determinant of schooling the implied instrumental variables estimates of the return to schooling are 25-60\% higher than conventional ordinary least squares estimates. Since the effect of a nearby college on schooling attainment varies by family background it is possible to test whether college proximity is a legitimately exogenous determinant of schooling. The results affirm that marginal returns to education among children of less-educated parents are as high and perhaps much higher than the rates of return estimated by conventional methods.}
}

@article{casey_coal-fired_2020,
  title = {Coal-Fired Power Plant Closures and Retrofits Reduce Asthma Morbidity in the Local Population},
  author = {Casey, Joan A. and Su, Jason G. and Henneman, Lucas R. F. and Zigler, Corwin and Neophytou, Andreas M. and Catalano, Ralph and Gondalia, Rahul and Chen, Yu-Ting and Kaye, Leanne and Moyer, Sarah S. and Combs, Veronica and Simrall, Grace and Smith, Ted and Sublett, James and Barrett, Meredith A.},
  year = 2020,
  month = may,
  journal = {Nature Energy},
  volume = {5},
  number = {5},
  pages = {365--366},
  issn = {2058-7546},
  doi = {10.1038/s41560-020-0622-9},
  urldate = {2020-06-03},
  langid = {english}
}

@article{casey_improved_2020,
  title = {Improved Asthma Outcomes Observed in the Vicinity of Coal Power Plant Retirement, Retrofit and Conversion to Natural Gas},
  author = {Casey, Joan A. and Su, Jason G. and Henneman, Lucas R. F. and Zigler, Corwin and Neophytou, Andreas M. and Catalano, Ralph and Gondalia, Rahul and Chen, Yu-Ting and Kaye, Leanne and Moyer, Sarah S. and Combs, Veronica and Simrall, Grace and Smith, Ted and Sublett, James and Barrett, Meredith A.},
  year = 2020,
  month = may,
  journal = {Nature Energy},
  volume = {5},
  number = {5},
  pages = {398--408},
  issn = {2058-7546},
  doi = {10.1038/s41560-020-0600-2},
  urldate = {2020-06-03},
  langid = {english}
}

@article{cattaneo_power_2019,
  title = {Power Calculations for Regression-Discontinuity Designs},
  author = {Cattaneo, Matias D. and Titiunik, Roc{\'i}o and {Vazquez-Bare}, Gonzalo},
  year = 2019,
  month = mar,
  journal = {The Stata Journal: Promoting communications on statistics and Stata},
  volume = {19},
  number = {1},
  pages = {210--245},
  issn = {1536-867X, 1536-8734},
  doi = {10.1177/1536867X19830919},
  abstract = {In this article, we introduce two commands, rdpow and rdsampsi, that conduct power calculations and survey sample selection when using local polynomial estimation and inference methods in regression-discontinuity designs. rdpow conducts power calculations using modern robust bias-corrected local polynomial inference procedures and allows for new hypothetical sample sizes and bandwidth selections, among other features. rdsampsi uses power calculations to compute the minimum sample size required to achieve a desired level of power, given estimated or user-supplied bandwidths, biases, and variances. Together, these commands are useful when devising new experiments or surveys in regression-discontinuity designs, which will later be analyzed using modern local polynomial techniques for estimation, inference, and falsification. Because our commands use the communitycontributed (and R) package rdrobust for the underlying bandwidths, biases, and variances estimation, all the options currently available in rdrobust can also be used for power calculations and sample-size selection, including preintervention covariate adjustment, clustered sampling, and many bandwidth selectors. Finally, we also provide companion R functions with the same syntax and capabilities.},
  langid = {english}
}

@misc{chabe-ferret_economists_2018,
  title = {An {{Economist}}'s {{Journey}}: {{Why}} p-Values Are {{Bad}} for {{Science}}},
  shorttitle = {An {{Economist}}'s {{Journey}}},
  author = {{Chab{\'e}-ferret}, Sylvain},
  year = 2018,
  month = jun,
  journal = {An Economist's Journey},
  urldate = {2020-12-11}
}

@article{chang_is_2022,
  title = {Is {{Economics Research Replicable}}? {{Sixty Published Papers From Thirteen Journals Say}} ``{{Often Not}}''},
  shorttitle = {Is {{Economics Research Replicable}}?},
  author = {Chang, Andrew C. and Li, Phillip},
  year = 2022,
  month = jul,
  journal = {Critical Finance Review},
  volume = {11},
  publisher = {Now Publishers, Inc.},
  issn = {2164-5744, 2164-5760},
  doi = {10.1561/104.00000053},
  abstract = {Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say ``Often Not''},
  langid = {english}
}

@article{chen_air_2018,
  title = {Air {{Pollution}}, {{Student Health}}, and {{School Absences}}: {{Evidence}} from {{China}}},
  shorttitle = {Air {{Pollution}}, {{Student Health}}, and {{School Absences}}},
  author = {Chen, Siyu and Guo, Chongshan and Huang, Xinfei},
  year = 2018,
  month = nov,
  journal = {Journal of Environmental Economics and Management},
  volume = {92},
  pages = {465--497},
  issn = {00950696},
  doi = {10.1016/j.jeem.2018.10.002},
  urldate = {2022-11-17},
  abstract = {Little is known about the impact of air pollution on school children in developing countries. This paper aims to fill this gap by quantifying the causal effects of air pollution on the health status and the school attendance of the Chinese students. We relate the arguably exogenous daily variation in air pollution-instrumented by the occurrence of temperature inversion-with student illnesses and absences from more than 3000 schools in Guangzhou City. We find a sizable deleterious effect of air pollution on school attendance through the health channel. The impact persists for at least four days and displays a monotonically increasing pattern. Notably, this harmful effect is non-negligible even when pollution levels are below the official standards for air quality in China, suggesting that the current ambient air quality standards in China are not low enough to protect students.},
  langid = {english}
}

@article{chen_effect_2018,
  title = {Effect of Air Quality Alerts on Human Health: A Regression Discontinuity Analysis in {{Toronto}}, {{Canada}}},
  shorttitle = {Effect of Air Quality Alerts on Human Health},
  author = {Chen, Hong and Li, Qiongsi and Kaufman, Jay S and Wang, Jun and Copes, Ray and Su, Yushan and Benmarhnia, Tarik},
  year = 2018,
  month = jan,
  journal = {The Lancet Planetary Health},
  volume = {2},
  number = {1},
  pages = {e19-e26},
  issn = {25425196},
  doi = {10.1016/S2542-5196(17)30185-7},
  urldate = {2020-11-05},
  abstract = {Background Ambient air pollution is a major health risk globally. To reduce adverse health effects on days when air pollution is high, government agencies worldwide have implemented air quality alert programmes. Despite their widespread use, little is known about whether these programmes produce any observable public-health benefits. We assessed the effectiveness of such programmes using a quasi-experimental approach.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{chen_effect_2018,
  title = {Effect of Air Quality Alerts on Human Health: A Regression Discontinuity Analysis in {{Toronto}}, {{Canada}}},
  shorttitle = {Effect of Air Quality Alerts on Human Health},
  author = {Chen, Hong and Li, Qiongsi and Kaufman, Jay S and Wang, Jun and Copes, Ray and Su, Yushan and Benmarhnia, Tarik},
  year = 2018,
  month = jan,
  journal = {The Lancet Planetary Health},
  volume = {2},
  number = {1},
  pages = {e19--e26},
  issn = {25425196},
  doi = {10.1016/S2542-5196(17)30185-7},
  urldate = {2022-11-17},
  abstract = {Background Ambient air pollution is a major health risk globally. To reduce adverse health effects on days when air pollution is high, government agencies worldwide have implemented air quality alert programmes. Despite their widespread use, little is known about whether these programmes produce any observable public-health benefits. We assessed the effectiveness of such programmes using a quasi-experimental approach.},
  langid = {english}
}

@article{cheung_mitigating_2020,
  title = {Mitigating the Air Pollution Effect? {{The}} Remarkable Decline in the Pollution-Mortality Relationship in {{Hong Kong}}},
  shorttitle = {Mitigating the Air Pollution Effect?},
  author = {Cheung, Chun Wai and He, Guojun and Pan, Yuhang},
  year = 2020,
  month = may,
  journal = {Journal of Environmental Economics and Management},
  volume = {101},
  pages = {102316},
  issn = {00950696},
  doi = {10.1016/j.jeem.2020.102316},
  urldate = {2022-11-17},
  abstract = {Using transboundary pollution from mainland China as an instrument, we show that air pollution leads to higher cardio-respiratory mortality in Hong Kong. However, the air pollution effect has dramatically decreased over the past two decades: before 2003, a 10unit increase in the Air Pollution Index could lead to a 3.1\% increase in monthly cardiorespiratory mortality, but this effect has declined to 0.5\% using recent data and is no longer statistically significant. Exploratory analyses suggest that a well-functioning medical system and immediate access to emergency services can help mitigate the contemporaneous effects of pollution on mortality.},
  langid = {english}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = 2018,
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  urldate = {2022-01-26},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  langid = {english},
  keywords = {Market for Economists Methodological Issues: General Higher Education,Research Institutions,Role of Economics,Role of Economists}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = 2018,
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  langid = {english},
  keywords = {Higher Education,Market for Economists,Methodological Issues: General,Research Institutions,Role of Economics,Role of Economists}
}

@article{cinelli_making_2020,
  title = {Making Sense of Sensitivity: {{Extending}} Omitted Variable Bias},
  shorttitle = {Making Sense of Sensitivity},
  author = {Cinelli, Carlos and Hazlett, Chad},
  year = 2020,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {82},
  number = {1},
  pages = {39--67},
  issn = {1467-9868},
  doi = {10.1111/rssb.12348},
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as `extreme scenarios'. Finally, we describe problems with a common `benchmarking' practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  copyright = {\copyright{} 2019 Royal Statistical Society},
  langid = {english},
  keywords = {Omitted variable bias,Statistics}
}

@article{cinelli_making_2020,
  title = {Making Sense of Sensitivity: Extending Omitted Variable Bias},
  shorttitle = {Making Sense of Sensitivity},
  author = {Cinelli, Carlos and Hazlett, Chad},
  year = 2020,
  month = feb,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {82},
  number = {1},
  pages = {39--67},
  issn = {13697412},
  doi = {10.1111/rssb.12348},
  urldate = {2022-11-17},
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting.The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as `extreme scenarios'. Finally, we describe problems with a common `benchmarking' practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  langid = {english}
}

@article{cochran_planning_2022,
  title = {The {{Planning}} of {{Observational Studies}} of {{Human Populations}}},
  author = {Cochran, W G},
  year = 2022,
  pages = {34},
  langid = {english}
}

@article{cook_waiting_2008,
  title = {``{{Waiting}} for {{Life}} to {{Arrive}}'': {{A}} History of the Regression-Discontinuity Design in {{Psychology}}, {{Statistics}} and {{Economics}}},
  shorttitle = {``{{Waiting}} for {{Life}} to {{Arrive}}''},
  author = {Cook, Thomas D.},
  year = 2008,
  month = feb,
  journal = {Journal of Econometrics},
  series = {The Regression Discontinuity Design: {{Theory}} and Applications},
  volume = {142},
  number = {2},
  pages = {636--654},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2007.05.002},
  abstract = {This paper reviews the history of the regression discontinuity design in three academic disciplines. It describes the design's birth and subsequent demise in Psychology even though most problems with it had been solved there. It further describes the scant interest shown in the design by scholars formally trained in Statistics, and the design's poor reception in Economics from 1972 until about 1995, when its profile and acceptance changed. Reasons are given for this checkered history that is characterized as waiting for life to arrive.},
  langid = {english},
  keywords = {History,RDD}
}

@article{cooperman_randomization_2017,
  title = {Randomization {{Inference}} with {{Rainfall Data}}: {{Using Historical Weather Patterns}} for {{Variance Estimation}}},
  shorttitle = {Randomization {{Inference}} with {{Rainfall Data}}},
  author = {Cooperman, Alicia Dailey},
  year = 2017,
  month = jul,
  journal = {Political Analysis},
  volume = {25},
  number = {3},
  pages = {277--288},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.17},
  abstract = {Many recent papers in political science and economics use rainfall as a strategy to facilitate causal inference. Rainfall shocks are as-if randomly assigned, but the assignment of rainfall by county is highly correlated across space. Since clustered assignment does not occur within well-defined boundaries, it is challenging to estimate the variance of the effect of rainfall on political outcomes. I propose using randomization inference with historical weather patterns from 73 years as potential randomizations. I replicate the influential work on rainfall and voter turnout in presidential elections in the United States by Gomez, Hansford, and Krause (2007) and compare the estimated average treatment effect (ATE) to a sampling distribution of estimates under the sharp null hypothesis of no effect. The alternate randomizations are random draws from national rainfall patterns on election and would-be election days, which preserve the clustering in treatment assignment and eliminate the need to simulate weather patterns or make assumptions about unit boundaries for clustering. I find that the effect of rainfall on turnout is subject to greater sampling variability than previously estimated using conventional standard errors.},
  langid = {english}
}

@article{cooperman2017randomization,
  title = {Randomization Inference with Rainfall Data: {{Using}} Historical Weather Patterns for Variance Estimation},
  author = {Cooperman, Alicia Dailey},
  year = 2017,
  journal = {Political Analysis},
  volume = {25},
  number = {3},
  pages = {277--288},
  publisher = {Cambridge University Press}
}

@book{cunningham_causal_2021,
  title = {Causal {{Inference}}: {{The Mixtape}}},
  shorttitle = {Causal {{Inference}}},
  author = {Cunningham, Scott},
  year = 2021,
  month = jan,
  publisher = {Yale University Press},
  doi = {10.2307/j.ctv1c29t27},
  isbn = {978-0-300-25588-1 978-0-300-25168-5},
  langid = {english},
  keywords = {Causal inference,Handbook,Simulations,Statistics}
}

@article{currie_environmental_2015,
  title = {Environmental {{Health Risks}} and {{Housing Values}}: {{Evidence}} from 1,600 {{Toxic Plant Openings}} and {{Closings}}},
  shorttitle = {Environmental {{Health Risks}} and {{Housing Values}}},
  author = {Currie, Janet and Davis, Lucas and Greenstone, Michael and Walker, Reed},
  year = 2015,
  month = feb,
  journal = {American Economic Review},
  volume = {105},
  number = {2},
  pages = {678--709},
  issn = {0002-8282},
  doi = {10.1257/aer.20121656},
  abstract = {Regulatory oversight of toxic emissions from industrial plants and understanding about these emissions' impacts are in their infancy. Applying a research design based on the openings and closings of 1,600 industrial plants to rich data on housing markets and infant health, we find that: toxic air emissions affect air quality only within 1 mile of the plant; plant openings lead to 11 percent declines in housing values within 0.5 mile or a loss of about \$4.25 million for these households; and a plant's operation is associated with a roughly 3 percent increase in the probability of low birthweight within 1 mile. (JEL I12, L60, Q52, Q53, Q58, R23, R31)},
  langid = {english},
  keywords = {Air Pollution,and Transportation Economics: Regional Migration,Distributional Effects,Employment Effects,Environmental Economics: Government Policy,Hazardous Waste,Health Behavior,Housing Supply and Markets,Industry Studies: Manufacturing: General,Neighborhood Characteristics,Noise,Pollution Control Adoption and Costs,Population,Real Estate,Recycling,Regional,Regional Labor Markets,Rural,Solid Waste,Urban,Water Pollution}
}

@article{currie2015environmental,
  title = {Environmental Health Risks and Housing Values: Evidence from 1,600 Toxic Plant Openings and Closings},
  author = {Currie, Janet and Davis, Lucas and Greenstone, Michael and Walker, Reed},
  year = 2015,
  journal = {American Economic Review},
  volume = {105},
  number = {2},
  pages = {678--709}
}

@techreport{de_chaisemartin_two-way_2022,
  type = {Working {{Paper}}},
  title = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}: {{A Survey}}},
  shorttitle = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = 2022,
  month = jan,
  series = {Working {{Paper Series}}},
  number = {29691},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w29691},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate policies' effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been show that those regressions may produce misleading estimates, if the policy's effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue, and that proposes alternative estimators robust to heterogeneous effects.}
}

@article{de_mesnard_pollution_2013,
  title = {Pollution Models and Inverse Distance Weighting: {{Some}} Critical Remarks},
  shorttitle = {Pollution Models and Inverse Distance Weighting},
  author = {{de Mesnard}, Louis},
  year = 2013,
  month = mar,
  journal = {Computers \& Geosciences},
  volume = {52},
  pages = {459--469},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2012.11.002},
  urldate = {2020-07-28},
  abstract = {When evaluating the impact of pollution, measurements from remote stations are often weighted by the inverse of distance raised to some nonnegative power (IDW). This is derived from Shepard's method of spatial interpolation (1968). The paper discusses the arbitrary character of the exponent of distance and the problem of monitoring stations that are close to the reference point. From elementary laws of physics, it is determined which exponent of distance should be chosen (or its upper bound) depending on the form of pollution encountered, such as radiant pollution (including radioactivity and sound), air pollution (plumes, puffs, and motionless clouds by using the classical Gaussian model), and polluted rivers. The case where a station is confused with the reference point (or zero distance) is also discussed: in real cases this station imposes its measurement on the whole area regardless of the measurements made by other stations. This is a serious flaw when evaluating the mean pollution of an area. However, it is shown that this is not so in the case of a continuum of monitoring stations, and the measurement at the reference point and for the whole area may differ, which is satisfactory.},
  langid = {english},
  keywords = {Air pollution,Imputation,Inverse distance weighting,Pollution physics}
}

@article{deaton_understanding_2018,
  title = {Understanding and Misunderstanding Randomized Controlled Trials},
  author = {Deaton, Angus and Cartwright, Nancy},
  year = 2018,
  month = aug,
  journal = {Social Science \& Medicine},
  series = {Randomized {{Controlled Trials}} and {{Evidence-based Policy}}: {{A Multidisciplinary Dialogue}}},
  volume = {210},
  pages = {2--21},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2017.12.005},
  abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding `external validity' is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not `what works', but `why things work'.},
  langid = {english},
  keywords = {Precision,RCT,Variance/bias trade off}
}

@article{dechezlepretre_economic_nodate,
  title = {{{THE ECONOMIC COST OF AIR POLLUTION}}: {{EVIDENCE FROM EUROPE ECONOMICS DEPARTMENT WORKING PAPERS No}}. 1584},
  author = {Dechezlepr{\^e}tre, Antoine and Rivers, Nicholas and Stadler, Balazs},
  pages = {62},
  abstract = {R\'esum\'e The economic cost of air pollution: Evidence from Europe This study provides the first evidence that air pollution causes economy-wide reductions in market economic activity based on data for Europe. The analysis combines satellite-based measures of air pollution with statistics on regional economic activity at the NUTS-3 level throughout the European Union over the period 2000-15. An instrumental variables approach based on thermal inversions is used to identify the causal impact of air pollution on economic activity. The estimates show that a 1{$\mu$}g/m3 increase in PM2.5 concentration (or a 10\% increase at the sample mean) causes a 0.8\% reduction in real GDP that same year. Ninety-five per cent of this impact is due to reductions in output per worker, which can occur through greater absenteeism at work or reduced labour productivity. Therefore, the results suggest that public policies to reduce air pollution may contribute positively to economic growth. Indeed, the large economic benefits from pollution reduction uncovered in the study compare with relatively small abatement costs. Thus, more stringent air quality regulations could be warranted based solely on economic grounds, even ignoring the large benefits in terms of avoided mortality.},
  langid = {english}
}

@article{decicca_when_2020,
  title = {When Good Fences Aren't Enough: {{The}} Impact of Neighboring Air Pollution on Infant Health},
  shorttitle = {When Good Fences Aren't Enough},
  author = {DeCicca, Philip and Malak, Natalie},
  year = 2020,
  month = jul,
  journal = {Journal of Environmental Economics and Management},
  volume = {102},
  pages = {102324},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2020.102324},
  urldate = {2020-06-15},
  abstract = {The Clean Air Interstate Rule (CAIR) mandated the reduction of power plant emissions in the eastern United States. Starting in March 2005, this policy greatly reduced exposure to a specific form of pollution---fine particulate matter---in neighboring downwind counties. Using data from the 1995 to 2013 waves of the U.S. Natality Detail Files, we investigate the impact of this reduced exposure on birth outcomes including premature birth, birth weight status and infant mortality. Our most consistent finding is that the CAIR reduced premature birth, particularly among women aged thirty-five and older, those who experienced clinically-designated ``risky'' pregnancies, and those who gave birth to female newborns. We also find a substantial reduction in infant mortality for newborns whose mothers experienced ``risky'' pregnancies. Taken together, our findings suggest that policy-induced reductions in exposure to fine particulate matter may lead to improved birth outcomes among those mothers and newborns most at risk.},
  langid = {english},
  keywords = {Air pollution,Infant health,Infant mortality,Low birth weight,Power plant,Premature birth}
}

@article{dehejia_causal_1999,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}},
  shorttitle = {Causal {{Effects}} in {{Nonexperimental Studies}}},
  author = {Dehejia, Rajeev H. and Wahba, Sadek},
  year = 1999,
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {448},
  pages = {1053--1062},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2669919},
  abstract = {This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables.}
}

@article{dellavigna_bottlenecks_2024,
  title = {Bottlenecks for {{Evidence Adoption}}},
  author = {DellaVigna, Stefano and Kim, Woojin and Linos, Elizabeth},
  year = 2024,
  month = aug,
  journal = {Journal of Political Economy},
  volume = {132},
  number = {8},
  pages = {2748--2789},
  publisher = {The University of Chicago Press},
  issn = {0022-3808},
  doi = {10.1086/729447},
  urldate = {2025-08-04},
  abstract = {Governments increasingly use randomized controlled trials (RCTs) to test innovations, yet we know little about how they incorporate results into policymaking. We study 30 US cities that ran 73 RCTs with a national nudge unit. Cities adopt a nudge treatment into their communications in 27\% of the cases. We find that the strength of the evidence and key city features do not strongly predict adoption; instead, the largest predictor is whether the RCT was implemented using preexisting communication, as opposed to new communication. We identify organizational inertia as a leading explanation: changes to preexisting infrastructure are more naturally folded into subsequent processes.}
}

@article{dellavigna_rcts_2022,
  title = {{{RCTs}} to {{Scale}}: {{Comprehensive Evidence From Two Nudge Units}}},
  shorttitle = {{{RCTs}} to {{Scale}}},
  author = {DellaVigna, Stefano and Linos, Elizabeth},
  year = 2022,
  journal = {Econometrica},
  volume = {90},
  number = {1},
  pages = {81--116},
  issn = {1468-0262},
  doi = {10.3982/ECTA18709},
  urldate = {2023-10-11},
  abstract = {Nudge interventions have quickly expanded from academic studies to larger implementation in so-called Nudge Units in governments. This provides an opportunity to compare interventions in research studies, versus at scale. We assemble a unique data set of 126 RCTs covering 23 million individuals, including all trials run by two of the largest Nudge Units in the United States. We compare these trials to a sample of nudge trials in academic journals from two recent meta-analyses. In the Academic Journals papers, the average impact of a nudge is very large---an 8.7 percentage point take-up effect, which is a 33.4\% increase over the average control. In the Nudge Units sample, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.0\% increase. We document three dimensions which can account for the difference between these two estimates: (i) statistical power of the trials; (ii) characteristics of the interventions, such as topic area and behavioral channel; and (iii) selective publication. A meta-analysis model incorporating these dimensions indicates that selective publication in the Academic Journals sample, exacerbated by low statistical power, explains about 70 percent of the difference in effect sizes between the two samples. Different nudge characteristics account for most of the residual difference.},
  copyright = {\copyright{} 2022 The Econometric Society},
  langid = {english},
  keywords = {Field experiment,notion,Nudges,Power,Publication bias,RCT}
}

@article{deryugina_mortality_2019,
  title = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}: {{Evidence}} from {{Changes}} in {{Wind Direction}}},
  shorttitle = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}},
  author = {Deryugina, Tatyana and Heutel, Garth and Miller, Nolan H. and Molitor, David and Reif, Julian},
  year = 2019,
  month = dec,
  journal = {American Economic Review},
  volume = {109},
  number = {12},
  pages = {4178--4219},
  issn = {0002-8282},
  doi = {10.1257/aer.20180279},
  urldate = {2020-06-05},
  abstract = {We estimate the causal effects of acute fine particulate matter exposure on mortality, health care use, and medical costs among the US elderly using Medicare data. We instrument for air pollution using changes in local wind direction and develop a new approach that uses machine learning to estimate the life-years lost due to pollution exposure. Finally, we characterize treatment effect heterogeneity using both life expectancy and generic machine learning inference. Both approaches find that mortality effects are concentrated in about 25 percent of the elderly population.},
  langid = {english},
  keywords = {Air pollution,Medicare,Mortality,Mortality displacement,US,Wind}
}

@article{deryugina_mortality_2019,
  title = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}: {{Evidence}} from {{Changes}} in {{Wind Direction}}},
  shorttitle = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}},
  author = {Deryugina, Tatyana and Heutel, Garth and Miller, Nolan H. and Molitor, David and Reif, Julian},
  year = 2019,
  month = dec,
  journal = {American Economic Review},
  volume = {109},
  number = {12},
  pages = {4178--4219},
  issn = {0002-8282},
  doi = {10.1257/aer.20180279},
  urldate = {2022-11-17},
  abstract = {We estimate the causal effects of acute fine particulate matter exposure on mortality, health care use, and medical costs among the US elderly using Medicare data. We instrument for air pollution using changes in local wind direction and develop a new approach that uses machine learning to estimate the life-years lost due to pollution exposure. Finally, we characterize treatment effect heterogeneity using both life expectancy and generic machine learning inference. Both approaches find that mortality effects are concentrated in about 25 percent of the elderly population. (JEL I12, J14, Q51, Q53)},
  langid = {english}
}

@article{devore_modern_2006,
  title = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}: {{Understanding Why}} and {{How}}},
  shorttitle = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Devore, Jay},
  year = 2006,
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {101},
  number = {473},
  pages = {393--394},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2006.s72},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Statistical power (Leo)}
}

@article{di_association_2017,
  title = {Association of {{Short-term Exposure}} to {{Air Pollution With Mortality}} in {{Older Adults}}},
  author = {Di, Qian and Dai, Lingzhen and Wang, Yun and Zanobetti, Antonella and Choirat, Christine and Schwartz, Joel D. and Dominici, Francesca},
  year = 2017,
  month = dec,
  journal = {JAMA},
  volume = {318},
  number = {24},
  pages = {2446},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.17923},
  urldate = {2021-06-10},
  abstract = {OBJECTIVE To estimate the association between short-term exposures to ambient fine particulate matter (PM2.5) and ozone, and at levels below the current daily NAAQS, and mortality in the continental United States. Editorial page 2431 Supplemental content CME Quiz at jamanetwork.com/learning and CME Questions page 2489 DESIGN, SETTING, AND PARTICIPANTS Case-crossover design and conditional logistic regression to estimate the association between short-term exposures to PM2.5 and ozone (mean of daily exposure on the same day of death and 1 day prior) and mortality in 2-pollutant models. The study included the entire Medicare population from January 1, 2000, to December 31, 2012, residing in 39 182 zip codes. EXPOSURES Daily PM2.5 and ozone levels in a 1-km \texttimes{} 1-km grid were estimated using published and validated air pollution prediction models based on land use, chemical transport modeling, and satellite remote sensing data. From these gridded exposures, daily exposures were calculated for every zip code in the United States. Warm-season ozone was defined as ozone levels for the months April to September of each year. MAIN OUTCOMES AND MEASURES All-cause mortality in the entire Medicare population from 2000 to 2012. RESULTS During the study period, there were 22 433 862 million case days and 76 143 209 control days. Of all case and control days, 93.6\% had PM2.5 levels below 25 {$\mu$}g/m3, during which 95.2\% of deaths occurred (21 353 817 of 22 433 862), and 91.1\% of days had ozone levels below 60 parts per billion, during which 93.4\% of deaths occurred (20 955 387 of 22 433 862). The baseline daily mortality rates were 137.33 and 129.44 (per 1 million persons at risk per day) for the entire year and for the warm season, respectively. Each short-term increase of 10 {$\mu$}g/m3 in PM2.5 (adjusted by ozone) and 10 parts per billion (10-9) in warm-season ozone (adjusted by PM2.5) were statistically significantly associated with a relative increase of 1.05\% (95\% CI, 0.95\%-1.15\%) and 0.51\% (95\% CI, 0.41\%-0.61\%) in daily mortality rate, respectively. Absolute risk differences in daily mortality rate were 1.42 (95\% CI, 1.29-1.56) and 0.66 (95\% CI, 0.53-0.78) per 1 million persons at risk per day. There was no evidence of a threshold in the exposure-response relationship. CONCLUSIONS AND RELEVANCE In the US Medicare population from 2000 to 2012, short-term exposures to PM2.5 and warm-season ozone were significantly associated with increased risk of mortality. This risk occurred at levels below current national air quality standards, suggesting that these standards may need to be reevaluated.},
  langid = {english}
}

@article{di_association_2017,
  title = {Association of {{Short-term Exposure}} to {{Air Pollution With Mortality}} in {{Older Adults}}},
  author = {Di, Qian and Dai, Lingzhen and Wang, Yun and Zanobetti, Antonella and Choirat, Christine and Schwartz, Joel D. and Dominici, Francesca},
  year = 2017,
  month = dec,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {318},
  number = {24},
  pages = {2446},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.17923},
  urldate = {2022-11-17},
  abstract = {OBJECTIVE To estimate the association between short-term exposures to ambient fine particulate matter (PM2.5) and ozone, and at levels below the current daily NAAQS, and mortality in the continental United States. Editorial page 2431 Supplemental content CME Quiz at jamanetwork.com/learning and CME Questions page 2489 DESIGN, SETTING, AND PARTICIPANTS Case-crossover design and conditional logistic regression to estimate the association between short-term exposures to PM2.5 and ozone (mean of daily exposure on the same day of death and 1 day prior) and mortality in 2-pollutant models. The study included the entire Medicare population from January 1, 2000, to December 31, 2012, residing in 39 182 zip codes. EXPOSURES Daily PM2.5 and ozone levels in a 1-km \texttimes{} 1-km grid were estimated using published and validated air pollution prediction models based on land use, chemical transport modeling, and satellite remote sensing data. From these gridded exposures, daily exposures were calculated for every zip code in the United States. Warm-season ozone was defined as ozone levels for the months April to September of each year. MAIN OUTCOMES AND MEASURES All-cause mortality in the entire Medicare population from 2000 to 2012. RESULTS During the study period, there were 22 433 862 million case days and 76 143 209 control days. Of all case and control days, 93.6\% had PM2.5 levels below 25 {$\mu$}g/m3, during which 95.2\% of deaths occurred (21 353 817 of 22 433 862), and 91.1\% of days had ozone levels below 60 parts per billion, during which 93.4\% of deaths occurred (20 955 387 of 22 433 862). The baseline daily mortality rates were 137.33 and 129.44 (per 1 million persons at risk per day) for the entire year and for the warm season, respectively. Each short-term increase of 10 {$\mu$}g/m3 in PM2.5 (adjusted by ozone) and 10 parts per billion (10-9) in warm-season ozone (adjusted by PM2.5) were statistically significantly associated with a relative increase of 1.05\% (95\% CI, 0.95\%-1.15\%) and 0.51\% (95\% CI, 0.41\%-0.61\%) in daily mortality rate, respectively. Absolute risk differences in daily mortality rate were 1.42 (95\% CI, 1.29-1.56) and 0.66 (95\% CI, 0.53-0.78) per 1 million persons at risk per day. There was no evidence of a threshold in the exposure-response relationship. CONCLUSIONS AND RELEVANCE In the US Medicare population from 2000 to 2012, short-term exposures to PM2.5 and warm-season ozone were significantly associated with increased risk of mortality. This risk occurred at levels below current national air quality standards, suggesting that these standards may need to be reevaluated.},
  langid = {english}
}

@article{dominici_best_2017,
  title = {Best {{Practices}} for {{Gauging Evidence}} of {{Causality}} in {{Air Pollution Epidemiology}}},
  author = {Dominici, Francesca and Zigler, Corwin},
  year = 2017,
  month = dec,
  journal = {American Journal of Epidemiology},
  volume = {186},
  number = {12},
  pages = {1303--1309},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwx307},
  urldate = {2021-06-10},
  langid = {english}
}

@article{dominici_best_2017,
  title = {Best {{Practices}} for {{Gauging Evidence}} of {{Causality}} in {{Air Pollution Epidemiology}}},
  author = {Dominici, Francesca and Zigler, Corwin},
  year = 2017,
  month = dec,
  journal = {American Journal of Epidemiology},
  volume = {186},
  number = {12},
  pages = {1303--1309},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwx307},
  urldate = {2022-11-17},
  langid = {english}
}

@article{dorie_flexible_2016,
  title = {A Flexible, Interpretable Framework for Assessing Sensitivity to Unmeasured Confounding},
  author = {Dorie, Vincent and Harada, Masataka and Carnegie, Nicole Bohme and Hill, Jennifer},
  year = 2016,
  month = sep,
  journal = {Statistics in Medicine},
  volume = {35},
  number = {20},
  pages = {3453--3470},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.6973},
  urldate = {2022-11-17},
  langid = {english}
}

@incollection{druckman_making_2011,
  title = {Making {{Effects Manifest}} in {{Randomized Experiments}}},
  booktitle = {Cambridge {{Handbook}} of {{Experimental Political Science}}},
  author = {Bowers, Jake},
  editor = {Druckman, James N. and Green, Donald P. and Kuklinski, James H. and Lupia, Arthur},
  year = 2011,
  pages = {459--480},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511921452.032},
  urldate = {2020-03-12},
  isbn = {978-0-511-92145-2},
  langid = {english}
}

@misc{drysdale_winners_nodate,
  title = {A Winner's Curse Adjustment for a Single Test Statistic},
  author = {Drysdale, Erik},
  urldate = {2020-10-27},
  abstract = {Background},
  howpublished = {http://www.erikdrysdale.com/winners\_curse/}
}

@incollection{duflo_using_2007,
  title = {Using {{Randomization}} in {{Development Economics Research}}: {{A Toolkit}}},
  shorttitle = {Chapter 61 {{Using Randomization}} in {{Development Economics Research}}},
  booktitle = {Handbook of {{Development Economics}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  editor = {Schultz, T. Paul and Strauss, John A.},
  year = 2007,
  month = jan,
  volume = {4},
  pages = {3895--3962},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4471(07)04061-2},
  abstract = {This paper is a practical guide (a toolkit) for researchers, students and practitioners wishing to introduce randomization as part of a research design in the field. It first covers the rationale for the use of randomization, as a solution to selection bias and a partial solution to publication biases. Second, it discusses various ways in which randomization can be practically introduced in a field settings. Third, it discusses designs issues such as sample size requirements, stratification, level of randomization and data collection methods. Fourth, it discusses how to analyze data from randomized evaluations when there are departures from the basic framework. It reviews in particular how to handle imperfect compliance and externalities. Finally, it discusses some of the issues involved in drawing general conclusions from randomized evaluations, including the necessary use of theory as a guide when designing evaluations and interpreting results.},
  langid = {english},
  keywords = {development,experiments,program evaluation,randomized evaluations}
}

@article{duflo_using_nodate,
  title = {{{USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH}}: {{A TOOLKIT}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  pages = {92},
  langid = {english}
}

@article{ebenstein_long-run_2016,
  title = {The {{Long-Run Economic Consequences}} of {{High-Stakes Examinations}}: {{Evidence}} from {{Transitory Variation}} in {{Pollution}}},
  shorttitle = {The {{Long-Run Economic Consequences}} of {{High-Stakes Examinations}}},
  author = {Ebenstein, Avraham and Lavy, Victor and Roth, Sefi},
  year = 2016,
  month = oct,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {4},
  pages = {36--65},
  issn = {1945-7782, 1945-7790},
  doi = {10.1257/app.20150213},
  urldate = {2022-11-17},
  abstract = {Cognitive performance during high-stakes exams can be affected by random disturbances that, even if transitory, may have permanent consequences. We evaluate this hypothesis among Israeli students who took a series of matriculation exams between 2000 and 2002. Exploiting variation across the same student taking multiple exams, we find that transitory PM 2.5 exposure is associated with a significant decline in student performance. We then examine these students in 2010 and find that PM 2.5 exposure during exams is negatively associated with postsecondary educational attainment and earnings. The results highlight how reliance on noisy signals of student quality can lead to allocative inefficiency. (JEL I21, I23, I26, J24, J31, Q51, Q53)},
  langid = {english}
}

@article{ebenstein_particulate_2015,
  title = {Particulate {{Matter Concentrations}}, {{Sandstorms}} and {{Respiratory Hospital Admissions}} in {{Israel}}},
  author = {Ebenstein, Avraham and Frank, Eyal and Reingewertz, Yaniv},
  year = 2015,
  volume = {17},
  pages = {6},
  abstract = {Objectives: To identify the effect of PM10 on hospital admissions due to respiratory illnesses. Methods: We used the Instrumental Variable (IV) methodology to control for confounding factors affecting hospital admissions. Exploiting the timing of sandstorms as an instrumental variable allows for a better estimate of the relationship between PM10 and hospital admissions. Data on PM10 concentrations and hospital admissions were compiled for Israel's two largest cities, Jerusalem and Tel Aviv, for 2007--2009. We compared our IV estimates to those derived from a Poisson regression, which is commonly used in the literature. Results: Sandstorms led to an increase of 307 \textmu g/m3 of PM10 concentrations. A 10 \textmu g/m3 increase in PM10 is associated with a 0.8\% increase in hospital admissions due to respiratory conditions, using IV methodology. The same finding was noted using the Poisson regression. Conclusions: The association between PM10 and hospital admission reflects a primarily causal relationship. Instrumental variable methodology could be applied to analyze the effect of air pollution on hospital admissions.},
  langid = {english}
}

@article{ebenstein_particulate_2015,
  title = {Particulate {{Matter Concentrations}}, {{Sandstorms}} and {{Respiratory Hospital Admissions}} in {{Israel}}},
  author = {Ebenstein, Avraham and Frank, Eyal and Reingewertz, Yaniv},
  year = 2015,
  journal = {The Israel Medical Association journal},
  volume = {17},
  pages = {6},
  abstract = {Objectives: To identify the effect of PM10 on hospital admissions due to respiratory illnesses. Methods: We used the Instrumental Variable (IV) methodology to control for confounding factors affecting hospital admissions. Exploiting the timing of sandstorms as an instrumental variable allows for a better estimate of the relationship between PM10 and hospital admissions. Data on PM10 concentrations and hospital admissions were compiled for Israel's two largest cities, Jerusalem and Tel Aviv, for 2007--2009. We compared our IV estimates to those derived from a Poisson regression, which is commonly used in the literature. Results: Sandstorms led to an increase of 307 \textmu g/m3 of PM10 concentrations. A 10 \textmu g/m3 increase in PM10 is associated with a 0.8\% increase in hospital admissions due to respiratory conditions, using IV methodology. The same finding was noted using the Poisson regression. Conclusions: The association between PM10 and hospital admission reflects a primarily causal relationship. Instrumental variable methodology could be applied to analyze the effect of air pollution on hospital admissions.},
  langid = {english}
}

@article{fan_impact_2020,
  title = {The Impact of {{PM2}}.5 on Mortality in Older Adults: Evidence from Retirement of Coal-Fired Power Plants in the {{United States}}},
  shorttitle = {The Impact of {{PM2}}.5 on Mortality in Older Adults},
  author = {Fan, Maoyong and Wang, Yi},
  year = 2020,
  month = dec,
  journal = {Environmental Health},
  volume = {19},
  number = {1},
  pages = {28},
  issn = {1476-069X},
  doi = {10.1186/s12940-020-00573-2},
  urldate = {2022-11-17},
  abstract = {Background: Evidence of causal relationship between mortality of older adults and low- concentration PM2.5 remains limited. Objectives: This study investigates the effects of low-concentration PM2.5 on the mortality of adults older than 65 using the closure of coal-fired power plants in the Eastern United States as a natural experiment. Methods: We investigated power plants in the Eastern United States (US) that had production changes through unit shutdown or plant retirement between 1999 and 2013. We included only non-clustered power plants without scrubbers and with capacities greater than 50 MW. We used instrumental variable (IV) and difference-in-differences (DID) approaches to estimate the causal impact of PM2.5 concentrations on mortality among Medicare beneficiaries. We compared changes in monthly age-adjusted mortality before and after the retirement of coal-fired plants between the treated and control counties; we accounted for annual wind direction in our selection of treated and control counties. In the models, we initially included only county and monthly fixed effects, and then adjusted for covariates including: 1) only weather variables (temperature, dew point, pressure); and 2) weather variables and socio-economic variables (median household income and poverty rate). Results: The monthly age-adjusted mortality rate averaged across all plants was approximately 423 per 100,000 (SD = 69) and was higher for males than females. Mean PM2.5 concentrations across all counties were 12 {$\mu$}g/m3 (SD = 3.78). Using the IV method, we found that reductions in PM2.5 concentrations significantly decreased monthly mortality among older adults. IV results show that a 1-{$\mu$}g/m3 reduction in PM2.5 concentrations leads to 7.17 fewer deaths per 100,000 per month, or a 1.7\% lower monthly mortality rate among people older than 65 years. Using the DID approach, we found that power plant retirement significantly decreased: 1) monthly PM2.5 levels by 2.1 {$\mu$}g/m3, and 2) monthly age-adjusted mortality by approximately 15 people per 100,000 (or 3.6\%) in treated counties relative to control counties. The mortality effects were higher among males than females and its impact was the greatest among people older than 75 years. Conclusion: These findings provide evidence of the effectiveness of local, plant-level control measures in reducing near-plant PM2.5 and mortality among U.S. Medicare beneficiaries.},
  langid = {english}
}

@article{fan_winter_2020,
  title = {The Winter Choke: {{Coal-Fired}} Heating, Air Pollution, and Mortality in {{China}}},
  shorttitle = {The Winter Choke},
  author = {Fan, Maoyong and He, Guojun and Zhou, Maigeng},
  year = 2020,
  month = may,
  journal = {Journal of Health Economics},
  volume = {71},
  pages = {102316},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2020.102316},
  urldate = {2021-06-10},
  abstract = {China's coal-fired winter heating systems generate large amounts of hazardous emissions that significantly deteriorate air quality. Exploiting regression discontinuity designs based on the exact starting dates of winter heating across different cities, we estimate the contemporaneous impact of winter heating on air pollution and health. We find that turning on the winter heating system increased the weekly Air Quality Index by 36\% and caused 14\% increase in mortality rate. This implies that a 10-point increase in the weekly Air Quality Index causes a 2.2\% increase in overall mortality. People in poor and rural areas are particularly affected by the rapid deterioration in air quality; this implies that the health impact of air pollution may be mitigated by improved socio-economic conditions. Exploratory costbenefit analysis suggests that replacing coal with natural gas for heating can improve social welfare.},
  langid = {english}
}

@article{fan_winter_2020,
  title = {The Winter Choke: {{Coal-Fired}} Heating, Air Pollution, and Mortality in {{China}}},
  shorttitle = {The Winter Choke},
  author = {Fan, Maoyong and He, Guojun and Zhou, Maigeng},
  year = 2020,
  month = may,
  journal = {Journal of Health Economics},
  volume = {71},
  pages = {102316},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2020.102316},
  urldate = {2022-11-17},
  langid = {english}
}

@article{ferman_inference_2019,
  title = {Inference in {{Differences-in-Differences}} with {{Few Treated Groups}} and {{Heteroskedasticity}}},
  author = {Ferman, Bruno and Pinto, Cristine},
  year = 2019,
  month = jul,
  journal = {The Review of Economics and Statistics},
  volume = {101},
  number = {3},
  pages = {452--467},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00759},
  abstract = {We derive an inference method that works in differences-in-differences settings with few treated and many control groups in the presence of heteroskedasticity. As a leading example, we provide theoretical justification and empirical evidence that heteroskedasticity generated by variation in group sizes can invalidate existing inference methods, even in data sets with a large number of observations per group. In contrast, our inference method remains valid in this case. Our test can also be combined with feasible generalized least squares, providing a safeguard against misspecification of the serial correlation.},
  keywords = {DID}
}

@article{ferraro_feature_2020,
  title = {Feature---{{Is}} a {{Replicability Crisis}} on the {{Horizon}} for {{Environmental}} and {{Resource Economics}}?},
  author = {Ferraro, Paul J. and Shukla, Pallavi},
  year = 2020,
  month = jun,
  journal = {Review of Environmental Economics and Policy},
  volume = {14},
  number = {2},
  pages = {339--351},
  publisher = {The University of Chicago Press},
  issn = {1750-6816},
  doi = {10.1093/reep/reaa011},
  urldate = {2021-03-23},
  abstract = {Environmental and resource economists pride themselves on the credibility of their           empirical research. In other disciplines, however, the credibility of empirical research           is increasingly being debated by scholars. At the core of these debates are critiques of           widespread practices, such as selectively reporting results or using designs with low           statistical power, and critiques of the professional incentives that encourage these           practices. These critiques have led to claims of a ``replicability crisis'' in science. We           show that questionable research practices are also prevalent in the environmental and           resource economics literature. We argue that the discipline needs to take the potential           harm from these practices more seriously. To mitigate this harm, we recommend changes in           the norms and practices of funders, editors, peer reviewers, and authors.             (JEL: Q0, C0)}
}

@article{ferraro_featureis_2020,
  title = {Feature---{{Is}} a {{Replicability Crisis}} on the {{Horizon}} for {{Environmental}} and {{Resource Economics}}?},
  author = {Ferraro, Paul J. and Shukla, Pallavi},
  year = 2020,
  month = jun,
  journal = {Review of Environmental Economics and Policy},
  volume = {14},
  number = {2},
  pages = {339--351},
  issn = {1750-6816, 1750-6824},
  doi = {10.1093/reep/reaa011},
  urldate = {2022-11-17},
  langid = {english}
}

@article{forastiere_assessing_2020,
  title = {Assessing Short-Term Impact of {{PM10}} on Mortality Using a Semiparametric Generalized Propensity Score Approach},
  author = {Forastiere, Laura and Carugno, Michele and Baccini, Michela},
  year = 2020,
  month = dec,
  journal = {Environmental Health},
  volume = {19},
  number = {1},
  pages = {46},
  issn = {1476-069X},
  doi = {10.1186/s12940-020-00599-6},
  urldate = {2021-06-10},
  abstract = {Background: The shape of the exposure-response curve describing the effects of air pollution on population health has crucial regulatory implications, and it is important in assessing causal impacts of hypothetical policies of air pollution reduction. Methods: After having reformulated the problem of assessing the short-term impact of air pollution on health within the potential outcome approach to causal inference, we developed a method based on the generalized propensity score (GPS) to estimate the average dose-response function (aDRF) and quantify attributable deaths under different counterfactual scenarios of air pollution reduction. We applied the proposed approach to assess the impact of airborne particles with a diameter less than or equal to 10 {$\mu$}m (PM10) on deaths from natural, cardiovascular and respiratory causes in the city of Milan, Italy (2003-2006). Results: As opposed to what is commonly assumed, the estimated aDRFs were not linear, being steeper for low-moderate values of exposure. In the case of natural mortality, the curve became flatter for higher levels; this behavior was less pronounced for cause-specific mortality. The effect was larger in days characterized by higher temperature. According to the curves, we estimated that a hypothetical intervention able to set the daily exposure levels exceeding 40 {$\mu$}g/m3 to exactly 40 would have avoided 1157 deaths (90\%CI: 689, 1645) in the whole study period, 312 of which for respiratory causes and 771 for cardiovascular causes. These impacts were higher than those obtained previously from regression-based methods. Conclusion: This novel method based on the GPS allowed estimating the average dose-response function and calculating attributable deaths, without requiring strong assumptions about the shape of the relationship. Its potential as a tool for investigating effect modification by temperature and its use in other environmental epidemiology contexts deserve further investigation.},
  langid = {english}
}

@article{forastiere_assessing_2020,
  title = {Assessing Short-Term Impact of {{PM10}} on Mortality Using a Semiparametric Generalized Propensity Score Approach},
  author = {Forastiere, Laura and Carugno, Michele and Baccini, Michela},
  year = 2020,
  month = dec,
  journal = {Environmental Health},
  volume = {19},
  number = {1},
  pages = {46},
  issn = {1476-069X},
  doi = {10.1186/s12940-020-00599-6},
  urldate = {2022-11-17},
  abstract = {Background: The shape of the exposure-response curve describing the effects of air pollution on population health has crucial regulatory implications, and it is important in assessing causal impacts of hypothetical policies of air pollution reduction. Methods: After having reformulated the problem of assessing the short-term impact of air pollution on health within the potential outcome approach to causal inference, we developed a method based on the generalized propensity score (GPS) to estimate the average dose-response function (aDRF) and quantify attributable deaths under different counterfactual scenarios of air pollution reduction. We applied the proposed approach to assess the impact of airborne particles with a diameter less than or equal to 10 {$\mu$}m (PM10) on deaths from natural, cardiovascular and respiratory causes in the city of Milan, Italy (2003-2006). Results: As opposed to what is commonly assumed, the estimated aDRFs were not linear, being steeper for low-moderate values of exposure. In the case of natural mortality, the curve became flatter for higher levels; this behavior was less pronounced for cause-specific mortality. The effect was larger in days characterized by higher temperature. According to the curves, we estimated that a hypothetical intervention able to set the daily exposure levels exceeding 40 {$\mu$}g/m3 to exactly 40 would have avoided 1157 deaths (90\%CI: 689, 1645) in the whole study period, 312 of which for respiratory causes and 771 for cardiovascular causes. These impacts were higher than those obtained previously from regression-based methods. Conclusion: This novel method based on the GPS allowed estimating the average dose-response function and calculating attributable deaths, without requiring strong assumptions about the shape of the relationship. Its potential as a tool for investigating effect modification by temperature and its use in other environmental epidemiology contexts deserve further investigation.},
  langid = {english}
}

@article{fowler_electoral_2013,
  title = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}: {{Evidence}} from {{Compulsory Voting}} in {{Australia}}},
  shorttitle = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}},
  author = {Fowler, Anthony},
  year = 2013,
  journal = {Quarterly Journal of Political Science},
  volume = {8},
  number = {2},
  pages = {159--182},
  publisher = {now publishers},
  abstract = {Despite extensive research on voting, there is little evidence connecting turnout to tangible outcomes. Would election results and public policy be different if everyone voted? The adoption of compulsory voting in Australia provides a rare opportunity to address this question. First, I collect two novel data sources to assess the extent of turnout inequality in Australia before compulsory voting. Overwhelmingly, wealthy citizens voted more than their working-class counterparts. Next, exploiting the differential adoption of compulsory voting across states, I find that the policy increased voter turnout by 24 percentage points which in turn increased the vote shares and seat shares of the Labor Party by 7--10 percentage points. Finally, comparing across OECD countries, I find that Australia's adoption of compulsory voting significantly increased turnout and pension spending at the national level. Results suggest that democracies with voluntary voting do not represent the preferences of all citizens. Instead, increased voter turnout can dramatically alter election outcomes and resulting public policies.}
}

@article{freeman_power_2013,
  title = {Power and Sample Size Calculations for {{Mendelian}} Randomization Studies Using One Genetic Instrument},
  author = {Freeman, G. and Cowling, B. J. and Schooling, C. M.},
  year = 2013,
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {42},
  number = {4},
  pages = {1157--1163},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyt110},
  langid = {english}
}

@article{fujiwara_habit_2016,
  title = {Habit {{Formation}} in {{Voting}}: {{Evidence}} from {{Rainy Elections}}},
  shorttitle = {Habit {{Formation}} in {{Voting}}},
  author = {Fujiwara, Thomas and Meng, Kyle and Vogl, Tom},
  year = 2016,
  month = oct,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {4},
  pages = {160--188},
  issn = {1945-7782},
  doi = {10.1257/app.20140533},
  abstract = {We estimate habit formation in voting--the effect of past on current turnout--by exploiting transitory voting cost shocks. Using county-level data on US presidential elections from 1952-2012, we find that rainfall on current and past election days reduces voter turnout. Our estimates imply that a 1-point decrease in past turnout lowers current turnout by 0.6-1.0 points. Further analyses suggest that habit formation operates by reinforcing the direct consumption value of voting and that our estimates may be amplified by social spillovers.},
  langid = {english},
  keywords = {Elections,IV,Rainfall,Turnout}
}

@article{fujiwara2016habit,
  title = {Habit Formation in Voting: {{Evidence}} from Rainy Elections},
  author = {Fujiwara, Thomas and Meng, Kyle and Vogl, Tom},
  year = 2016,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {4},
  pages = {160--88}
}

@article{gehrsitz_effect_2017-1,
  title = {The Effect of Low Emission Zones on Air Pollution and Infant Health},
  author = {Gehrsitz, Markus},
  year = 2017,
  month = may,
  journal = {Journal of Environmental Economics and Management},
  volume = {83},
  pages = {121--144},
  issn = {00950696},
  doi = {10.1016/j.jeem.2017.02.003},
  urldate = {2020-11-05},
  abstract = {This paper investigates the effect of low emission zones on air quality and birth outcomes in Germany. The staggered introduction of the policy measure creates a credible natural experiment and a natural control group for births and air pollution measurements in cities that enact low emission zones. I show that the introduction of the most restrictive type of low emission zone decreases average levels of fine particulate matter by about 4 percent and by up to 8 percent at a city's highest-polluting monitor. Low emission zones also reduce the number of days per year on which legal pollution limits are exceeded by three. However, these reductions are too small to translate into substantial improvements in infant health. My results are not driven by changes in maternal or city specific characteristics, and are robust to variations in specification and to the choice of control group.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{gelman_beyond_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = 2014,
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {p-values,Power,Statistics,Type S/M errors}
}

@article{gelman_beyond_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = 2014,
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614551642},
  urldate = {2022-11-17},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english}
}

@incollection{gelman_missing_2006,
  title = {Missing Data Imputation},
  booktitle = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = 2006,
  month = dec,
  publisher = {Cambridge University Press},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  googlebooks = {c9xLKzZWoZ4C},
  isbn = {978-1-139-46093-4},
  langid = {english},
  keywords = {Gelman,Imputation,Missing data}
}

@article{gelman_power_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = 2014,
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  urldate = {2020-10-21},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {p-values,Power,Statistics,Type S/M errors}
}

@book{gelman_regression_2020,
  title = {{Regression and other stories}},
  author = {Gelman, Andrew},
  year = 2020,
  publisher = {Cambridge University Press},
  address = {Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore},
  isbn = {978-1-107-67651-0},
  langid = {Anglais}
}

@book{gelman_regression_nodate,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  langid = {english},
  keywords = {Handbook,Statistics}
}

@article{gelman_regression_nodate,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  pages = {552},
  langid = {english}
}

@article{gelman_too_2009,
  title = {Too Little Attention Has Been Paid to the Statistical Challenges in Estimating Small Effects},
  author = {Gelman, Andrew and Weakliem, David},
  year = 2009,
  pages = {7},
  langid = {english}
}

@article{gelman_type_2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  year = 2000,
  month = sep,
  journal = {Computational Statistics},
  volume = {15},
  number = {3},
  pages = {373--390},
  issn = {1613-9658},
  doi = {10.1007/s001800000040},
  urldate = {2020-10-26},
  abstract = {In classical statistics, the significance of comparisons (e.g., \texttheta 1- \texttheta 2) is calibrated using the Type 1 error rate, relying on the assumption that the true difference is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state ``\texttheta 1 {$>$} \texttheta 2 with confidence,'' ``\texttheta 2 {$>$} \texttheta 1 with confidence,'' or ``no claim with confidence.'' We focus on the Type S (for sign) error, which occurs when you claim ``\texttheta 1 {$>$} \texttheta 2 with confidence'' when \texttheta 2{$>$} \texttheta 1 (or vice-versa). We compute the Type S error rates for classical and Bayesian confidence statements and find that classical Type S error rates can be extremely high (up to 50\%). Bayesian confidence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  langid = {english}
}

@book{gelman2020regression,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = 2020,
  publisher = {Cambridge University Press}
}

@article{gelmanType2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  year = 2000,
  month = sep,
  journal = {Computational Statistics},
  volume = {15},
  number = {3},
  pages = {373--390},
  issn = {1613-9658},
  doi = {10.1007/s001800000040},
  urldate = {2020-10-26},
  abstract = {In classical statistics, the significance of comparisons (e.g., \texttheta 1- \texttheta 2) is calibrated using the Type 1 error rate, relying on the assumption that the true difference is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state ``\texttheta 1 {$>$} \texttheta 2 with confidence,'' ``\texttheta 2 {$>$} \texttheta 1 with confidence,'' or ``no claim with confidence.'' We focus on the Type S (for sign) error, which occurs when you claim ``\texttheta 1 {$>$} \texttheta 2 with confidence'' when \texttheta 2{$>$} \texttheta 1 (or vice-versa). We compute the Type S error rates for classical and Bayesian confidence statements and find that classical Type S error rates can be extremely high (up to 50\%). Bayesian confidence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  langid = {english}
}

@article{giaccherini_when_2021,
  title = {When Particulate Matter Strikes Cities: {{Social}} Disparities and Health Costs of Air Pollution},
  shorttitle = {When Particulate Matter Strikes Cities},
  author = {Giaccherini, Matilde and Kopinska, Joanna and Palma, Alessandro},
  year = 2021,
  month = jul,
  journal = {Journal of Health Economics},
  volume = {78},
  pages = {102478},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2021.102478},
  urldate = {2022-11-17},
  abstract = {We investigate the heterogeneous effects of particle pollution on Italian daily hospitalizations and their costs by exploiting public transportation strikes as plausibly-exogenous shocks in pollution exposure. We find that a one standard deviation increase in PM10 causes additional 0.79 hospitalizations per 100,000 residents, and the effect is stronger for the elderly, low educated individuals and migrants. Furthermore, we find that young individuals, an arguably healthy age group, exhibit economically meaningful responses to air pollution with an effect ranging between 0.45 and 1.04. Our results imply a large role of avoidance behavior driving heterogeneous marginal health effects. Total daily costs of a one standard deviation increase in PM10 represent 0.5\% of the total daily health expenditure, and 85\% of this additional spending comes from more patients hospitalized, while the remaining 15\% can be attributable to more costly, and likely more complex, hospitalizations.},
  langid = {english}
}

@article{giaccherini_when_nodate,
  title = {When a ({{Particulate}}) {{Matter Strikes}} the {{City}}. {{Social Disparities}} and {{Health Costs}} of {{Air Pollution}}},
  author = {Giaccherini, Matilde and Kopinska, Joanna and Palma, Alessandro},
  pages = {58},
  abstract = {We investigate the unequal effects of air pollution on health outcomes and costs by linking daily pollutant concentration levels to information on the universe of hospitalizations in all major Italian municipalities. By exploiting daily episodes of all public transportation strikes occurred between 2013 and 2015 as an instrumental variable for pollutant concentrations, we find that higher values of particle pollution (P M10 and P M2.5) induced by strikes cause a rise in urgent respiratory hospital admissions, with a larger penalty for the young, the oldest and the least educated We also estimate direct monetary costs, showing not only that air pollution increases the medical spending for a higher number of hospitalizations, but it also increases their complexity, hence their costs. In terms of total costs, we show that individuals of different ages in combination with different exposures to air pollution may face similar health costs. Our study provides large evidence of environmental inequality, suggesting that effective mitigation policies not only have to account for air pollution as a technological issue, but also as a socio-economic phenomenon with largely heterogeneous effects.},
  langid = {english}
}

@article{gibson_effects_2015,
  title = {The Effects of Road Pricing on Driver Behavior and Air Pollution},
  author = {Gibson, Matthew and Carnovale, Maria},
  year = 2015,
  month = sep,
  journal = {Journal of Urban Economics},
  volume = {89},
  pages = {62--73},
  issn = {00941190},
  doi = {10.1016/j.jue.2015.06.005},
  urldate = {2020-06-05},
  abstract = {Exploiting the natural experiment created by an unanticipated court injunction, we evaluate driver responses to road pricing. We find evidence of intertemporal substitution toward unpriced times and spatial substitution toward unpriced roads. The effect on traffic volume varies with public transit availability. Net of these responses, Milan's pricing policy reduces air pollution substantially, generating large welfare gains. In addition, we use long-run policy changes to estimate price elasticities.},
  langid = {english}
}

@article{godzinski_disentangling_2021,
  title = {Disentangling the Effects of Air Pollutants with Many Instruments},
  author = {Godzinski, Alexandre and Suarez Castillo, Milena},
  year = 2021,
  month = sep,
  journal = {Journal of Environmental Economics and Management},
  volume = {109},
  pages = {102489},
  issn = {00950696},
  doi = {10.1016/j.jeem.2021.102489},
  urldate = {2022-11-17},
  abstract = {Air pollution poses a major threat to human health. Far from unidimensional, air pollution is multifaceted, but quasi-experimental studies have been struggling to grasp the consequences of the multiple hazards. By selecting optimal instruments from a novel and large set of altitude--weather instrumental variables, we disentangle the impact of five air pollutants in a comprehensive assessment of their short-term health impact in the largest urban areas of France over 2010--2015. We find that higher levels of at least two air pollutants, ozone and sulfur dioxide, lead to more respiratory-related emergency admissions. Children and elderly are mostly affected. Carbon monoxide increases emergency admissions for cardiovascular diseases while particulate matter is found responsible for increasing the cardiovascular-related mortality rate, and sulfur dioxide the respiratory-related mortality rate. Assuming a five air pollutants context, we show that an analyst who ignored the presence of interrelations between air pollutants would have reached partially false conclusions.},
  langid = {english}
}

@techreport{godzinski2019short,
  title = {Short-Term Health Effects of Public Transport Disruptions: Air Pollution and Viral Spread Channels},
  author = {Godzinski, Alexandre and Castillo, M Suarez and others},
  year = 2019,
  institution = {Institut National de la Statistique et des Etudes Economiques}
}

@article{goldberg_disentangling_2020,
  title = {Disentangling the {{Impact}} of the {{COVID-19 Lockdowns}} on {{Urban NO2 From Natural Variability}}},
  author = {Goldberg, Daniel L. and Anenberg, Susan C. and Griffin, Debora and McLinden, Chris A. and Lu, Zifeng and Streets, David G.},
  year = 2020,
  journal = {Geophysical Research Letters},
  volume = {47},
  number = {17},
  pages = {e2020GL089269},
  issn = {1944-8007},
  doi = {10.1029/2020GL089269},
  urldate = {2020-09-09},
  abstract = {TROPOMI satellite data show substantial drops in nitrogen dioxide (NO2) during COVID-19 physical distancing. To attribute NO2 changes to NOx emissions changes over short timescales, one must account for meteorology. We find that meteorological patterns were especially favorable for low NO2 in much of the United States in spring 2020, complicating comparisons with spring 2019. Meteorological variations between years can cause column NO2 differences of 15\% over monthly timescales. After accounting for solar angle and meteorological considerations, we calculate that NO2 drops ranged between 9.2\% and 43.4\% among 20 cities in North America, with a median of 21.6\%. Of the studied cities, largest NO2 drops ({$>$}30\%) were in San Jose, Los Angeles, and Toronto, and smallest drops ({$<$}12\%) were in Miami, Minneapolis, and Dallas. These normalized NO2 changes can be used to highlight locations with greater activity changes and better understand the sources contributing to adverse air quality in each city.},
  copyright = {\copyright 2020. The Authors.},
  langid = {english},
  keywords = {COVID-19,meteorological effects,NO2 trends,NOx emissions,TROPOMI NO2}
}

@misc{goldfeld_generating_nodate,
  title = {Generating Data to Explore the Myriad Causal Effects That Can Be Estimated in Observational Data Analysis},
  author = {Goldfeld, Keith},
  abstract = {I've been inspired by two recent talks describing the challenges of using instrumental variable (IV) methods. IV methods are used to estimate the causal effects of an exposure or intervention when there is unmeasured confounding. This estimated causal effect is very specific: the complier average causal effect (CACE). But, the CACE is just one of several possible causal estimands that we might be interested in. For example, there's the average causal effect (ACE) that represents a population average (not just based the subset of compliers).},
  langid = {english},
  keywords = {Simulations}
}

@article{goldsmith-pinkham_bartik_2020,
  title = {Bartik {{Instruments}}: {{What}}, {{When}}, {{Why}}, and {{How}}},
  shorttitle = {Bartik {{Instruments}}},
  author = {{Goldsmith-Pinkham}, Paul and Sorkin, Isaac and Swift, Henry},
  year = 2020,
  month = aug,
  journal = {American Economic Review},
  volume = {110},
  number = {8},
  pages = {2586--2624},
  issn = {0002-8282},
  doi = {10.1257/aer.20181047},
  abstract = {The Bartik instrument is formed by interacting local industry shares and national industry growth rates. We show that the typical use of a Bartik instrument assumes a pooled exposure research design, where the shares measure differential exposure to common shocks, and identification is based on exogeneity of the shares. Next, we show how the Bartik instrument weights each of the exposure designs. Finally, we discuss how to assess the plausibility of the research design. We illustrate our results through two applications: estimating the elasticity of labor supply, and estimating the elasticity of substitution between immigrants and natives.},
  langid = {english},
  keywords = {and Immigrants,and Transportation Economics: Regional Migration,Economics of Minorities,Empirical Studies of Trade,Indigenous Peoples,Industry Studies: Manufacturing: General,Model Construction and Estimation,Neighborhood Characteristics,Non-labor Discrimination,Other Spatial Production and Pricing Analysis,Population,Races,Real Estate,Regional,Regional Labor Markets,Rural,Time Allocation and Labor Supply,Urban}
}

@article{gomez_republicans_2007,
  title = {The {{Republicans Should Pray}} for {{Rain}}: {{Weather}}, {{Turnout}}, and {{Voting}} in {{U}}.{{S}}. {{Presidential Elections}}},
  shorttitle = {The {{Republicans Should Pray}} for {{Rain}}},
  author = {Gomez, Brad T. and Hansford, Thomas G. and Krause, George A.},
  year = 2007,
  journal = {The Journal of Politics},
  volume = {69},
  number = {3},
  pages = {649--663},
  publisher = {[The University of Chicago Press, Southern Political Science Association]},
  issn = {0022-3816},
  doi = {10.1111/j.1468-2508.2007.00565.x},
  abstract = {The relationship between bad weather and lower levels of voter turnout is widely espoused by media, political practitioners, and, perhaps, even political scientists. Yet, there is virtually no solid empirical evidence linking weather to voter participation. This paper provides an extensive test of the claim. We examine the effect of weather on voter turnout in 14 U.S. presidential elections. Using GIS interpolations, we employ meteorological data drawn from over 22,000 U.S. weather stations to provide election day estimates of rain and snow for each U.S. county. We find that, when compared to normal conditions, rain significantly reduces voter participation by a rate of just less than 1\% per inch, while an inch of snowfall decreases turnout by almost .5\%. Poor weather is also shown to benefit the Republican party's vote share. Indeed, the weather may have contributed to two Electoral College outcomes, the 1960 and 2000 presidential elections.}
}

@article{gomez-carracedo_practical_2014,
  title = {A Practical Comparison of Single and Multiple Imputation Methods to Handle Complex Missing Data in Air Quality Datasets},
  author = {{G{\'o}mez-Carracedo}, M. P. and Andrade, J. M. and {L{\'o}pez-Mah{\'i}a}, P. and Muniategui, S. and Prada, D.},
  year = 2014,
  month = may,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {134},
  pages = {23--33},
  issn = {0169-7439},
  doi = {10.1016/j.chemolab.2014.02.007},
  urldate = {2020-07-28},
  abstract = {Datasets with missing data ratios ranging from 24\% to 4\%, corresponding to three air quality monitoring studies, were used to ascertain whether major differences occur when five currently used imputation methods are applied (four single imputation methods and a multiple imputation one). Unrotated and Varimax-rotated factor analyses performed on the imputed datasets were compared. All methods performed similarly, although multiple imputation yielded more disperse imputed values. Main differences occurred when a variable with missing values correlated poorly to the other features and when a variable had relevant loadings in several unrotated factors, which sometimes changed the order of the rotated factors.},
  langid = {english},
  keywords = {Air pollution,Comparison imputation methods,Imputation,Imputation method}
}

@article{gomez2007republicans,
  title = {The {{Republicans}} Should Pray for Rain: {{Weather}}, Turnout, and Voting in {{US}} Presidential Elections},
  author = {Gomez, Brad T and Hansford, Thomas G and Krause, George A},
  year = 2007,
  journal = {The Journal of Politics},
  volume = {69},
  number = {3},
  pages = {649--663},
  publisher = {Cambridge University Press New York, USA}
}

@article{grange_averaging_nodate,
  title = {Averaging Wind Speeds and Directions},
  author = {Grange, Stuart K},
  pages = {12},
  langid = {english}
}

@article{greenland_invited_2017,
  title = {Invited {{Commentary}}: {{The Need}} for {{Cognitive Science}} in {{Methodology}}},
  shorttitle = {Invited {{Commentary}}},
  author = {Greenland, Sander},
  year = 2017,
  month = sep,
  journal = {American Journal of Epidemiology},
  volume = {186},
  number = {6},
  pages = {639--645},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwx259},
  urldate = {2022-11-17},
  langid = {english}
}

@article{griffin_moving_2021,
  title = {Moving beyond the Classic Difference-in-Differences Model: {{A}} Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies},
  shorttitle = {Moving beyond the Classic Difference-in-Differences Model},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Stuart, Elizabeth A. and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley D. and Schell, Terry and Pacula, Rosalie L.},
  year = 2021,
  month = jun,
  journal = {arXiv:2003.12008 [stat]},
  eprint = {2003.12008},
  primaryclass = {stat},
  abstract = {State-level policy evaluations commonly employ a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. Motivated by applied state-level opioid policy evaluations, this simulation study compares statistical performance of multiple variations of two-way fixed effect models traditionally used for DID under a range of simulation conditions. While most linear models resulted in minimal bias, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160\%). Further, root mean square error is minimized by linear AR models when examining crude mortality rates and by negative binomial models when examining raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis ({$<$} 10\%), raising concerns of spurious conclusions about policy effectiveness. When considering performance across models, the linear autoregressive models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. These findings highlight notable limitations of traditional statistical models commonly used for DID designs, designs widely used in opioid policy studies and in state policy evaluations more broadly.},
  archiveprefix = {arXiv},
  keywords = {DID}
}

@article{griffin_moving_2021,
  title = {Moving beyond the Classic Difference-in-Differences Model: A Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies},
  shorttitle = {Moving beyond the Classic Difference-in-Differences Model},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Stuart, Elizabeth A. and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley D. and Schell, Terry L. and Pacula, Rosalie Liccardo},
  year = 2021,
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {21},
  number = {1},
  pages = {279},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01471-y},
  urldate = {2022-11-17},
  abstract = {Background:\enspace{} Reliable evaluations of state-level policies are essential for identifying effective policies and informing policymakers' decisions. State-level policy evaluations commonly use a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. More guidance is needed about which set of statistical models perform best when estimating how state-level policies affect outcomes. Methods:\enspace{} Motivated by applied state-level opioid policy evaluations, we implemented an extensive simulation study to compare the statistical performance of multiple variations of the two-way fixed effect models traditionally used for DID under a range of simulation conditions. We also explored the performance of autoregressive (AR) and GEE models. We simulated policy effects on annual state-level opioid mortality rates and assessed statistical performance using various metrics, including directional bias, magnitude bias, and root mean squared error. We also reported Type I error rates and the rate of correctly rejecting the null hypothesis (e.g., power), given the prevalence of frequentist null hypothesis significance testing in the applied literature. Results:\enspace{} Most linear models resulted in minimal bias. However, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160\%). Further, root mean square error was minimized by linear AR models when we examined crude mortality rates and by negative binomial models when we examined raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis ({$<$}\,10\%), raising concerns of spurious conclusions about policy effectiveness in the opioid literature. When considering performance across models, the linear AR models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. Conclusions:\enspace{} The findings highlight notable limitations of commonly used statistical models for DID designs, which are widely used in opioid policy studies and in state policy evaluations more broadly. In contrast, the optimal model we identified--the AR model--is rarely used in state policy evaluation. We urge applied researchers to move beyond the classic DID paradigm and adopt use of AR models.},
  langid = {english}
}

@techreport{griffin_variation_2020,
  title = {Variation in {{Performance}} of {{Commonly Used Statistical Methods}} for {{Estimating Effectiveness}} of {{State-Level Opioid Policies}} on {{Opioid-Related Mortality}}},
  author = {Griffin, Beth Ann and Schuler, Megan and Stuart, Elizabeth and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley and Schell, Terry and Pacula, Rosalie Liccardo},
  year = 2020,
  month = apr,
  number = {w27029},
  pages = {w27029},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w27029},
  urldate = {2020-11-05},
  abstract = {Over the last two decades, there has been a surge of opioid-related overdose deaths resulting in a myriad of state policy responses. Researchers have evaluated the effectiveness of such policies using a wide-range of statistical models, each of which requires multiple design choices that can influence the accuracy and precision of the estimated policy effects. This simulation study used real-world data to compare model performance across a range of important statistical constructs to better understand which methods are appropriate for measuring the impacts of state-level opioid policies on opioid-related mortality. Our findings show that many commonly-used methods have very low statistical power to detect a significant policy effect ({$<$} 10\%) when the policy effect size is small yet impactful (e.g., 5\% reduction in opioid mortality). Many methods yielded high rates of Type I error, raising concerns of spurious conclusions about policy effectiveness. Finally, model performance was reduced when policy effectiveness had incremental, rather than instantaneous, onset. These findings highlight the limitations of existing statistical methods under scenarios that are likely to affect real-world policy studies. Given the necessity of identifying and implementing effective opioid-related policies, researchers and policymakers should be mindful of evaluation study statistical design.},
  langid = {english},
  keywords = {Statistical power (Leo)}
}

@article{guidetti_placebo_2021,
  title = {``{{Placebo Tests}}'' for the {{Impacts}} of {{Air Pollution}} on {{Health}}: {{The Challenge}} of {{Limited Health Care Infrastructure}}},
  shorttitle = {``{{Placebo Tests}}'' for the {{Impacts}} of {{Air Pollution}} on {{Health}}},
  author = {Guidetti, Bruna and Pereda, Paula and Severnini, Edson},
  year = 2021,
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {111},
  pages = {371--375},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.20211031},
  urldate = {2022-11-17},
  abstract = {"Placebo tests" are normally used to support evidence of pollution impacts on health outcomes. In this study, we argue that one should be cautious to proceed with falsification tests. We examine how a large metropolitan area in Brazil copes with increased health-care demand due to high air pollution under hospital capacity constraints. Using wind as an instrument, we find that the pollution exposure increases pediatric hospitalization for respiratory diseases while the number of planned procedures decreases in public hospitals. On average, for every four additional pollution-related admissions, one elective care procedure is displaced. Urgent procedures are not displaced.},
  langid = {english}
}

@article{gutman_analyses_2012,
  title = {Analyses That {{Inform Policy Decisions}}},
  author = {Gutman, R. and Rubin, D.B.},
  year = 2012,
  month = sep,
  journal = {Biometrics},
  volume = {68},
  number = {3},
  pages = {671--675},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2011.01732.x},
  urldate = {2021-06-10},
  langid = {english}
}

@article{haber_policy_2020,
  title = {Policy Evaluation in {{COVID-19}}: {{A}} Graphical Guide to Common Design Issues},
  shorttitle = {Policy Evaluation in {{COVID-19}}},
  author = {Haber, Noah A. and {Clarke-Deelder}, Emma and Salomon, Joshua A. and Feller, Avi and Stuart, Elizabeth A.},
  year = 2020,
  month = sep,
  journal = {arXiv:2009.01940 [stat]},
  eprint = {2009.01940},
  primaryclass = {stat},
  urldate = {2020-09-09},
  abstract = {Policy responses to COVID-19, particularly those related to non-pharmaceutical interventions, are unprecedented in scale and scope. Researchers and policymakers are striving to understand the impact of these policies on a variety of outcomes. Policy impact evaluations always require a complex combination of circumstance, study design, data, statistics, and analysis. Beyond the issues that are faced for any policy, evaluation of COVID-19 policies is complicated by additional challenges related to infectious disease dynamics and lags, lack of direct observation of key outcomes, and a multiplicity of interventions occurring on an accelerated time scale. In this paper, we (1) introduce the basic suite of policy impact evaluation designs for observational data, including cross-sectional analyses, pre/post, interrupted time-series, and difference-in-differences analysis, (2) demonstrate key ways in which the requirements and assumptions underlying these designs are often violated in the context of COVID-19, and (3) provide decision-makers and reviewers a conceptual and graphical guide to identifying these key violations. The overall goal of this paper is to help policy-makers, journal editors, journalists, researchers, and other research consumers understand and weigh the strengths and limitations of evidence that is essential to decision-making.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@article{hadeed_imputation_2020,
  title = {Imputation Methods for Addressing Missing Data in Short-Term Monitoring of Air Pollutants},
  author = {Hadeed, Steven J. and O'Rourke, Mary Kay and Burgess, Jefferey L. and Harris, Robin B. and Canales, Robert A.},
  year = 2020,
  month = aug,
  journal = {Science of The Total Environment},
  volume = {730},
  pages = {139140},
  issn = {0048-9697},
  doi = {10.1016/j.scitotenv.2020.139140},
  urldate = {2020-07-28},
  abstract = {Monitoring of environmental contaminants is a critical part of exposure sciences research and public health practice. Missing data are often encountered when performing short-term monitoring ({$<$}24~h) of air pollutants with real-time monitors, especially in resource-limited areas. Approaches for handling consecutive periods of missing and incomplete data in this context remain unclear. Our aim is to evaluate existing imputation methods for handling missing data for real-time monitors operating for short durations. In a current field-study, realtime PM2.5 monitors were placed outside of 20 households and ran for 24-hours. Missing data was simulated in these households at four consecutive periods of missingness (20\%, 40\%, 60\%, 80\%). Univariate (Mean, Median, Last Observation Carried Forward, Kalman Filter, Random, Markov) and multivariate time-series (Predictive Mean Matching, Row Mean Method) methods were used to impute missing concentrations, and performance was evaluated using five error metrics (Absolute Bias, Percent Absolute Error in Means, R2 Coefficient of Determination, Root Mean Square Error, Mean Absolute Error). Univariate methods of Markov, random, and mean imputations were the best performing methods that yielded 24-hour mean concentrations with the lowest error and highest R2 values across all levels of missingness. When evaluating error metrics minute-by-minute, Kalman filters, median, and Markov methods performed well at low levels of missingness (20--40\%). However, at higher levels of missingness (60--80\%), Markov, random, median, and mean imputation performed best on average. Multivariate methods were the worst performing imputation methods across all levels of missingness. Imputation using univariate methods may provide a reasonable solution to addressing missing data for short-term monitoring of air pollutants, especially in resource-limited areas. Further efforts are needed to evaluate imputation methods that are generalizable across a diverse range of study environments.},
  langid = {english},
  keywords = {Air pollution,Imputation,Missing data,Particulate matter,To read}
}

@article{halliday_vog_2019,
  title = {Vog: {{Using Volcanic Eruptions}} to {{Estimate}} the {{Health Costs}} of {{Particulates}}},
  shorttitle = {Vog},
  author = {Halliday, Timothy J and Lynham, John and {de Paula}, {\'A}ureo},
  year = 2019,
  month = may,
  journal = {The Economic Journal},
  volume = {129},
  number = {620},
  pages = {1782--1816},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/ecoj.12609},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{halliday_vog_2019,
  title = {Vog: {{Using Volcanic Eruptions}} to {{Estimate}} the {{Health Costs}} of {{Particulates}}},
  shorttitle = {Vog},
  author = {Halliday, Timothy J and Lynham, John and {de Paula}, {\'A}ureo},
  year = 2019,
  month = may,
  journal = {The Economic Journal},
  volume = {129},
  number = {620},
  pages = {1782--1816},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/ecoj.12609},
  urldate = {2022-11-17},
  langid = {english}
}

@article{hanlon2018london,
  title = {London Fog: A Century of Pollution and Mortality, 1866-1965},
  author = {Hanlon, W Walker},
  year = 2018,
  journal = {The Review of Economics and Statistics},
  pages = {1--49}
}

@book{harrerDoingMetaAnalysis,
  title = {Doing {{Meta-Analysis}} in {{R}}},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi A. and Ebert, David D.},
  urldate = {2024-03-29},
  abstract = {This is a guide on how to conduct Meta-Analyses in R.},
  langid = {english},
  keywords = {Handbook,Meta-analysis,R}
}

@article{he_effect_2016,
  title = {The Effect of Air Pollution on Mortality in {{China}}: {{Evidence}} from the 2008 {{Beijing Olympic Games}}},
  shorttitle = {The Effect of Air Pollution on Mortality in {{China}}},
  author = {He, Guojun and Fan, Maoyong and Zhou, Maigeng},
  year = 2016,
  month = sep,
  journal = {Journal of Environmental Economics and Management},
  volume = {79},
  pages = {18--39},
  issn = {00950696},
  doi = {10.1016/j.jeem.2016.04.004},
  urldate = {2022-11-17},
  abstract = {By exploiting exogenous variations in air quality during the 2008 Beijing Olympic Games, we estimate the effect of air pollution on mortality in China. We find that a 10 percent decrease in concentrations reduces the monthly standardized all-cause mortality rate by 8 percent.},
  langid = {english}
}

@article{he_straw_2020,
  title = {Straw Burning, {{PM2}}.5, and Death: {{Evidence}} from {{China}}},
  shorttitle = {Straw Burning, {{PM2}}.5, and Death},
  author = {He, Guojun and Liu, Tong and Zhou, Maigeng},
  year = 2020,
  month = jun,
  journal = {Journal of Development Economics},
  volume = {145},
  pages = {102468},
  issn = {03043878},
  doi = {10.1016/j.jdeveco.2020.102468},
  urldate = {2022-11-17},
  abstract = {This study uses satellite data to detect agricultural straw burning and estimates its impact on air pollution and health in China. We find that straw burning increases particulate matter pollution and causes people to die from cardiorespiratory diseases. We estimate that a 10 {$\mu$}g/m3 increase in PM2.5 increases mortality by 3.25\%. Middleaged and old people in rural areas are particularly sensitive to straw burning pollution. Exploratory analysis of China's programs to subsidize straw recycling suggests that extending these programs to all the straw burning regions would bring about a health benefit that is an order of magnitude larger than the cost.},
  langid = {english}
}

@article{he_straw_nodate,
  title = {Straw {{Burning}}, {{PM2}}.5 and {{Death}}: {{Evidence}} from {{China}}},
  author = {He, Guojun and Liu, Tong and Zhou, Maigeng},
  pages = {64},
  abstract = {This study uses satellite data to detect agricultural straw burnings and estimates its impact on air pollution and health in China. We find that straw burning increases particulate matter pollution and causes people to die from cardio-respiratory diseases. Middle-aged and old people in rural areas are particularly sensitive to straw burning pollution. We estimate that a 10\textmu g/m3 increase in PM2.5 will increase mortality by 3.25\%. Subsidizing the recycling of straw brings significant health benefits and is estimated to avert 21,400 pre-mature deaths annually.},
  langid = {english}
}

@book{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = 2020,
  edition = {Boca Raton: Chapman \& Hall/CRC},
  langid = {english}
}

@article{hernan_causal_2021,
  title = {Causal Analyses of Existing Databases: {{No}} Power Calculations Required},
  shorttitle = {Causal Analyses of Existing Databases},
  author = {Hern{\'a}n, Miguel A.},
  year = 2021,
  month = aug,
  journal = {Journal of Clinical Epidemiology},
  pages = {S0895435621002730},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2021.08.028},
  abstract = {Observational databases are often used to study causal questions. Before being granted access to data or funding, researchers may need to prove that ``the statistical power of their analysis will be high''. Analyses expected to have low power, and hence result in imprecise estimates, will not be approved. This restrictive attitude towards observational analyses is misguided.},
  langid = {english}
}

@article{hernan_causal_2022,
  title = {Causal Analyses of Existing Databases: No Power Calculations Required},
  shorttitle = {Causal Analyses of Existing Databases},
  author = {Hern{\'a}n, Miguel A.},
  year = 2022,
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {144},
  pages = {203--205},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2021.08.028},
  urldate = {2022-11-17},
  abstract = {Observational databases are often used to study causal questions. Before being granted access to data or funding, researchers may need to prove that ``the statistical power of their analysis will be high''. Analyses expected to have low power, and hence result in imprecise estimates, will not be approved. This restrictive attitude towards observational analyses is misguided.},
  langid = {english}
}

@article{herrnstadt_air_2021,
  title = {Air {{Pollution}} and {{Criminal Activity}}: {{Microgeographic Evidence}} from {{Chicago}}},
  shorttitle = {Air {{Pollution}} and {{Criminal Activity}}},
  author = {Herrnstadt, Evan and Heyes, Anthony and Muehlegger, Erich and Saberian, Soodeh},
  year = 2021,
  month = oct,
  journal = {American Economic Journal: Applied Economics},
  volume = {13},
  number = {4},
  pages = {70--100},
  issn = {1945-7782, 1945-7790},
  doi = {10.1257/app.20190091},
  urldate = {2022-11-17},
  abstract = {A growing literature documents that air pollution adversely impacts health, productivity, and cognition. This paper provides the first evidence of a causal link between air pollution and aggressive behavior, as documented by violent crime. Using the geolocation of crimes in Chicago from 2001--2012, we compare crime upwind and downwind of major highways on days when wind blows orthogonally to the road. Consistent with research linking pollution to aggression, we find that air pollution increases violent crime on the downwind sides of interstates. Our results suggest that pollution may reduce welfare and affect behavior through a wider set of channels than previously considered. (JEL K42, Q53)},
  langid = {english}
}

@article{heyes_air_2019,
  title = {Air Pollution as a Cause of Sleeplessness: {{Social}} Media Evidence from a Panel of {{Chinese}} Cities},
  shorttitle = {Air Pollution as a Cause of Sleeplessness},
  author = {Heyes, Anthony and Zhu, Mingying},
  year = 2019,
  month = jul,
  journal = {Journal of Environmental Economics and Management},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2019.07.002},
  urldate = {2019-07-18},
  abstract = {We provide first evidence of a link from daily air pollution exposure to sleep loss in a panel of Chinese cities. We develop a social media-based, city-level metric for sleeplessness, and bolster causal claims by instrumenting for pollution with plausibly exogenous variations in wind patterns. Estimates of effect sizes are substantial and robust. In our preferred specification a one standard deviation increase in AQI causes an 11.6\% increase in sleeplessness, and for PM2.5 is 12.8\%. The results sustain qualitatively under OLS estimation but are attenuated. The analysis provides a previously unaccounted for benefit of more stringent air quality regulation. It also offers a candidate mechanism in support of recent research that links daily air quality to diminished workplace productivity, cognitive performance, school absence, traffic accidents, and other detrimental outcomes.},
  keywords = {Air pollution,China,IV,Sleep,Social media,To read}
}

@article{hill_bayesian_2011,
  title = {Bayesian {{Nonparametric Modeling}} for {{Causal Inference}}},
  author = {Hill, Jennifer L.},
  year = 2011,
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {1},
  pages = {217--240},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2010.08162},
  langid = {english}
}

@article{ho_randomization_2005,
  title = {Randomization {{Inference}} with {{Natural Experiments}}: {{An Analysis}} of {{Ballot Effects}} in the 2003 {{California Recall Election}}},
  shorttitle = {Randomization {{Inference}} with {{Natural Experiments}}},
  author = {Ho, Daniel E. and Imai, Kosuke},
  year = 2005,
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.656139},
  urldate = {2020-03-12},
  langid = {english}
}

@article{ho2007matching,
  title = {Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference},
  author = {Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth A},
  year = 2007,
  journal = {Political analysis},
  volume = {15},
  number = {3},
  pages = {199--236},
  publisher = {Cambridge University Press}
}

@article{holman_review_2015,
  title = {Review of the Efficacy of Low Emission Zones to Improve Urban Air Quality in {{European}} Cities},
  author = {Holman, Claire and Harrison, Roy and Querol, Xavier},
  year = 2015,
  month = jun,
  journal = {Atmospheric Environment},
  volume = {111},
  pages = {161--169},
  issn = {13522310},
  doi = {10.1016/j.atmosenv.2015.04.009},
  urldate = {2020-06-03},
  abstract = {Many cities still exceed the European Union (EU) air quality limit values for particulate matter (PM10, particles with an aerodynamic diameter less than 10 mm) and/or nitrogen dioxide (NO2). In an attempt to reduce emissions approximately 200 low emission zones (LEZs) have been established in 12 EU countries. These restrict the entry of vehicles based on the emission standard the vehicles were originally constructed to meet, but the restrictions vary considerably. This paper reviews the evidence on the efficacy of LEZs to improve urban air quality in five EU countries (Denmark, Germany, Netherlands, Italy and UK), and concludes that there have been mixed results. There is some evidence from ambient measurements that LEZs in Germany, which restrict passenger cars as well as heavy duty vehicles (HDVs), have reduced long term average PM10 and NO2 concentrations by a few percent. Elsewhere, where restrictions are limited to HDVs, the picture is much less clear. This may be due to the large number of confounding factors. On the other hand there is some, albeit limited, evidence that LEZs may result in larger reductions in concentrations of carbonaceous particles, due to traffic making a larger contribution to ambient concentrations of these particles than to PM10 and PM2.5. The effects of day to day variations in meteorology on concentrations often mask more subtle effects of a LEZ. In addition, separating the direct effects of a LEZ from the effects of other policy measures, the economy and the normal renewal of the vehicle fleet is not easy, and may give rise to false results.},
  langid = {english}
}

@article{hopke_multiple_2001,
  title = {Multiple {{Imputation}} for {{Multivariate Data}} with {{Missing}} and {{Below}}-{{Threshold Measurements}}: {{Time}}-{{Series Concentrations}} of {{Pollutants}} in the {{Arctic}}},
  shorttitle = {Multiple {{Imputation}} for {{Multivariate Data}} with {{Missing}} and {{Below}}-{{Threshold Measurements}}},
  author = {Hopke, Philip K. and Liu, Chuanhai and Rubin, Donald B.},
  year = 2001,
  month = mar,
  journal = {Biometrics},
  volume = {57},
  number = {1},
  pages = {22--33},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/j.0006-341X.2001.00022.x},
  urldate = {2020-06-15},
  abstract = {M. any chemical and environmental data sets are complicated by the existence of fully missing values or censored values known to lie below detection thresholds. For example, week-long samples of airborne particulate matter were obtained at Alert, NWT, Canada, between 1980 and 1991, where some of the concentrations of 24 particulate constituents were coarsened in the sense of being either fully missing or below detection limits. To facilitate scientific analysis, it is appealing to create complete data by filling in missing values so that standard complete-data methods can be applied. We briefly review commonly used strategies for handling missing values and focus on the multiple-imputation approach, which generally leads to valid inferences when faced with missing data. Three statistical models are developed for multiply imputing the missing values of airborne particulate matter. We expect that these models are useful for creating multiple imputations in a variety of incomplete multivariate time series data sets.},
  langid = {english}
}

@article{hoxby_effects_2000,
  title = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}: {{New Evidence}} from {{Population Variation}}*},
  shorttitle = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}},
  author = {Hoxby, Caroline M.},
  year = 2000,
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {115},
  number = {4},
  pages = {1239--1285},
  issn = {0033-5533},
  doi = {10.1162/003355300555060},
  abstract = {I identify the effects of class size on student achievement using longitudinal variation in the population associated with each grade in 649 elementary schools. I use variation in class size driven by idiosyncratic variation in the population. I also use discrete jumps in class size that occur when a small change in enrollment triggers a maximum or minimum class size rule. The estimates indicate that class size does not have a statistically significant effect on student achievement. I rule out even modest effects (2 to 4 percent of a standard deviation in scores for a 10 percent reduction in class size).},
  keywords = {Class size,Education}
}

@book{huntington-klein_effect_2021,
  title = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  shorttitle = {The {{Effect}}},
  author = {{Huntington-Klein}, Nick},
  year = 2021,
  month = nov,
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  doi = {10.1201/9781003226055},
  isbn = {978-1-003-22605-5},
  langid = {english}
}

@book{imbens_causal_2015,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  year = 2015,
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139025751},
  abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  isbn = {978-0-521-88588-1}
}

@article{imbens_optimal_2012,
  title = {Optimal {{Bandwidth Choice}} for the {{Regression Discontinuity Estimator}}},
  author = {Imbens, Guido and Kalyanaraman, Karthik},
  year = 2012,
  journal = {The Review of Economic Studies},
  volume = {79},
  number = {3},
  pages = {933--959},
  publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
  issn = {0034-6527},
  abstract = {We investigate the choice of the bandwidth for the regression discontinuity estimator. We focus on estimation by local linear regression, which was shown to have attractive properties (Porter, J. 2003, "Estimation in the Regression Discontinuity Model" (unpublished, Department of Economics, University of Wisconsin, Madison)). We derive the asymptotically optimal bandwidth under squared error loss. This optimal bandwidth depends on unknown functionals of the distribution of the data and we propose simple and consistent estimators for these functionals to obtain a fully data-driven bandwidth algorithm. We show that this bandwidth estimator is optimal according to the criterion of Li (1987, "Asymptotic Optimality for C p , C L , Cross-validation and Generalized Cross-validation: Discrete Index Set", Annals of Statistics, 15, 958--975), although it is not unique in the sense that alternative consistent estimators for the unknown functionals would lead to bandwidth estimators with the same optimality properties. We illustrate the proposed bandwidth, and the sensitivity to the choices made in our algorithm, by applying the methods to a data set previously analysed by Lee (2008, "Randomized Experiments from Non-random Selection in U.S. House Elections", Journal of Econometrics, 142, 675--697) as well as by conducting a small simulation study.},
  keywords = {Optimal bandwith,Precision,RDD}
}

@article{imbens_statistical_2021,
  title = {Statistical {{Significance}}, {{Values}}, and the {{Reporting}} of {{Uncertainty}}},
  author = {Imbens, Guido W.},
  year = 2021,
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {157--174},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.157},
  abstract = {The use of statistical significance and p-values has become a matter of substantial controversy in various fields using statistical methods. This has gone as far as some journals banning the use of indicators for statistical significance, or even any reports of p-values, and, in one case, any mention of confidence intervals. I discuss three of the issues that have led to these often-heated debates. First, I argue that in many cases, p-values and indicators of statistical significance do not answer the questions of primary interest. Such questions typically involve making (recommendations on) decisions under uncertainty. In that case, point estimates and measures of uncertainty in the form of confidence intervals or even better, Bayesian intervals, are often more informative summary statistics. In fact, in that case, the presence or absence of statistical significance is essentially irrelevant, and including them in the discussion may confuse the matter at hand. Second, I argue that there are also cases where testing null hypotheses is a natural goal and where p-values are reasonable and appropriate summary statistics. I conclude that banning them in general is counterproductive. Third, I discuss that the overemphasis in empirical work on statistical significance has led to abuse of p-values in the form of p-hacking and publication bias. The use of pre-analysis plans and replication studies, in combination with lowering the emphasis on statistical significance may help address these problems.},
  langid = {english}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = 2017,
  month = oct,
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  publisher = {Oxford Academic},
  issn = {0013-0133},
  doi = {10.1111/ecoj.12461},
  abstract = {Abstract. We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical econom},
  langid = {english},
  keywords = {Economics,Metaanalysis,Power,Statistics}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = 2017,
  month = oct,
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236--F265},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/ecoj.12461},
  urldate = {2022-11-17},
  langid = {english}
}

@article{ioannidis_power_2017a,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = 2017,
  month = oct,
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  publisher = {Oxford Academic},
  issn = {0013-0133},
  doi = {10.1111/ecoj.12461},
  urldate = {2020-10-09},
  abstract = {Abstract.  We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical econom},
  langid = {english},
  keywords = {Economics,Metaanalysis,Power,Statistics}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = 2005,
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2021-06-10},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english}
}

@article{ioannidis_why_2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = 2008,
  journal = {Epidemiology},
  volume = {19},
  number = {5},
  eprint = {25662607},
  eprinttype = {jstor},
  pages = {640--648},
  langid = {english}
}

@article{ioannidis_why_2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = 2008,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {19},
  number = {5},
  pages = {640--648},
  langid = {english},
  keywords = {Inflated effects,To read}
}

@article{ioannidis_why_2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = 2008,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {19},
  number = {5},
  eprint = {25662607},
  eprinttype = {jstor},
  pages = {640--648},
  langid = {english}
}

@article{isphording_pandemic_2021,
  title = {Pandemic Meets Pollution: {{Poor}} Air Quality Increases Deaths by {{COVID-19}}},
  shorttitle = {Pandemic Meets Pollution},
  author = {Isphording, Ingo E. and Pestel, Nico},
  year = 2021,
  month = jul,
  journal = {Journal of Environmental Economics and Management},
  volume = {108},
  pages = {102448},
  issn = {00950696},
  doi = {10.1016/j.jeem.2021.102448},
  urldate = {2021-06-10},
  abstract = {We study the impact of short-term exposure to ambient air pollution on the spread and severity of COVID-19 in Germany. We combine data at the county-by-day level on confirmed cases and deaths with information on local air quality and weather conditions. Following Deryugina et al. (2019), we instrument short-term variation in local concentrations of particulate matter (PM10) by region-specific daily variation in wind directions. We find significant positive effects of PM10 concentration on death numbers from four days before to ten days after the onset of symptoms. Specifically, for elderly patients (80+ years) an increase in ambient PM10 concentration by one standard deviation between two and four days after developing symptoms increases the number of deaths by 19 percent of a standard deviation. In addition, higher levels air pollution raise the number of confirmed cases of COVID-19 for all age groups. The timing of effects surrounding the onset of illness suggests that air pollution affects the severity of already-realized infections. We discuss the implications of our results for immediate policy levers to reduce the exposure and level of ambient air pollution, as well as for cost-benefit considerations of policies aiming at sustainable longer-term reductions of pollution levels.},
  langid = {english}
}

@article{jacob_remedial_2004,
  title = {Remedial {{Education}} and {{Student Achievement}}: {{A Regression-Discontinuity Analysis}}},
  shorttitle = {Remedial {{Education}} and {{Student Achievement}}},
  author = {Jacob, Brian A. and Lefgren, Lars},
  year = 2004,
  month = feb,
  journal = {The Review of Economics and Statistics},
  volume = {86},
  number = {1},
  pages = {226--244},
  issn = {0034-6535},
  doi = {10.1162/003465304323023778},
  abstract = {As standards and accountability have become increasingly prominent features of the educational landscape, educators have relied more on remedial programs such as summer school and grade retention to help low-achieving students meet minimum academic standards. Yet the evidence on the effectiveness of such programs is mixed, and prior research suffers from selection bias. However, recent school reform efforts in Chicago provide an opportunity to examine the causal impact of these remedial education programs. In 1996, the Chicago Public Schools instituted an accountability policy that tied summer school and promotional decisions to performance on standardized tests, which resulted in a highly nonlinear relationship between current achievement and the probability of attending summer school or being retained. Using a regression discontinuity design, we find that the net effect of these programs was to substantially increase academic achievement among third-graders, but not sixth-graders. In addition, contrary to conventional wisdom and prior research, we find that retention increases achievement for third-grade students and has little effect on math achievement for sixth-grade students.},
  keywords = {Chicago,Education,Grade retention,RDD,Summer schools,US}
}

@article{janke_air_2014,
  title = {Air Pollution, Avoidance Behaviour and Children's Respiratory Health: {{Evidence}} from {{England}}},
  shorttitle = {Air Pollution, Avoidance Behaviour and Children's Respiratory Health},
  author = {Janke, Katharina},
  year = 2014,
  month = dec,
  journal = {Journal of Health Economics},
  volume = {38},
  pages = {23--42},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2014.07.002},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{jans_economic_2018,
  title = {Economic Status, Air Quality, and Child Health: {{Evidence}} from Inversion Episodes},
  shorttitle = {Economic Status, Air Quality, and Child Health},
  author = {Jans, Jenny and Johansson, Per and Nilsson, J. Peter},
  year = 2018,
  month = sep,
  journal = {Journal of Health Economics},
  volume = {61},
  pages = {220--232},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2018.08.002},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{jans_economic_2018,
  title = {Economic Status, Air Quality, and Child Health: {{Evidence}} from Inversion Episodes},
  shorttitle = {Economic Status, Air Quality, and Child Health},
  author = {Jans, Jenny and Johansson, Per and Nilsson, J. Peter},
  year = 2018,
  month = sep,
  journal = {Journal of Health Economics},
  volume = {61},
  pages = {220--232},
  issn = {01676296},
  doi = {10.1016/j.jhealeco.2018.08.002},
  urldate = {2022-11-17},
  langid = {english}
}

@article{jia_is_2019,
  title = {Is {{China}}'s {{Pollution}} the {{Culprit}} for the {{Choking}} of {{South Korea}}? {{Evidence}} from the {{Asian Dust}}},
  shorttitle = {Is {{China}}'s {{Pollution}} the {{Culprit}} for the {{Choking}} of {{South Korea}}?},
  author = {Jia, Ruixue and Ku, Hyejin},
  year = 2019,
  month = nov,
  journal = {The Economic Journal},
  volume = {129},
  number = {624},
  pages = {3154--3188},
  issn = {0013-0133, 1468-0297},
  doi = {10.1093/ej/uez021},
  urldate = {2020-11-05},
  abstract = {Abstract             This paper studies the impact of air pollution spillover from China to South Korea. To isolate the effects of cross-border pollution spillover from that of locally generated pollution, we exploit within-South Korea and over-time variation in the incidence of Asian dust---a meteorological phenomenon exogenous to district--time cells in South Korea---together with temporal variations in China's air quality. We find that conditional on being exposed to Asian dust, increased pollution in China leads to increased mortality from respiratory and cardiovascular diseases in South Korean districts, with the most vulnerable being the elderly and children under five.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{jia_is_2019,
  title = {Is {{China}}'s {{Pollution}} the {{Culprit}} for the {{Choking}} of {{South Korea}}? {{Evidence}} from the {{Asian Dust}}},
  shorttitle = {Is {{China}}'s {{Pollution}} the {{Culprit}} for the {{Choking}} of {{South Korea}}?},
  author = {Jia, Ruixue and Ku, Hyejin},
  year = 2019,
  month = nov,
  journal = {The Economic Journal},
  volume = {129},
  number = {624},
  pages = {3154--3188},
  issn = {0013-0133, 1468-0297},
  doi = {10.1093/ej/uez021},
  urldate = {2022-11-17},
  abstract = {Abstract This paper studies the impact of air pollution spillover from China to South Korea. To isolate the effects of cross-border pollution spillover from that of locally generated pollution, we exploit within-South Korea and over-time variation in the incidence of Asian dust---a meteorological phenomenon exogenous to district--time cells in South Korea---together with temporal variations in China's air quality. We find that conditional on being exposed to Asian dust, increased pollution in China leads to increased mortality from respiratory and cardiovascular diseases in South Korean districts, with the most vulnerable being the elderly and children under five.},
  langid = {english}
}

@article{jiroutek_praise_2016,
  title = {In {{Praise}} of {{Confidence Intervals}}: {{Much More Informative Than}} {{{\emph{P}}}} {{Values Alone}}},
  shorttitle = {In {{Praise}} of {{Confidence Intervals}}},
  author = {Jiroutek, Michael R. and Turner, J. Rick},
  year = 2016,
  month = sep,
  journal = {The Journal of Clinical Hypertension},
  volume = {18},
  number = {9},
  pages = {955--957},
  issn = {15246175},
  doi = {10.1111/jch.12908},
  urldate = {2021-06-10},
  langid = {english}
}

@article{jiroutek_praise_2016,
  title = {In {{Praise}} of {{Confidence Intervals}}: {{Much More Informative Than}} {{{\emph{P}}}} {{Values Alone}}},
  shorttitle = {In {{Praise}} of {{Confidence Intervals}}},
  author = {Jiroutek, Michael R. and Turner, J. Rick},
  year = 2016,
  month = sep,
  journal = {The Journal of Clinical Hypertension},
  volume = {18},
  number = {9},
  pages = {955--957},
  issn = {15246175},
  doi = {10.1111/jch.12908},
  urldate = {2022-11-17},
  langid = {english}
}

@article{junger_imputation_2015,
  title = {Imputation of Missing Data in Time Series for Air Pollutants},
  author = {Junger, W. L. and {Ponce de Leon}, A.},
  year = 2015,
  month = feb,
  journal = {Atmospheric Environment},
  volume = {102},
  pages = {96--104},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2014.11.049},
  urldate = {2020-07-03},
  abstract = {Missing data are major concerns in epidemiological studies of the health effects of environmental air pollutants. This article presents an imputation-based method that is suitable for multivariate time series data, which uses the EM algorithm under the assumption of normal distribution. Different approaches are considered for filtering the temporal component. A simulation study was performed to assess validity and performance of proposed method in comparison with some frequently used methods. Simulations showed that when the amount of missing data was as low as 5\%, the complete data analysis yielded satisfactory results regardless of the generating mechanism of the missing data, whereas the validity began to degenerate when the proportion of missing values exceeded 10\%. The proposed imputation method exhibited good accuracy and precision in different settings with respect to the patterns of missing observations. Most of the imputations obtained valid results, even under missing not at random. The methods proposed in this study are implemented as a package called mtsdi for the statistical software system R.},
  langid = {english},
  keywords = {Air pollution,Brazil,Comparison imputation methods,EM,Imputation,Imputation method,Method,Missing data}
}

@article{junninen_methods_2004,
  title = {Methods for Imputation of Missing Values in Air Quality Data Sets},
  author = {Junninen, Heikki and Niska, Harri and Tuppurainen, Kari and Ruuskanen, Juhani and Kolehmainen, Mikko},
  year = 2004,
  month = jun,
  journal = {Atmospheric Environment},
  volume = {38},
  number = {18},
  pages = {2895--2907},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2004.02.026},
  urldate = {2020-07-28},
  abstract = {Methods for data imputation applicable to air quality data sets were evaluated in the context of univariate (linear, spline and nearest neighbour interpolation), multivariate (regression-based imputation (REGEM), nearest neighbour (NN), self-organizing map (SOM), multi-layer perceptron (MLP)), and hybrid methods of the previous by using simulated missing data patterns. Additionally, a multiple imputation procedure was considered in order to make comparison between single and multiple imputations schemes. Four statistical criteria were adopted: the index of agreement, the squared correlation coefficient (R2), the root mean square error and the mean absolute error with bootstrapped standard errors. The results showed that the performance of interpolation in respect to the length of gaps could be estimated separately for each variable of air quality by calculating a gradient and an exponent {$\alpha$} (Hurst exponent). This can be further utilised in hybrid approach in which the imputation has been performed either by interpolation or multivariate method depending on the length of gaps and variable under study. Among the multivariate methods, SOM and MLP performed slightly better than REGEM and NN methods. The advantage of SOM over the others was that it was less dependent on the actual location of the missing values. If priority is given to computational speed, however, NN can be recommended. The results in general showed that the slight improvement in the performances of multivariate methods can be achieved by using the hybridisation and more substantial one by using the multiple imputations where a final estimate is composed of the outputs of several multivariate fill-in methods.},
  langid = {english},
  keywords = {Air quality,Imputing,Missing data,Multivariate,Neural networks}
}

@article{kamenica_bayesian_2011,
  title = {Bayesian {{Persuasion}}},
  author = {Kamenica, Emir and Gentzkow, Matthew},
  year = 2011,
  month = oct,
  journal = {American Economic Review},
  volume = {101},
  number = {6},
  pages = {2590--2615},
  issn = {0002-8282},
  doi = {10.1257/aer.101.6.2590},
  abstract = {When is it possible for one person to persuade another to change her action? We consider a symmetric information model where a sender chooses a signal to reveal to a receiver, who then takes a noncontractible action that affects the welfare of both players. We derive necessary and sufficient conditions for the existence of a signal that strictly benefits the sender. We characterize sender-optimal signals. We examine comparative statics with respect to the alignment of the sender's and the receiver's preferences. Finally, we apply our results to persuasion by litigators, lobbyists, and salespeople. (JEL D72, D82, D83, K40, M31)},
  langid = {english},
  keywords = {and Illegal Behavior: General,and Voting Behavior,Asymmetric and Private Information,Belief,Communication,Elections,Information and Knowledge,Learning,Legal Procedure,Legislatures,Lobbying,Marketing,Political Processes: Rent-seeking,Search,the Legal System}
}

@article{kasy_forking_2021,
  title = {Of {{Forking Paths}} and {{Tied Hands}}: {{Selective Publication}} of {{Findings}}, and {{What Economists Should Do}} about {{It}}},
  shorttitle = {Of {{Forking Paths}} and {{Tied Hands}}},
  author = {Kasy, Maximilian},
  year = 2021,
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {175--192},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.175},
  abstract = {A key challenge for interpreting published empirical research is the fact that published findings might be selected by researchers or by journals. Selection might be based on criteria such as significance, consistency with theory, or the surprisingness of findings or their plausibility. Selection leads to biased estimates, reduced coverage of confidence intervals, and distorted posterior beliefs. I review methods for detecting and quantifying selection based on the distribution of p-values, systematic replication studies, and meta-studies. I then discuss the conflicting recommendations regarding selection result ing from alternative objectives, in particular, the validity of inference versus the relevance of findings for decision-makers. Based on this discussion, I consider various reform proposals, such as deemphasizing significance, pre-analysis plans, journals for null results and replication studies, and a functionally differentiated publication system. In conclusion, I argue that we need alternative foundations of statistics that go beyond the single-agent model of decision theory.},
  langid = {english},
  keywords = {Forking paths,Statistics}
}

@article{kaufman_association_2016,
  title = {Association between Air Pollution and Coronary Artery Calcification within Six Metropolitan Areas in the {{USA}} (the {{Multi-Ethnic Study}} of {{Atherosclerosis}} and {{Air Pollution}}): A Longitudinal Cohort Study},
  shorttitle = {Association between Air Pollution and Coronary Artery Calcification within Six Metropolitan Areas in the {{USA}} (the {{Multi-Ethnic Study}} of {{Atherosclerosis}} and {{Air Pollution}})},
  author = {Kaufman, Joel D. and Adar, Sara D. and Barr, R. Graham and Budoff, Matthew and Burke, Gregory L. and Curl, Cynthia L. and Daviglus, Martha L. and Roux, Ana V. Diez and Gassett, Amanda J. and Jacobs, David R. and Kronmal, Richard and Larson, Timothy V. and {Navas-Acien}, Ana and Olives, Casey and Sampson, Paul D. and Sheppard, Lianne and Siscovick, David S. and Stein, James H. and Szpiro, Adam A. and Watson, Karol E.},
  year = 2016,
  month = aug,
  journal = {The Lancet},
  volume = {388},
  number = {10045},
  pages = {696--704},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(16)00378-0},
  urldate = {2020-03-12},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Long-term exposure to fine particulate matter less than 2{$\cdot$}5 {$\mu$}m in diameter (PM\textsubscript{2{$\cdot$}5}) and traffic-related air pollutant concentrations are associated with cardiovascular risk. The disease process underlying these associations remains uncertain. We aim to assess association between long-term exposure to ambient air pollution and progression of coronary artery calcium and common carotid artery intima-media thickness.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}In this prospective 10-year cohort study, we repeatedly measured coronary artery calcium by CT in 6795 participants aged 45--84 years enrolled in the Multi-Ethnic Study of Atherosclerosis and Air Pollution (MESA Air) in six metropolitan areas in the USA. Repeated scans were done for nearly all participants between 2002 and 2005, for a subset of participants between 2005 and 2007, and for half of all participants between 2010 and 2012. Common carotid artery intima-media thickness was measured by ultrasound in all participants at baseline and in 2010--12 for 3459 participants. Residence-specific spatio-temporal pollution concentration models, incorporating community-specific measurements, agency monitoring data, and geographical predictors, estimated concentrations of PM\textsubscript{2{$\cdot$}5} and nitrogen oxides (NO\textsubscript{X}) between 1999 and 2012. The primary aim was to examine the association between both progression of coronary artery calcium and mean carotid artery intima-media thickness and long-term exposure to ambient air pollutant concentrations (PM\textsubscript{2{$\cdot$}5}, NO\textsubscript{X}, and black carbon) between examinations and within the six metropolitan areas, adjusting for baseline age, sex, ethnicity, socioeconomic characteristics, cardiovascular risk factors, site, and CT scanner technology.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}In this population, coronary calcium increased on average by 24 Agatston units per year (SD 58), and intima-media thickness by 12 {$\mu$}m per year (10), before adjusting for risk factors or air pollutant exposures. Participant-specific pollutant concentrations averaged over the years 2000--10 ranged from 9{$\cdot$}2--22{$\cdot$}6 {$\mu$}g PM\textsubscript{2{$\cdot$}5}/m{$^3$} and 7{$\cdot$}2--139{$\cdot$}2 parts per billion (ppb) NO\textsubscript{X}. For each 5 {$\mu$}g PM\textsubscript{2{$\cdot$}5}/m{$^3$} increase, coronary calcium progressed by 4{$\cdot$}1 Agatston units per year (95\% CI 1{$\cdot$}4--6{$\cdot$}8) and for each 40 ppb NO\textsubscript{X} coronary calcium progressed by 4{$\cdot$}8 Agatston units per year (0{$\cdot$}9--8{$\cdot$}7). Pollutant exposures were not associated with intima-media thickness change. The estimate for the effect of a 5 {$\mu$}g/m{$^3$} higher long-term exposure to PM\textsubscript{2{$\cdot$}5} in intima-media thickness was -0{$\cdot$}9 {$\mu$}m per year (95\% CI -3{$\cdot$}0 to 1{$\cdot$}3). For 40 ppb higher NO\textsubscript{X}, the estimate was 0{$\cdot$}2 {$\mu$}m per year (-1{$\cdot$}9 to 2{$\cdot$}4).{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Increased concentrations of PM\textsubscript{2{$\cdot$}5} and traffic-related air pollution within metropolitan areas, in ranges commonly encountered worldwide, are associated with progression in coronary calcification, consistent with acceleration of atherosclerosis. This study supports the case for global efforts of pollution reduction in prevention of cardiovascular diseases.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}US Environmental Protection Agency and US National Institutes of Health.{$<$}/p{$>$}},
  langid = {english}
}

@article{kim_air_2021,
  title = {Air {{Pollution}}, {{Health}}, and {{Avoidance Behavior}}: {{Evidence}} from {{South Korea}}},
  shorttitle = {Air {{Pollution}}, {{Health}}, and {{Avoidance Behavior}}},
  author = {Kim, Moon Joon},
  year = 2021,
  month = may,
  journal = {Environmental and Resource Economics},
  volume = {79},
  number = {1},
  pages = {63--91},
  issn = {0924-6460, 1573-1502},
  doi = {10.1007/s10640-021-00553-1},
  urldate = {2022-11-17},
  abstract = {Using detailed data on the beneficiaries of the Korean National Health Insurance Service (c.2006--2015), this paper estimates the health effects of air pollution in South Korea while controlling for avoidance behaviors. In particular, we investigate changes in respiratory hospitalization rates due to increases in PM10 and O{$\mkern1mu$}3 concentrations. To address the endogeneity of air pollution, this paper applies the historical average concentration of air pollution, which includes rich information about the meteorological and geographical factors that affect regional air pollution levels, as an instrumental variable and compares the results with other count data models. We find that a 10 {$\mu$}g/m3 increase in PM10 and a 10 ppb increase in O{$\mkern1mu$}3 lead to an increase in daily respiratory hospital visits of up to 10.39\% [95\% confidence interval (CI) 4.04--16.80] and 10.93\% (95\% CI 9.23--12.63), resulting in additional health care costs of US\$67 million and US\$70 million, respectively. This paper also shows that the effects of PM10 and O{$\mkern1mu$}3 are elevated in highly populated cities, children, and patients without chronic respiratory diseases.},
  langid = {english}
}

@article{kim_sensitivity_2013,
  title = {The Sensitivity of Health Effect Estimates from Time-Series Studies to Fine Particulate Matter Component Sampling Schedule},
  author = {Kim, Sun-young and Sheppard, Lianne and Hannigan, Michael P. and Dutton, Steven J. and Peel, Jennifer L. and Clark, Maggie L. and Vedal, Sverre},
  year = 2013,
  month = sep,
  journal = {Journal of Exposure Science and Environmental Epidemiology; Tuxedo},
  volume = {23},
  number = {5},
  pages = {481--6},
  publisher = {Nature Publishing Group},
  address = {Tuxedo, United States, Tuxedo},
  issn = {15590631},
  doi = {http://dx.doi.org.ezproxy.cul.columbia.edu/10.1038/jes.2013.28},
  urldate = {2020-08-03},
  abstract = {The US Environmental Protection Agency air pollution monitoring data have been a valuable resource commonly used for investigating the associations between short-term exposures to PM2.5 chemical components and human health. However, the temporally sparse sampling on every third or sixth day may affect health effect estimation. We examined the impact of non-daily monitoring data on health effect estimates using daily data from the Denver Aerosol Sources and Health (DASH) study. Daily concentrations of four PM2.5 chemical components (elemental and organic carbon, sulfate, and nitrate) and hospital admission counts from 2003 through 2007 were used. Three every-third-day time series were created from the daily DASH monitoring data, imitating the US Speciation Trend Network (STN) monitoring schedule. A fourth, partly irregular, every-third-day time series was created by matching existing sampling days at a nearby STN monitor. Relative risks (RRs) of hospital admissions for PM2.5 components at lags 0-3 were estimated for each data set, adjusting for temperature, relative humidity, longer term temporal trends, and day of week using generalized additive models, and compared across different sampling schedules. The estimated RRs varied somewhat between the non-daily and daily sampling schedules and between the four non-daily schedules, and in some instances could lead to different conclusions. It was not evident which features of the data or analysis were responsible for the variation in effect estimates, although seeing similar variability in resampled data sets with relaxation of the every-third-day constraint suggests that limited power may have had a role. The use of non-daily monitoring data can influence interpretation of estimated effects of PM2.5 components on hospital admissions in time-series studies.},
  copyright = {Copyright Nature Publishing Group Sep 2013},
  langid = {english},
  keywords = {Air pollution,Estimate,Health,Missing data}
}

@article{klaauw_estimating_2002,
  title = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}: {{A Regression}}--{{Discontinuity Approach}}*},
  shorttitle = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}},
  author = {Klaauw, Wilbert Van Der},
  year = 2002,
  journal = {International Economic Review},
  volume = {43},
  number = {4},
  pages = {1249--1287},
  issn = {1468-2354},
  doi = {10.1111/1468-2354.t01-1-00055},
  abstract = {An important problem faced by colleges and universities, that of evaluating the effect of their financial aid offers on student enrollment decisions, is complicated by the likely endogeneity of the aid offer variable in a student enrollment equation. This article shows how discontinuities in an East Coast college's aid assignment rule can be exploited to obtain credible estimates of the aid effect without having to rely on arbitrary exclusion restrictions and functional form assumptions. Semiparametric estimates based on a regression--discontinuity (RD) approach affirm the importance of financial aid as an effective instrument in competing with other colleges for students.},
  langid = {english}
}

@article{klemm_impact_2011,
  title = {The {{Impact}} of {{Frequency}} and {{Duration}} of {{Air Quality Monitoring}}: {{Atlanta}}, {{GA}}, {{Data Modeling}} of {{Air Pollution}} and {{Mortality}}},
  shorttitle = {The {{Impact}} of {{Frequency}} and {{Duration}} of {{Air Quality Monitoring}}},
  author = {Klemm, Rebecca J. and Thomas, Eddie L. and Wyzga, Ronald E.},
  year = 2011,
  month = nov,
  journal = {Journal of the Air \& Waste Management Association},
  volume = {61},
  number = {11},
  pages = {1281--1291},
  issn = {1096-2247, 2162-2906},
  doi = {10.1080/10473289.2011.617648},
  urldate = {2020-08-03},
  langid = {english},
  keywords = {Air pollution,Estimate,Missing data,Mortality}
}

@article{knittel_caution_2016,
  title = {Caution, {{Drivers}}! {{Children Present}}: {{Traffic}}, {{Pollution}}, and {{Infant Health}}},
  shorttitle = {Caution, {{Drivers}}! {{Children Present}}},
  author = {Knittel, Christopher R. and Miller, Douglas L. and Sanders, Nicholas J.},
  year = 2016,
  month = may,
  journal = {Review of Economics and Statistics},
  volume = {98},
  number = {2},
  pages = {350--366},
  issn = {0034-6535, 1530-9142},
  doi = {10.1162/REST_a_00548},
  urldate = {2020-11-05},
  abstract = {We investigate the effects of automobile congestion on ambient air pollution and local infant mortality rates using data from California spanning 2002 to 2007. Constructing instrumental variables (IV) using the relationship of traffic, weather conditions, and pollutants, we show that particulate matter, even at modern levels, has large marginal effects on weekly infant mortality rates, especially for premature or low birthweight infants. We also find suggestive evidence of large effects for carbon monoxide, though results are imprecise. Finally, we check estimate sensitivity to nonclassical measurement error in local pollution and show that our IV results are robust to such concerns.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{knittel_caution_2016,
  title = {Caution, {{Drivers}}! {{Children Present}}: {{Traffic}}, {{Pollution}}, and {{Infant Health}}},
  shorttitle = {Caution, {{Drivers}}! {{Children Present}}},
  author = {Knittel, Christopher R. and Miller, Douglas L. and Sanders, Nicholas J.},
  year = 2016,
  month = may,
  journal = {Review of Economics and Statistics},
  volume = {98},
  number = {2},
  pages = {350--366},
  issn = {0034-6535, 1530-9142},
  doi = {10.1162/REST_a_00548},
  urldate = {2022-11-17},
  abstract = {We investigate the effects of automobile congestion on ambient air pollution and local infant mortality rates using data from California spanning 2002 to 2007. Constructing instrumental variables (IV) using the relationship of traffic, weather conditions, and pollutants, we show that particulate matter, even at modern levels, has large marginal effects on weekly infant mortality rates, especially for premature or low birthweight infants. We also find suggestive evidence of large effects for carbon monoxide, though results are imprecise. Finally, we check estimate sensitivity to nonclassical measurement error in local pollution and show that our IV results are robust to such concerns.},
  langid = {english}
}

@article{koop_measuring_2004,
  title = {Measuring the Health Effects of Air Pollution: To What Extent Can We Really Say That People Are Dying from Bad Air?},
  shorttitle = {Measuring the Health Effects of Air Pollution},
  author = {Koop, Gary and Tole, Lise},
  year = 2004,
  month = jan,
  journal = {Journal of Environmental Economics and Management},
  volume = {47},
  number = {1},
  pages = {30--54},
  issn = {0095-0696},
  doi = {10.1016/S0095-0696(03)00075-5},
  urldate = {2020-11-02},
  abstract = {Estimation of the effects of environmental impacts is a major focus of current theoretical and policy research in environmental economics. Such estimates are used to set regulatory standards for pollution exposure; design appropriate environmental protection and damage mitigation strategies; guide the assessment of environmental impacts; and measure public willingness to pay for environmental amenities. It is a truism that the effectiveness of such strategies depends crucially on the quality of the estimates used to inform them. However, this paper argues that in respect to at least one area of the empirical literature---the estimation of the health impacts of air pollution using daily time series data---existing estimates are questionable and thus have limited relevance for environmental decision-making. By neglecting the issue of model uncertainty---or which models, among the myriad of possible models researchers should choose from to estimate health effects---most studies overstate confidence in their chosen model and underestimate the evidence from other models, thereby greatly enhancing the risk of obtaining uncertain and inaccurate results. This paper discusses the importance of model uncertainty for accurate estimation of the health effects of air pollution and demonstrates its implications in an exercise that models pollution-mortality impacts using a new and comprehensive data set for Toronto, Canada. The main empirical finding of the paper is that standard deviations for air pollution-mortality impacts become very large when model uncertainty is incorporated into the analysis. Indeed they become so large as to question the plausibility of previously measured links between air pollution and mortality. Although applied to the estimation of the effects of air pollution, the general message of this paper---that proper treatment of model uncertainty critically determines the accuracy of the resulting estimates---applies to many studies that seek to estimate environmental effects.},
  langid = {english},
  keywords = {Air pollution,Bayesian model averaging,EPA,Mortality}
}

@article{kraft2020interpreting,
  title = {Interpreting Effect Sizes of Education Interventions},
  author = {Kraft, Matthew A},
  year = 2020,
  journal = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {Sage Publications Sage CA: Los Angeles, CA}
}

@techreport{lal_how_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}? {{Practical Advice}} Based on {{Over}} 60 {{Replicated Studies}}},
  shorttitle = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}?},
  author = {Lal, Apoorva and Lockhart, Mackenzie William and Xu, Yiqing and Zu, Ziwen},
  year = 2021,
  month = aug,
  number = {ID 3905329},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.3905329},
  abstract = {Instrumental variable (IV) strategies are commonly used in political science to establish causal relationships, yet the identifying assumptions required by an IV design are demanding and it remains challenging for researchers to evaluate their plausibility. We replicate 61 papers published in three top journals in political science from the past decade (2011-2020) and document several troubling patterns: (1) researchers often miscalculate the first-stage F statistics, overestimating the strength of their IVs; (2) most researchers rely on classical asymptotic standard errors, which often severely underestimate the uncertainties around the two-stage-least-squared (2SLS) estimates; (3) in the majority of the replicated studies, the 2SLS estimates are much bigger than the ordinary-least-squared estimates, and their ratio is negatively correlated with the strength of the IVs in studies where the IVs are not experimentally generated, suggesting potential violations of the exclusion restriction; such a relationship is much weaker with experimentally generated IVs. To improve practice, we provide a checklist for researchers to avoid these pitfalls and recommend a zero-first-stage test and a local-to-zero procedure to guard against failure of the identifying assumptions.},
  langid = {english},
  keywords = {exclusion restriction,instrumental variables,replications,two-stage-least-squared,weak IV,zero-first-stage test}
}

@article{lal_how_2021,
  title = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}? {{Practical Advice}} Based on {{Over}} 60 {{Replicated Studies}}},
  shorttitle = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}?},
  author = {Lal, Apoorva and Lockhart, Mackenzie William and Xu, Yiqing and Zu, Ziwen},
  year = 2021,
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3905329},
  urldate = {2022-11-17},
  abstract = {Instrumental variable (IV) strategies are commonly used in political science to establish causal relationships, yet the identifying assumptions required by an IV design are demanding and it remains challenging for researchers to evaluate their plausibility. We replicate 61 papers published in three top journals in political science from the past decade (2010-2020) and document several troubling patterns: (1) researchers often miscalculate the first-stage F statistics, overestimating the strength of their IVs; (2) most researchers rely on classical asymptotic standard errors, which often severely underestimate the uncertainties around the two-stage-least-squares (2SLS) estimates; (3) in the majority of the replicated studies, the 2SLS estimates are much bigger than the ordinary-least-squares estimates, and their ratio is negatively correlated with the strength of the IVs in studies where the IVs are not experimentally generated, suggesting potential violations of the exclusion restriction. To improve practice, we provide a checklist for researchers to avoid these pitfalls and recommend a zero-first-stage test and a local-to-zero procedure to guard against failures of the identifying assumptions.},
  langid = {english}
}

@article{lalonde_evaluating_1986,
  title = {Evaluating the {{Econometric Evaluations}} of {{Training Programs}} with {{Experimental Data}}},
  author = {LaLonde, Robert J.},
  year = 1986,
  journal = {The American Economic Review},
  volume = {76},
  number = {4},
  pages = {604--620},
  publisher = {American Economic Association},
  issn = {0002-8282},
  abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.}
}

@article{lavaine_energy_2017,
  title = {Energy {{Production}} and {{Health Externalities}}: {{Evidence}} from {{Oil Refinery Strikes}} in {{France}}},
  shorttitle = {Energy {{Production}} and {{Health Externalities}}},
  author = {Lavaine, Emmanuelle and Neidell, Matthew},
  year = 2017,
  month = jun,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {4},
  number = {2},
  pages = {447--477},
  publisher = {The University of Chicago Press},
  issn = {2333-5955},
  doi = {10.1086/691554},
  abstract = {This paper examines the effect of energy production on health using a recent strike that affected oil refineries in France as a natural experiment. First, we show that the temporary reduction in refining led to a significant reduction in sulfur dioxide (SO2) concentrations. Second, this shock significantly increased birth weight and gestational age of newborns, particularly for those exposed to the strike during the first and third trimesters of pregnancy, and decreased asthma and bronchitis admissions. Back-of-the-envelope calculations suggest that a 1-unit (or 26\%) decline in monthly SO2 leads to an \texteuro 89 million increase in lifetime earnings per birth-year cohort. This externality from oil refineries should be an important part of policy discussions surrounding the production of energy.},
  keywords = {Air pollution,Birth outcomes,Difference in difference,Q40,Q51,Q53}
}

@article{le_tertre_short-term_2002,
  title = {Short-Term Effects of Particulate Air Pollution on Cardiovascular Diseases in Eight {{European}} Cities},
  author = {Le Tertre, A},
  year = 2002,
  month = oct,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {56},
  number = {10},
  pages = {773--779},
  issn = {0143005X},
  doi = {10.1136/jech.56.10.773},
  urldate = {2022-11-17},
  langid = {english}
}

@article{le2002short,
  title = {Short-Term Effects of Particulate Air Pollution on Cardiovascular Diseases in Eight {{European}} Cities},
  author = {Le Tertre, A and Medina, S and Samoli, E and Forsberg, B and Michelozzi, P and Boumghar, A and Vonk, {\relax JM} and Bellini, A and Atkinson, R and Ayres, {\relax JG} and others},
  year = 2002,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {56},
  number = {10},
  pages = {773--779},
  publisher = {BMJ Publishing Group Ltd}
}

@article{leamer_lets_2021,
  title = {Let's {{Take}} the {{Con Out}} of {{Econometrics}}},
  author = {Leamer, Edward E},
  year = 2021,
  pages = {14},
  langid = {english},
  keywords = {Inflated effects,To read}
}

@article{lee_regression_2010,
  title = {Regression {{Discontinuity Designs}} in {{Economics}}},
  author = {Lee, David S. and Lemieux, Thomas},
  year = 2010,
  journal = {Journal of Economic Literature},
  volume = {48},
  number = {2},
  pages = {281--355},
  publisher = {American Economic Association},
  issn = {0022-0515},
  abstract = {This paper provides an introduction and "user guide" to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a "quasi-experimental" design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD.},
  keywords = {Literature review,RDD}
}

@article{letertre_shortterm_2002,
  title = {Short-Term Effects of Particulate Air Pollution on Cardiovascular Diseases in Eight {{European}} Cities},
  author = {Le Tertre, A. and Medina, S. and Samoli, E. and Forsberg, B. and Michelozzi, P. and Boumghar, A. and Vonk, J. M. and Bellini, A. and Atkinson, R. and Ayres, J. G. and Sunyer, J. and Schwartz, J. and Katsouyanni, K.},
  year = 2002,
  month = oct,
  journal = {Journal of Epidemiology and Community Health},
  volume = {56},
  number = {10},
  pages = {773--779},
  issn = {0143-005X},
  doi = {10.1136/jech.56.10.773},
  abstract = {STUDY OBJECTIVE: As part of the APHEA project this study examined the association between airborne particles and hospital admissions for cardiac causes (ICD9 390-429) in eight European cities (Barcelona, Birmingham, London, Milan, the Netherlands, Paris, Rome, and Stockholm). All admissions were studied, as well as admissions stratified by age. The association for ischaemic heart disease (ICD9 410-413) and stroke (ICD9 430-438) was also studied, also stratified by age. DESIGN: Autoregressive Poisson models were used that controlled for long term trend, season, influenza epidemics, and meteorology to assess the short-term effects of particles in each city. The study also examined confounding by other pollutants. City specific results were pooled in a second stage regression to obtain more stable estimates and examine the sources of heterogeneity. MAIN RESULTS: The pooled percentage increases associated with a 10 micro g/m(3) increase in PM(10) and black smoke were respectively 0.5\% (95\% CI: 0.2 to 0.8) and 1.1\% (95\% CI: 0.4 to 1.8) for cardiac admissions of all ages, 0.7\% (95\% CI: 0.4 to 1.0) and 1.3\% (95\% CI: 0.4 to 2.2) for cardiac admissions over 65 years, and, 0.8\% (95\% CI: 0.3 to 1.2) and 1.1\% (95\% CI: 0.7 to 1.5) for ischaemic heart disease over 65 years. The effect of PM(10) was little changed by control for ozone or SO(2), but was substantially reduced (CO) or eliminated (NO(2)) by control for other traffic related pollutants. The effect of black smoke remained practically unchanged controlling for CO and only somewhat reduced controlling for NO(2). CONCLUSIONS: These effects of particulate air pollution on cardiac admissions suggest the primary effect is likely to be mainly attributable to diesel exhaust. Results for ischaemic heart disease below 65 years and for stroke over 65 years were inconclusive.},
  langid = {english},
  pmcid = {PMC1732027},
  pmid = {12239204},
  keywords = {Adult,Aged,Air Pollutants,Cardiovascular Diseases,Environmental Exposure,Europe,Hospitalization,Humans,Middle Aged,Myocardial Ischemia,Poisson Distribution,Smoke,Stroke,Urban Health,Vehicle Emissions}
}

@article{lewbel_identification_2019,
  title = {The {{Identification Zoo}}: {{Meanings}} of {{Identification}} in {{Econometrics}}},
  shorttitle = {The {{Identification Zoo}}},
  author = {Lewbel, Arthur},
  year = 2019,
  month = dec,
  journal = {Journal of Economic Literature},
  volume = {57},
  number = {4},
  pages = {835--903},
  issn = {0022-0515},
  doi = {10.1257/jel.20181361},
  urldate = {2021-03-09},
  abstract = {Over two dozen different terms for identification appear in the econometrics literature, including set identification, causal identification, local identification, generic identification, weak identification, identification at infinity, and many more. This survey (i) gives a new framework unifying existing definitions of point identification; (ii) summarizes and compares the zooful of different terms associated with identification that appear in the literature; and (iii) discusses concepts closely related to identification, such as normalizations and the differences in identification between structural models and causal, reduced form models.},
  langid = {english},
  keywords = {Econometrics Single Equation Models,Single Variables: General Econometric Modeling: General}
}

@misc{linden_retrodesign_2019,
  title = {{{RETRODESIGN}}: {{Stata}} Module to Compute Type-{{S}} ({{Sign}}) and Type-{{M}} ({{Magnitude}}) Errors},
  shorttitle = {{{RETRODESIGN}}},
  author = {Linden, Ariel},
  year = 2019,
  month = oct,
  abstract = {retrodesign computes power, type-S, and type-M errors for one or more specified effect sizes. A type-S (sign) error indicates the probability of an effect size estimate being in the wrong direction, and a type-M (magnitude) error indicates the factor by which the magnitude of an effect might be overestimated -- given that the test statistic is statistically significant (Gelman and Carlin 2014). Gelman and Carlin (2014) propose computing the type-M error using the Student's t distribution while Lu, Qiu, and Deng (2019) propose a closed form solution for computing the type-M error. Both methods are implemented in retrodesign. retrodesign produces results identical to those computed in the retrodesign package for R.},
  howpublished = {Boston College Department of Economics},
  langid = {english},
  keywords = {design calculation,power analysis,replication crisis,Stata,statistical significance,type M error,type S error}
}

@book{lipsey2001practical,
  title = {Practical Meta-Analysis.},
  author = {Lipsey, Mark W and Wilson, David B},
  year = 2001,
  publisher = {SAGE publications, Inc}
}

@book{little_statistical_1979,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = 1979,
  edition = {Third edition, 2019},
  publisher = {John Wiley \& Sons},
  abstract = {An up-to-date, comprehensive treatment of a classic text on missing data in statisticsThe topic of missing data has gained considerable attention in recent decades. This new edition by two acknowledged experts on the subject offers an up-to-date account of practical methodology for handling missing data problems. Blending theory and application, authors Roderick Little and Donald Rubin review historical approaches to the subject and describe simple methods for multivariate analysis with missing values. They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing data mechanism, and then they apply the theory to a wide range of important missing data problems.Statistical Analysis with Missing Data, Third Edition starts by introducing readers to the subject and approaches toward solving it. It looks at the patterns and mechanisms that create the missing data, as well as a taxonomy of missing data. It then goes on to examine missing data in experiments, before discussing complete-case and available-case analysis, including weighting methods. The new edition expands its coverage to include recent work on topics such as nonresponse in sample surveys, causal inference, diagnostic methods, and sensitivity analysis, among a host of other topics.  An updated ``classic'' written by renowned authorities on the subject Features over 150 exercises (including many new ones) Covers recent work on important methods like multiple imputation, robust alternatives to weighting, and Bayesian methods Revises previous topics based on past student feedback and class experience Contains an updated and expanded bibliography  The authors were awarded The Karl Pearson Prize in 2017 by the International Statistical Institute, for a research contribution that has had profound influence on statistical theory, methodology or applications. Their work "has been no less than defining and transforming." (ISI)Statistical Analysis with Missing Data, Third Edition is an ideal textbook for upper undergraduate and/or beginning graduate level students of the subject. It is also an excellent source of information for applied statisticians and practitioners in government and industry.},
  googlebooks = {BemMDwAAQBAJ},
  isbn = {978-0-470-52679-8},
  langid = {english},
  keywords = {Handbook,Missing data}
}

@article{liu_ambient_2019,
  title = {Ambient {{Particulate Air Pollution}} and {{Daily Mortality}} in 652 {{Cities}}},
  author = {Liu, Cong and Chen, Renjie and Sera, Francesco and {Vicedo-Cabrera}, Ana M. and Guo, Yuming and Tong, Shilu and Coelho, Micheline S.Z.S. and Saldiva, Paulo H.N. and Lavigne, Eric and Matus, Patricia and Valdes Ortega, Nicolas and Osorio Garcia, Samuel and Pascal, Mathilde and Stafoggia, Massimo and Scortichini, Matteo and Hashizume, Masahiro and Honda, Yasushi and {Hurtado-D{\'i}az}, Magali and Cruz, Julio and Nunes, Baltazar and Teixeira, Jo{\~a}o P. and Kim, Ho and Tobias, Aurelio and {\'I}{\~n}iguez, Carmen and Forsberg, Bertil and {\AA}str{\"o}m, Christofer and Ragettli, Martina S. and Guo, Yue-Leon and Chen, Bing-Yu and Bell, Michelle L. and Wright, Caradee Y. and Scovronick, Noah and Garland, Rebecca M. and Milojevic, Ai and Kysel{\'y}, Jan and Urban, Ale{\v s} and Orru, Hans and Indermitte, Ene and Jaakkola, Jouni J.K. and Ryti, Niilo R.I. and Katsouyanni, Klea and Analitis, Antonis and Zanobetti, Antonella and Schwartz, Joel and Chen, Jianmin and Wu, Tangchun and Cohen, Aaron and Gasparrini, Antonio and Kan, Haidong},
  year = 2019,
  month = aug,
  journal = {New England Journal of Medicine},
  volume = {381},
  number = {8},
  pages = {705--715},
  issn = {0028-4793, 1533-4406},
  doi = {10.1056/NEJMoa1817364},
  urldate = {2020-11-05},
  abstract = {BACKGROUND The systematic evaluation of the results of time-series studies of air pollution is challenged by differences in model specification and publication bias. METHODS We evaluated the associations of inhalable particulate matter (PM) with an aerodynamic diameter of 10 {$\mu$}m or less (PM10) and fine PM with an aerodynamic diameter of 2.5 {$\mu$}m or less (PM2.5) with daily all-cause, cardiovascular, and respiratory mortality across multiple countries or regions. Daily data on mortality and air pollution were collected from 652 cities in 24 countries or regions. We used overdispersed generalized additive models with random-effects meta-analysis to investigate the associations. Two-pollutant models were fitted to test the robustness of the associations. Concentration--response curves from each city were pooled to allow global estimates to be derived. The authors' full names, academic degrees, and affiliations are listed in the Appendix. Address reprint requests to Dr. Kan at P.O. Box 249, 130 Dong-An Road, Shanghai 200032, China, or at \-kanh@ \-fudan.\-edu.\-cn. Drs. Liu and R. Chen and Drs. Gasparrini and Kan contributed equally to this article. N Engl J Med 2019;381:705-15. DOI: 10.1056/NEJMoa1817364 Copyright \copyright{} 2019 Massachusetts Medical Society. RESULTS On average, an increase of 10 {$\mu$}g per cubic meter in the 2-day moving average of PM10 concentration, which represents the average over the current and previous day, was associated with increases of 0.44\% (95\% confidence interval [CI], 0.39 to 0.50) in daily all-cause mortality, 0.36\% (95\% CI, 0.30 to 0.43) in daily cardiovascular mortality, and 0.47\% (95\% CI, 0.35 to 0.58) in daily respiratory mortality. The corresponding increases in daily mortality for the same change in PM2.5 concentration were 0.68\% (95\% CI, 0.59 to 0.77), 0.55\% (95\% CI, 0.45 to 0.66), and 0.74\% (95\% CI, 0.53 to 0.95). These associations remained significant after adjustment for gaseous pollutants. Associations were stronger in locations with lower annual mean PM concentrations and higher annual mean temperatures. The pooled concentration--response curves showed a consistent increase in daily mortality with increasing PM concentration, with steeper slopes at lower PM concentrations. CONCLUSIONS Our data show independent associations between short-term exposure to PM10 and PM2.5 and daily all-cause, cardiovascular, and respiratory mortality in more than 600 cities across the globe. These data reinforce the evidence of a link between mortality and PM concentration established in regional and local studies. (Funded by the National Natural Science Foundation of China and others.)},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{liu_ambient_2019,
  title = {Ambient {{Particulate Air Pollution}} and {{Daily Mortality}} in 652 {{Cities}}},
  author = {Liu, Cong and Chen, Renjie and Sera, Francesco and {Vicedo-Cabrera}, Ana M. and Guo, Yuming and Tong, Shilu and Coelho, Micheline S.Z.S. and Saldiva, Paulo H.N. and Lavigne, Eric and Matus, Patricia and Valdes Ortega, Nicolas and Osorio Garcia, Samuel and Pascal, Mathilde and Stafoggia, Massimo and Scortichini, Matteo and Hashizume, Masahiro and Honda, Yasushi and {Hurtado-D{\'i}az}, Magali and Cruz, Julio and Nunes, Baltazar and Teixeira, Jo{\~a}o P. and Kim, Ho and Tobias, Aurelio and {\'I}{\~n}iguez, Carmen and Forsberg, Bertil and {\AA}str{\"o}m, Christofer and Ragettli, Martina S. and Guo, Yue-Leon and Chen, Bing-Yu and Bell, Michelle L. and Wright, Caradee Y. and Scovronick, Noah and Garland, Rebecca M. and Milojevic, Ai and Kysel{\'y}, Jan and Urban, Ale{\v s} and Orru, Hans and Indermitte, Ene and Jaakkola, Jouni J.K. and Ryti, Niilo R.I. and Katsouyanni, Klea and Analitis, Antonis and Zanobetti, Antonella and Schwartz, Joel and Chen, Jianmin and Wu, Tangchun and Cohen, Aaron and Gasparrini, Antonio and Kan, Haidong},
  year = 2019,
  month = aug,
  journal = {New England Journal of Medicine},
  volume = {381},
  number = {8},
  pages = {705--715},
  issn = {0028-4793, 1533-4406},
  doi = {10.1056/NEJMoa1817364},
  urldate = {2022-11-17},
  abstract = {BACKGROUND The systematic evaluation of the results of time-series studies of air pollution is challenged by differences in model specification and publication bias. METHODS We evaluated the associations of inhalable particulate matter (PM) with an aerodynamic diameter of 10 {$\mu$}m or less (PM10) and fine PM with an aerodynamic diameter of 2.5 {$\mu$}m or less (PM2.5) with daily all-cause, cardiovascular, and respiratory mortality across multiple countries or regions. Daily data on mortality and air pollution were collected from 652 cities in 24 countries or regions. We used overdispersed generalized additive models with random-effects meta-analysis to investigate the associations. Two-pollutant models were fitted to test the robustness of the associations. Concentration--response curves from each city were pooled to allow global estimates to be derived. The authors' full names, academic degrees, and affiliations are listed in the Appendix. Address reprint requests to Dr. Kan at P.O. Box 249, 130 Dong-An Road, Shanghai 200032, China, or at \-kanh@ \-fudan.\-edu.\-cn. Drs. Liu and R. Chen and Drs. Gasparrini and Kan contributed equally to this article. N Engl J Med 2019;381:705-15. DOI: 10.1056/NEJMoa1817364 Copyright \copyright{} 2019 Massachusetts Medical Society. RESULTS On average, an increase of 10 {$\mu$}g per cubic meter in the 2-day moving average of PM10 concentration, which represents the average over the current and previous day, was associated with increases of 0.44\% (95\% confidence interval [CI], 0.39 to 0.50) in daily all-cause mortality, 0.36\% (95\% CI, 0.30 to 0.43) in daily cardiovascular mortality, and 0.47\% (95\% CI, 0.35 to 0.58) in daily respiratory mortality. The corresponding increases in daily mortality for the same change in PM2.5 concentration were 0.68\% (95\% CI, 0.59 to 0.77), 0.55\% (95\% CI, 0.45 to 0.66), and 0.74\% (95\% CI, 0.53 to 0.95). These associations remained significant after adjustment for gaseous pollutants. Associations were stronger in locations with lower annual mean PM concentrations and higher annual mean temperatures. The pooled concentration--response curves showed a consistent increase in daily mortality with increasing PM concentration, with steeper slopes at lower PM concentrations. CONCLUSIONS Our data show independent associations between short-term exposure to PM10 and PM2.5 and daily all-cause, cardiovascular, and respiratory mortality in more than 600 cities across the globe. These data reinforce the evidence of a link between mortality and PM concentration established in regional and local studies. (Funded by the National Natural Science Foundation of China and others.)},
  langid = {english}
}

@article{liu_effect_2017,
  title = {The Effect of the Driving Restriction Policy on Public Health in {{Beijing}}},
  author = {Liu, Yan and Yan, Zhijun and Liu, Su and Wu, Yuting and Gan, Qingmei and Dong, Chao},
  year = 2017,
  month = jan,
  journal = {Natural Hazards},
  volume = {85},
  number = {2},
  pages = {751--762},
  issn = {0921-030X, 1573-0840},
  doi = {10.1007/s11069-016-2602-8},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{liu_effect_2021,
  title = {Effect of Air Pollution on Health Care Expenditure: {{Evidence}} from Respiratory Diseases},
  shorttitle = {Effect of Air Pollution on Health Care Expenditure},
  author = {Liu, Ya-Ming and Ao, Chon-Kit},
  year = 2021,
  month = apr,
  journal = {Health Economics},
  volume = {30},
  number = {4},
  pages = {858--875},
  issn = {1057-9230, 1099-1050},
  doi = {10.1002/hec.4221},
  urldate = {2022-11-17},
  abstract = {Recent reports show that at least 95\% of the world's population is breathing polluted air. However, the impact of air quality on air pollution-related medical expenditure and utilization is sparse. This study estimates the shortterm health care cost impacts of air pollution using a meteorological phenomenon---thermal inversion---as an instrumental variable for air quality. Using information on outpatient care for respiratory diseases from universal health insurance claim data in Taiwan during 2006--2012, our estimates suggest that a one-unit reduction in the air quality index (AQI) leads to NT\$2.3 billion (nearly US\$74 million) of savings in respiratory-related outpatient expenditure per year. Given that the average AQI is equal to 32 during our study period, completely removing air pollution would reduce the national health expenditure by approximately 8\% annually. Our results provide the important implication that the cost of controlling air pollutant emissions can be offset by curtailing health care expenditure.},
  langid = {english}
}

@article{lu_note_2019,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = 2019,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {72},
  number = {1},
  pages = {1--17},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12132},
  urldate = {2021-06-03},
  abstract = {Motivated by the recent replication and reproducibility crisis, Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641) advocated focusing on controlling for Type S/M errors, instead of the classic Type I/II errors, when conducting hypothesis testing. In this paper, we aim to fill several theoretical gaps in the methodology proposed by Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641). In particular, we derive the closed-form expression for the expected Type M error, and study the mathematical properties of the probability of Type S error as well as the expected Type M error, such as monotonicity. We demonstrate the advantages of our results through numerical and empirical examples.},
  copyright = {\copyright{} 2018 The British Psychological Society},
  langid = {english},
  keywords = {design calculation,monotonicity,p-value,power calculation,replication,reproducibility,statistical significance}
}

@article{lu_note_2019,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = 2019,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {72},
  number = {1},
  pages = {1--17},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12132},
  abstract = {Motivated by the recent replication and reproducibility crisis, Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641) advocated focusing on controlling for Type S/M errors, instead of the classic Type I/II errors, when conducting hypothesis testing. In this paper, we aim to fill several theoretical gaps in the methodology proposed by Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641). In particular, we derive the closed-form expression for the expected Type M error, and study the mathematical properties of the probability of Type S error as well as the expected Type M error, such as monotonicity. We demonstrate the advantages of our results through numerical and empirical examples.},
  copyright = {\copyright{} 2018 The British Psychological Society},
  langid = {english},
  keywords = {Maths}
}

@article{lu_note_2019,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = 2019,
  month = feb,
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {72},
  number = {1},
  pages = {1--17},
  issn = {00071102},
  doi = {10.1111/bmsp.12132},
  urldate = {2022-11-17},
  langid = {english}
}

@article{maniadis_one_2014,
  title = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}: {{New Evidence}} on {{Anchoring Effects}}},
  shorttitle = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}},
  author = {Maniadis, Zacharias and Tufano, Fabio and List, John A.},
  year = 2014,
  month = jan,
  journal = {American Economic Review},
  volume = {104},
  number = {1},
  pages = {277--290},
  issn = {0002-8282},
  doi = {10.1257/aer.104.1.277},
  abstract = {Some researchers have argued that anchoring in economic valuations casts doubt on the assumption of consistent and stable preferences. We present new evidence that explores the strength of certain anchoring results. We then present a theoretical framework that provides insights into why we should be cautious of initial empirical findings in general. The model importantly highlights that the rate of false positives depends not only on the observed significance level, but also on statistical power, research priors, and the number of scholars exploring the question. Importantly, a few independent replications dramatically increase the chances that the original finding is true.},
  langid = {english},
  keywords = {Consumer Economics: Empirical Analysis,Design of Experiments: Laboratory,Example,Individual}
}

@misc{mayer_missranger_2019,
  title = {{{missRanger}}: {{Fast Imputation}} of {{Missing Values}}},
  author = {Mayer, Michael},
  year = 2019,
  urldate = {2021-06-10},
  abstract = {Alternative implementation of the beautiful 'MissForest' algorithm used to impute mixed-type data sets by chaining random forests, introduced by Stekhoven, D.J. and Buehlmann, P. (2012) \&lt;{$<$}a href="https://doi.org/10.1093\%2Fbioinformatics\%2Fbtr597"{$>$}doi:10.1093/bioinformatics/btr597{$<$}/a{$>\&$}gt;. Under the hood, it uses the lightning fast random jungle package 'ranger'. Between the iterative model fitting, we offer the option of using predictive mean matching. This firstly avoids imputation with values not already present in the original data (like a value 0.3334 in 0-1 coded variable). Secondly, predictive mean matching tries to raise the variance in the resulting conditional distributions to a realistic level. This would allow e.g. to do multiple imputation when repeating the call to missRanger(). A formula interface allows to control which variables should be imputed by which.},
  howpublished = {Comprehensive R Archive Network (CRAN)}
}

@misc{mayer_missranger_2019,
  title = {{{missRanger}}: {{Fast Imputation}} of {{Missing Values}}},
  author = {Mayer, Michael},
  year = 2019,
  abstract = {Alternative implementation of the beautiful 'MissForest' algorithm used to impute mixed-type data sets by chaining random forests, introduced by Stekhoven, D.J. and Buehlmann, P. (2012) \&lt;{$<$}a href="https://doi.org/10.1093\%2Fbioinformatics\%2Fbtr597"{$>$}doi:10.1093/bioinformatics/btr597{$<$}/a{$>\&$}gt;. Under the hood, it uses the lightning fast random jungle package 'ranger'. Between the iterative model fitting, we offer the option of using predictive mean matching. This firstly avoids imputation with values not already present in the original data (like a value 0.3334 in 0-1 coded variable). Secondly, predictive mean matching tries to raise the variance in the resulting conditional distributions to a realistic level. This would allow e.g. to do multiple imputation when repeating the call to missRanger(). A formula interface allows to control which variables should be imputed by which.},
  howpublished = {Comprehensive R Archive Network (CRAN)}
}

@techreport{mcconnell_going_2015,
  title = {Going beyond Simple Sample Size Calculations: {{A}} Practitioner's Guide},
  shorttitle = {Going beyond Simple Sample Size Calculations},
  author = {McConnell, Brendon and {Vera-Hernandez}, Marcos},
  year = 2015,
  month = sep,
  institution = {Institute for Fiscal Studies},
  doi = {10.1920/wp.ifs.2015.1517},
  abstract = {Basic methods to compute required sample sizes are well understood and supported by widely available software. However, the sophistication of the methods commonly used has not kept pace with the complexity of commonly employed experimental designs. We compile available methods for sample size calculations for continuous and binary outcomes with and without covariates, for both clustered and non-clustered RCTs. Formulae for both panel data and unbalanced designs are provided. Extensions include methods to: (1) optimise the sample when costs constraints are binding, (2) compute the power of a complex design by simulation, and (3) adjust calculations for multiple testing.},
  langid = {english}
}

@article{mcdowall_interrupted_nodate,
  title = {Interrupted {{Time Series Analysis}}},
  author = {McDowall, David and McCleary, Richard and Bartos, Bradley J},
  pages = {201},
  langid = {english}
}

@article{mcshane_abandon_2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = 2019,
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1527253},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm---and the p-value thresholds intrinsic to it---as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  langid = {english}
}

@article{mcshane_abandon_2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = 2019,
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1527253},
  urldate = {2022-11-17},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm---and the p-value thresholds intrinsic to it---as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  langid = {english}
}

@article{mellon_rain_2021,
  title = {Rain, {{Rain}}, {{Go Away}}: 176 {{Potential Exclusion-Restriction Violations}} for {{Studies Using Weather}} as an {{Instrumental Variable}}},
  author = {Mellon, Jonathan},
  year = 2021,
  month = jul,
  pages = {112},
  abstract = {Instrumental variable (IV) analysis assumes that the instrument only affects the dependent variable via its relationship with the independent variable. Other possible causal routes from the IV to the dependent variable are exclusion-restriction violations and make the instrument invalid. Weather has been widely used as an instrumental variable in social science to predict many different variables. The use of weather to instrument different independent variables represents strong prima facie evidence of exclusion violations for all studies using weather as an IV. A review of 279 studies reveals 176 variables which have been linked to weather: all of which represent potential exclusion violations. I show that the magnitude of several of these violations is sufficient overturn many existing IV results. I conclude with practical steps to systematically review existing literature to identify possible exclusion violations when using IV designs. I demonstrate how sensitivity analysis can quantify the vulnerability of a particular IV estimate to exclusion restriction violations in the literature.},
  langid = {english},
  keywords = {Critique,IV,Rainfall}
}

@article{moretti_pollution_2011,
  title = {Pollution, {{Health}}, and {{Avoidance Behavior}}: {{Evidence}} from the {{Ports}} of {{Los Angeles}}},
  shorttitle = {Pollution, {{Health}}, and {{Avoidance Behavior}}},
  author = {Moretti, Enrico and Neidell, Matthew},
  year = 2011,
  journal = {Journal of Human Resources},
  volume = {46},
  number = {1},
  pages = {154--175},
  issn = {1548-8004},
  doi = {10.1353/jhr.2011.0012},
  urldate = {2020-11-05},
  abstract = {A pervasive problem in estimating the costs of pollution is that optimizing individuals may compensate for increases in pollution by reducing their exposure, resulting in estimates that understate the full welfare costs. To account for this issue, measurement error, and environmental confounding, we estimate the health effects of ozone using daily boat traffic at the port of Los Angeles as an instrumental variable for ozone. We estimate that ozone causes at least \$44 million in annual costs in Los Angeles from respiratory related hospitalizations alone and that the cost of avoidance behavior is at least \$11 million per year.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{moretti_pollution_2011,
  title = {Pollution, {{Health}}, and {{Avoidance Behavior}}: {{Evidence}} from the {{Ports}} of {{Los Angeles}}},
  shorttitle = {Pollution, {{Health}}, and {{Avoidance Behavior}}},
  author = {Moretti, Enrico and Neidell, Matthew},
  year = 2011,
  journal = {Journal of Human Resources},
  volume = {46},
  number = {1},
  pages = {154--175},
  issn = {1548-8004},
  doi = {10.1353/jhr.2011.0012},
  urldate = {2022-11-17},
  abstract = {A pervasive problem in estimating the costs of pollution is that optimizing individuals may compensate for increases in pollution by reducing their exposure, resulting in estimates that understate the full welfare costs. To account for this issue, measurement error, and environmental confounding, we estimate the health effects of ozone using daily boat traffic at the port of Los Angeles as an instrumental variable for ozone. We estimate that ozone causes at least \$44 million in annual costs in Los Angeles from respiratory related hospitalizations alone and that the cost of avoidance behavior is at least \$11 million per year.},
  langid = {english}
}

@article{morgan_counterfactuals_nodate,
  title = {Counterfactuals and {{Causal Inference}}},
  author = {Morgan, Stephen L and Winship, Christopher},
  pages = {526},
  langid = {english}
}

@article{morris_using_2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = 2019,
  month = may,
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8086},
  urldate = {2021-01-29},
  langid = {english}
}

@article{mullins_effects_2015,
  title = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}: {{Evidence}} from {{Santiago}}, {{Chile}}},
  shorttitle = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}},
  author = {Mullins, Jamie and Bharadwaj, Prashant},
  year = 2015,
  month = jul,
  journal = {American Journal of Agricultural Economics},
  volume = {97},
  number = {4},
  pages = {1107--1134},
  issn = {0002-9092, 1467-8276},
  doi = {10.1093/ajae/aau081},
  urldate = {2020-11-05},
  abstract = {It is well established that exposure to high levels of air pollution in the short term leads to negative health outcomes; yet there exist very few policies intended to address short run spikes in air pollution. In this article we examine a policy implemented by the Government of Chile that uses temporary measures to reduce the severity and negative health impacts of poor air quality in the short run. This policy involves the announcement of ``Environmental Episodes'' on days forecast to have particularly poor air quality. Such Episode announcements trigger a number of government protocols and public notices intended to both improve regional air quality and encourage avoidance behaviors among the populace. By comparing days on which Episodes were announced to observationally similar days before the policy was fully implemented, we demonstrate that the announcement of an Environmental Episode reduces ambient concentrations of particulate matter in the Santiago Metropolitan Region by approximately 20\% on the day of implementation, with effects persisting into subsequent days. We also find that the temporary restrictions, government actions, and informational campaigns that make up an Episode reduce mortality among the elderly on the day-of and days-after Episode implementation. Our findings suggest that the Environmental Episode program effectively addresses poor air quality in the short term and could serve as a valuable model for policymakers seeking to augment long-term air quality strategies with a means of addressing temporary spikes in local or regional air pollution levels.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{mullins_effects_2015,
  title = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}: {{Evidence}} from {{Santiago}}, {{Chile}}},
  shorttitle = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}},
  author = {Mullins, Jamie and Bharadwaj, Prashant},
  year = 2015,
  month = jul,
  journal = {American Journal of Agricultural Economics},
  volume = {97},
  number = {4},
  pages = {1107--1134},
  issn = {0002-9092, 1467-8276},
  doi = {10.1093/ajae/aau081},
  urldate = {2022-11-17},
  abstract = {It is well established that exposure to high levels of air pollution in the short term leads to negative health outcomes; yet there exist very few policies intended to address short run spikes in air pollution. In this article we examine a policy implemented by the Government of Chile that uses temporary measures to reduce the severity and negative health impacts of poor air quality in the short run. This policy involves the announcement of ``Environmental Episodes'' on days forecast to have particularly poor air quality. Such Episode announcements trigger a number of government protocols and public notices intended to both improve regional air quality and encourage avoidance behaviors among the populace. By comparing days on which Episodes were announced to observationally similar days before the policy was fully implemented, we demonstrate that the announcement of an Environmental Episode reduces ambient concentrations of particulate matter in the Santiago Metropolitan Region by approximately 20\% on the day of implementation, with effects persisting into subsequent days. We also find that the temporary restrictions, government actions, and informational campaigns that make up an Episode reduce mortality among the elderly on the day-of and days-after Episode implementation. Our findings suggest that the Environmental Episode program effectively addresses poor air quality in the short term and could serve as a valuable model for policymakers seeking to augment long-term air quality strategies with a means of addressing temporary spikes in local or regional air pollution levels.},
  langid = {english}
}

@article{mullins_effects_2015-1,
  title = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}: {{Evidence}} from {{Santiago}}, {{Chile}}},
  shorttitle = {Effects of {{Short}}-{{Term Measures}} to {{Curb Air Pollution}}},
  author = {Mullins, Jamie and Bharadwaj, Prashant},
  year = 2015,
  month = jul,
  journal = {American Journal of Agricultural Economics},
  volume = {97},
  number = {4},
  pages = {1107--1134},
  issn = {0002-9092, 1467-8276},
  doi = {10.1093/ajae/aau081},
  urldate = {2021-06-10},
  abstract = {It is well established that exposure to high levels of air pollution in the short term leads to negative health outcomes; yet there exist very few policies intended to address short run spikes in air pollution. In this article we examine a policy implemented by the Government of Chile that uses temporary measures to reduce the severity and negative health impacts of poor air quality in the short run. This policy involves the announcement of ``Environmental Episodes'' on days forecast to have particularly poor air quality. Such Episode announcements trigger a number of government protocols and public notices intended to both improve regional air quality and encourage avoidance behaviors among the populace. By comparing days on which Episodes were announced to observationally similar days before the policy was fully implemented, we demonstrate that the announcement of an Environmental Episode reduces ambient concentrations of particulate matter in the Santiago Metropolitan Region by approximately 20\% on the day of implementation, with effects persisting into subsequent days. We also find that the temporary restrictions, government actions, and informational campaigns that make up an Episode reduce mortality among the elderly on the day-of and days-after Episode implementation. Our findings suggest that the Environmental Episode program effectively addresses poor air quality in the short term and could serve as a valuable model for policymakers seeking to augment long-term air quality strategies with a means of addressing temporary spikes in local or regional air pollution levels.},
  langid = {english}
}

@article{neidell_air_2004,
  title = {Air Pollution, Health, and Socio-Economic Status: The Effect of Outdoor Air Quality on Childhood Asthma},
  shorttitle = {Air Pollution, Health, and Socio-Economic Status},
  author = {Neidell, Matthew J.},
  year = 2004,
  month = nov,
  journal = {Journal of Health Economics},
  volume = {23},
  number = {6},
  pages = {1209--1236},
  issn = {0167-6296},
  doi = {10.1016/j.jhealeco.2004.05.002},
  urldate = {2020-07-28},
  abstract = {This paper estimates the effect of air pollution on child hospitalizations for asthma using naturally occurring seasonal variations in pollution within zip codes. Of the pollutants considered, carbon monoxide (CO) has a significant effect on asthma for children ages 1--18: if 1998 pollution levels were at their 1992 levels, there would be a 5--14\% increase in asthma admissions. Also, households respond to information about pollution with avoidance behavior, suggesting it is important to account for these endogenous responses when measuring the effect of pollution on health. Finally, the effect of pollution is greater for children of lower socio-economic status (SES), indicating that pollution is one potential mechanism by which SES affects health.},
  langid = {english},
  keywords = {Air pollution,Avoidance,Health}
}

@misc{noauthor_10_nodate,
  title = {10 {{Things}} to {{Know About Cluster Randomization}} -- {{EGAP}}},
  keywords = {Clustering,Overall Problems}
}

@misc{noauthor_201114999_nodate,
  title = {[2011.14999] {{An Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?}
}

@misc{noauthor_210914526_nodate,
  title = {[2109.14526] {{On}} the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science}
}

@misc{noauthor_causal_nodate,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}: {{Journal}} of the {{American Statistical Association}}: {{Vol}} 94, {{No}} 448}
}

@misc{noauthor_mostly_nodate,
  title = {Mostly {{Harmless Econometrics}} -- {{An Empiricist}}`s {{Companion}} : {{Angrist}}, {{Joshua D}}., {{Pischke}}, {{Jorn}}--{{Steffen}}, {{Pischke}}, {{J\~A}}\P rn--{{Steffen}}: {{Amazon}}.{{Fr}}: {{Livres}}}
}

@misc{noauthor_moving_nodate,
  title = {Moving beyond the Classic Difference-in-Differences Model: {{A}} Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies \textbar{} {{BMC Medical Research Methodology}} \textbar{} {{Full Text}}}
}

@book{noauthor_notitle_nodate,
  type = {Book}
}

@book{noauthor_notitle_nodate,
  type = {Book}
}

@misc{noauthor_pubmed2_nodate,
  title = {Pubmed2: Searching {{Pubmed}} for Articles on Public Health and Data Science},
  urldate = {2020-11-23},
  howpublished = {https://rstudio-pubs-static.s3.amazonaws.com/235239\_f75d5005bc0f4015bd97fb4be182144a.html},
  keywords = {PubMed,Systematic literature review,Text mining}
}

@misc{noauthor_shrinkage_nodate,
  title = {The {{Shrinkage Trilogy}}: {{How}} to Be {{Bayesian}} When Analyzing Simple Experiments << {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  urldate = {2020-12-08},
  howpublished = {https://statmodeling.stat.columbia.edu/2020/12/02/the-shrinkage-trilogy-how-to-be-bayesian-in-analyzing-simple-experiments/\#comments}
}

@book{noauthor_statistical_2008,
  title = {Statistical {{Methods}} for {{Environmental Epidemiology}} with {{R}}},
  year = 2008,
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-78167-9},
  urldate = {2022-11-17},
  isbn = {978-0-387-78166-2 978-0-387-78167-9},
  langid = {english}
}

@misc{noauthor_web_nodate,
  title = {Web Scraping - {{Retrieve}} Citations of a Journal Paper Using {{R}}},
  journal = {Stack Overflow},
  urldate = {2021-05-17},
  howpublished = {https://stackoverflow.com/questions/55064193/retrieve-citations-of-a-journal-paper-using-r}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = 2015,
  month = aug,
  journal = {Science (New York, N.Y.)},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aac4716},
  keywords = {Experiments,Power,Psychology,Replications}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = 2015,
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2022-11-17},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science , this issue 10.1126/science.aac4716 , A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. , INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects ( M r = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects ( M r = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results ( P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Original study effect size versus replication effect size (correlation coefficients). Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. , Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english}
}

@article{oster_unobservable_2019,
  title = {Unobservable {{Selection}} and {{Coefficient Stability}}: {{Theory}} and {{Evidence}}},
  shorttitle = {Unobservable {{Selection}} and {{Coefficient Stability}}},
  author = {Oster, Emily},
  year = 2019,
  month = apr,
  journal = {Journal of Business \& Economic Statistics},
  volume = {37},
  number = {2},
  pages = {187--204},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2016.1227711},
  langid = {english}
}

@article{oster2019unobservable,
  title = {Unobservable Selection and Coefficient Stability: {{Theory}} and Evidence},
  author = {Oster, Emily},
  year = 2019,
  journal = {Journal of Business \& Economic Statistics},
  volume = {37},
  number = {2},
  pages = {187--204},
  publisher = {Taylor \& Francis}
}

@inproceedings{pena_novel_2019,
  title = {A Novel Imputation Method for Missing Values in Air Pollutant Time Series Data},
  booktitle = {2019 {{IEEE Latin American Conference}} on {{Computational Intelligence}} ({{LA-CCI}})},
  author = {Pe{\~n}a, Mario and Ortega, Patricia and Orellana, Marcos},
  year = 2019,
  month = nov,
  pages = {1--6},
  doi = {10.1109/LA-CCI47412.2019.9037053},
  abstract = {Missing data is a widespread problem that studies in air quality have to deal with. The causes are varied, including sensor malfunctions and errors, power outages, computer system crashes, pollutant levels lower than detection limits, among others. Existing methods of data cleaning focus more on anomaly detection paying less attention to repairing them. Anomaly detection is a common approach used for filtering out dirty data. However, this approach could still result in unreliable data due to the incomplete data resulting from this process. This article presents an approach for repairing continuous sections of missing values in time series of air pollutant data. This study considers two regularized methods, Lasso and Ridge regression, to determine the number of data points forward and backward needed to estimate the value of a missing data point. Lasso presents superiority in front of Ridge regression to generate models for data imputation. The performance of the method was evaluated using the correlation coefficient (R2), the index agreement (d), the mean absolute error (MAE), and root mean square error (RMSE). The proposed imputation method exhibited good results in terms of accuracy and precision in gaps of different lengths. This study proved that the proposed imputation method is useful to impute accurately missing values in air pollution contaminant datasets, and it has the potential to be applied to any dataset configured as a time series.},
  keywords = {air pollutant time series data,air pollution,air pollution contaminant datasets,air quality,anomaly detection,computer system crashes,correlation coefficient,data analysis,data imputation,data points,detection limits,environmental science computing,Imputation method,index agreement,Lasso,mean absolute error,mean square error methods,missing data point,Missing values,pollutant levels,power outages,regression analysis,regularized methods,Ridge regression,root mean square error,sensor malfunctions,time series,Time series}
}

@article{peng_model_2006,
  title = {Model Choice in Time Series Studies of Air Pollution and Mortality},
  author = {Peng, Roger D. and Dominici, Francesca and Louis, Thomas A.},
  year = 2006,
  month = mar,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {169},
  number = {2},
  pages = {179--203},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/j.1467-985X.2006.00410.x},
  urldate = {2022-11-17},
  abstract = {Multicity time series studies of particulate matter and mortality and morbidity have provided evidence that daily variation in air pollution levels is associated with daily variation in mortality counts. These findings served as key epidemiological evidence for the recent review of the US national ambient air quality standards for particulate matter. As a result, methodological issues concerning time series analysis of the relationship between air pollution and health have attracted the attention of the scientific community and critics have raised concerns about the adequacy of current model formulations. Time series data on pollution and mortality are generally analysed by using log-linear, Poisson regression models for overdispersed counts with the daily number of deaths as outcome, the (possibly lagged) daily level of pollution as a linear predictor and smooth functions of weather variables and calendar time used to adjust for timevarying confounders. Investigators around the world have used different approaches to adjust for confounding, making it difficult to compare results across studies. To date, the statistical properties of these different approaches have not been comprehensively compared. To address these issues, we quantify and characterize model uncertainty and model choice in adjusting for seasonal and long-term trends in time series models of air pollution and mortality. First, we conduct a simulation study to compare and describe the properties of statistical methods that are commonly used for confounding adjustment. We generate data under several confounding scenarios and systematically compare the performance of the various methods with respect to the mean-squared error of the estimated air pollution coefficient. We find that the bias in the estimates generally decreases with more aggressive smoothing and that model selection methods which optimize prediction may not be suitable for obtaining an estimate with small bias. Second, we apply and compare the modelling approaches with the National Morbidity, Mortality, and Air Pollution Study database which comprises daily time series of several pollutants, weather variables and mortality counts covering the period 1987--2000 for the largest 100 cities in the USA. When applying these approaches to adjusting for seasonal and long-term trends we find that the Study's estimates for the national average effect of PM10 at lag 1 on mortality vary over approximately a twofold range, with 95\% posterior intervals always excluding zero risk.},
  langid = {english}
}

@article{peng_r_2004,
  title = {The {{R Journal}}: {{The NMMAPSdata}} Package},
  shorttitle = {The {{R Journal}}},
  author = {Peng, Roger D. and Welty, Leah J.},
  year = 2004,
  month = sep,
  journal = {R News},
  volume = {4},
  number = {2},
  pages = {10--14},
  issn = {1609-3631},
  urldate = {2023-04-25},
  abstract = {"The NMMAPSdata package" published in R News.}
}

@article{peng2006model,
  title = {Model Choice in Time Series Studies of Air Pollution and Mortality},
  author = {Peng, Roger D and Dominici, Francesca and Louis, Thomas A},
  year = 2006,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {169},
  number = {2},
  pages = {179--203},
  publisher = {Wiley Online Library}
}

@article{peng2008statistical,
  title = {Statistical Methods for Environmental Epidemiology with {{R}}},
  author = {Peng, Roger D and Dominici, Francesca},
  year = 2008,
  journal = {R: a case study in air pollution and health},
  publisher = {Springer}
}

@article{pepinsky_note_2018,
  title = {A {{Note}} on {{Listwise Deletion}} versus {{Multiple Imputation}}},
  author = {Pepinsky, Thomas B.},
  year = 2018,
  month = oct,
  journal = {Political Analysis},
  volume = {26},
  number = {4},
  pages = {480--488},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2018.18},
  urldate = {2020-06-17},
  abstract = {This letter compares the performance of multiple imputation and listwise deletion using a simulation approach. The focus is on data that are ``missing not at random'' (MNAR), in which case both multiple imputation and listwise deletion are known to be biased. In these simulations, multiple imputation yields results that are frequently more biased, less e icient, and with worse coverage than listwise deletion when data are MNAR. This is the case even with very strong correlations between fully observed variables and variables with missing values, such that the data are very nearly ``missing at random.'' These results recommend caution when comparing the results from multiple imputation and listwise deletion, when the true data generating process is unknown.},
  langid = {english}
}

@article{pestel_low_nodate,
  title = {Low {{Emission Zones}} for {{Better Health}}: {{Evidence}} from {{German Hospitals}}},
  author = {Pestel, Nico and Wozny, Florian},
  pages = {49},
  abstract = {This paper studies health effects from restricting the access of high-emission vehicles to innercities by implementing low emission zones. For identification, we exploit variation in the timing and the spatial distribution of the introduction of new low emission zones across cities in Germany. We use detailed hospitalization data combined with geo-coded information on the coverage of low emission zones. We confirm that low emission zones significantly reduce levels of air pollution in urban areas and that these improvements in air quality translate into population health benefits. We find that the number of diagnoses related to exposure to air pollution is significantly reduced for hospitals located within or in close proximity to a low emission zone after it becomes effective. In particular, the number of diagnosis for diseases of the circulatory and the respiratory system are significantly reduced.},
  langid = {english}
}

@article{pestel_low_nodate-1,
  title = {Low {{Emission Zones}} for {{Better Health}}: {{Evidence}} from {{German Hospitals}}},
  author = {Pestel, Nico and Wozny, Florian},
  pages = {49},
  abstract = {This paper studies health effects from restricting the access of high-emission vehicles to innercities by implementing low emission zones. For identification, we exploit variation in the timing and the spatial distribution of the introduction of new low emission zones across cities in Germany. We use detailed hospitalization data combined with geo-coded information on the coverage of low emission zones. We confirm that low emission zones significantly reduce levels of air pollution in urban areas and that these improvements in air quality translate into population health benefits. We find that the number of diagnoses related to exposure to air pollution is significantly reduced for hospitals located within or in close proximity to a low emission zone after it becomes effective. In particular, the number of diagnosis for diseases of the circulatory and the respiratory system are significantly reduced.},
  langid = {english}
}

@article{plaia_single_2006,
  title = {Single Imputation Method of Missing Values in Environmental Pollution Data Sets},
  author = {Plaia, A. and Bond{\`i}, A. L.},
  year = 2006,
  month = dec,
  journal = {Atmospheric Environment},
  volume = {40},
  number = {38},
  pages = {7316--7330},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2006.06.040},
  urldate = {2020-07-03},
  abstract = {Missing data represent a general problem in many scientific fields above all in environmental research. Several methods have been proposed in literature for handling missing data and the choice of an appropriate method depends, among others, on the missing data pattern and on the missing-data mechanism. One approach to the problem is to impute them to yield a complete data set. The goal of this paper is to propose a new single imputation method and to compare its performance to other single and multiple imputation methods known in literature. Considering a data set of PM10 concentration measured every 2h by eight monitoring stations distributed over the metropolitan area of Palermo, Sicily, during 2003, simulated incomplete data have been generated, and the performance of the imputation methods have been compared on the correlation coefficient ({$\rho$}), the index of agreement (d), the root mean square deviation (RMSD) and the mean absolute deviation (MAD). All the performance indicators agree to evaluate the proposed method as the best among the ones compared, independently on the gap length and on the number of stations with missing data.},
  langid = {english},
  keywords = {Air pollution,Comparison imputation methods,Imputation,Imputation method,Italy,Missing data,Spatial,Temporal}
}

@article{rani_pubmedminer_2015,
  title = {Pubmed.{{mineR}}: An {{R}} Package with Text-Mining Algorithms to Analyse {{PubMed}} Abstracts},
  shorttitle = {Pubmed.{{mineR}}},
  author = {Rani, Jyoti and Shah, A. B. Rauf and Ramachandran, Srinivasan},
  year = 2015,
  month = oct,
  journal = {Journal of Biosciences},
  volume = {40},
  number = {4},
  pages = {671--682},
  issn = {0973-7138},
  doi = {10.1007/s12038-015-9552-2},
  abstract = {The PubMed literature database is a valuable source of information for scientific research. It is rich in biomedical literature with more than 24 million citations. Data-mining of voluminous literature is a challenging task. Although several text-mining algorithms have been developed in recent years with focus on data visualization, they have limitations such as speed, are rigid and are not available in the open source. We have developed an R package, pubmed.mineR, wherein we have combined the advantages of existing algorithms, overcome their limitations, and offer user flexibility and link with other packages in Bioconductor and the Comprehensive R Network (CRAN) in order to expand the user capabilities for executing multifaceted approaches. Three case studies are presented, namely, 'Evolving role of diabetes educators', 'Cancer risk assessment' and 'Dynamic concepts on disease and comorbidity' to illustrate the use of pubmed.mineR. The package generally runs fast with small elapsed times in regular workstations even on large corpus sizes and with compute intensive functions. The pubmed.mineR is available at http://cran.rproject. org/web/packages/pubmed.mineR.},
  langid = {english},
  pmid = {26564970},
  keywords = {PubMed,Systematic literature review,Text mining}
}

@techreport{ravallion_should_2020,
  type = {Working {{Paper}}},
  title = {Should the {{Randomistas}} ({{Continue}} to) {{Rule}}?},
  author = {Ravallion, Martin},
  year = 2020,
  month = jul,
  series = {Working {{Paper Series}}},
  number = {27554},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w27554},
  abstract = {The rising popularity of randomized controlled trials (RCTs) in development applications has come with continuing debates about the merits of this approach. The paper takes stock of the issues. It argues that an unconditional preference for RCTs is questionable on three main counts. First, the case for such a preference is unclear on a priori grounds. For example, with a given budget, even a biased observational study can come closer to the truth than a costly RCT. Second, the ethical objections to RCTs have not been properly addressed by advocates. Third, there is a risk of distorting the evidence-base for informing policymaking, given that an insistence on RCTs generates selection bias in what gets evaluated. Going forward, pressing knowledge gaps should drive the questions asked and how they are answered, not the methodological preferences of some researchers. The gold standard is the best method for the question at hand.}
}

@article{rich_accountability_2017,
  title = {Accountability Studies of Air Pollution and Health Effects: Lessons Learned and Recommendations for Future Natural Experiment Opportunities},
  shorttitle = {Accountability Studies of Air Pollution and Health Effects},
  author = {Rich, David Q.},
  year = 2017,
  month = mar,
  journal = {Environment International},
  volume = {100},
  pages = {62--78},
  issn = {01604120},
  doi = {10.1016/j.envint.2016.12.019},
  urldate = {2020-06-03},
  abstract = {To address limitations of observational epidemiology studies of air pollution and health effects, including residual confounding by temporal and spatial factors, several studies have taken advantage of `natural experiments', where an environmental policy or air quality intervention has resulted in reductions in ambient air pollution concentrations. Researchers have examined whether the population impacted by these air quality improvements, also experienced improvements in various health indices (e.g. reduced morbidity/mortality). In this paper, I review key accountability studies done previously and new studies done over the past several years in Beijing, Atlanta, London, Ireland, and other locations, describing study design and analysis strengths and limitations of each. As new `natural experiment' opportunities arise, several lessons learned from these studies should be applied when planning a new accountability study. Comparison of health outcomes during the intervention to both before and after the intervention in the population of interest, as well as use of a control population to assess whether any temporal changes in the population of interest were also seen in populations not impacted by air quality improvements, should aid in minimizing residual confounding by these long term time trends. Use of either detailed health records for a population, or prospectively collected data on relevant mechanistic biomarkers coupled with such morbidity/mortality data may provide a more thorough assessment of if the intervention beneficially impacted the health of the community, and if so by what mechanism(s). Further, prospective measurement of a large suite of air pollutants may allow a more thorough understanding of what pollutant source(s) is/ are responsible for any health benefit observed. The importance of using multiple statistical analysis methods in each paper and the difference in how the timing of the air pollution/outcome association may impact which of these design features is most important is also discussed. Based on these and other lessons learned, researchers may provide a more epidemiologically rigorous evaluation of cause-specific health impacts of an air quality intervention or action.},
  langid = {english}
}

@article{rodgers_bootstrap_1999,
  title = {The {{Bootstrap}}, the {{Jackknife}}, and the {{Randomization Test}}: {{A Sampling Taxonomy}}},
  shorttitle = {The {{Bootstrap}}, the {{Jackknife}}, and the {{Randomization Test}}},
  author = {Rodgers, Joseph Lee},
  year = 1999,
  month = oct,
  journal = {Multivariate Behavioral Research},
  volume = {34},
  number = {4},
  pages = {441--456},
  issn = {0027-3171, 1532-7906},
  doi = {10.1207/S15327906MBR3404_2},
  urldate = {2020-03-12},
  langid = {english}
}

@article{rokicki_inference_2018,
  title = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}: {{A Review}}, {{Simulation Study}}, and {{Empirical Application Using SHARE Data}}},
  shorttitle = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}},
  author = {Rokicki, Slawa and Cohen, Jessica and Fink, G{\"u}nther and Salomon, Joshua A. and Landrum, Mary Beth},
  year = 2018,
  month = jan,
  journal = {Medical Care},
  volume = {56},
  number = {1},
  pages = {97--105},
  issn = {1537-1948},
  doi = {10.1097/MLR.0000000000000830},
  abstract = {BACKGROUND: Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. METHODS: First, we review the most commonly used modeling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing, and Retirement in Europe. RESULTS: When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE, and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. CONCLUSIONS: In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.},
  langid = {english},
  pmid = {29112050},
  keywords = {DID}
}

@article{romer_praise_2020,
  title = {In {{Praise}} of {{Confidence Intervals}}},
  author = {Romer, David},
  year = 2020,
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {55--60},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.20201059},
  abstract = {Most empirical papers in economics focus on two aspects of their results: whether the estimates are statistically significantly different from zero and the interpretation of the point estimates. This focus obscures important information about the implications of the results for economically interesting hypotheses about values of the parameters other than zero, and in some cases, about the strength of the evidence against values of zero. This limitation can be overcome by reporting confidence intervals for papers' main estimates and discussing their economic interpretation.},
  langid = {english}
}

@inproceedings{romer2020praise,
  title = {In Praise of Confidence Intervals},
  booktitle = {{{AEA}} Papers and Proceedings},
  author = {Romer, David},
  year = 2020,
  volume = {110},
  pages = {55--60}
}

@article{rosenbaum_covariance_nodate,
  title = {Covariance {{Adjustment}} in {{Randomized Experiments}} and {{Observational Studies}}},
  author = {Rosenbaum, Paul R},
  pages = {42},
  abstract = {By slightly reframing the concept of covariance adjustment in randomized experiments, a method of exact permutation inference is derived that is entirely free of distributional assumptions and uses the random assignment of treatments as the ``reasoned basis for inference.'' This method of exact permutation inference may be used with many forms of covariance adjustment, including robust regression and locally weighted smoothers. The method is then generalized to observational studies where treatments were not randomly assigned, so that sensitivity to hidden biases must be examined. Adjustments using an instrumental variable are also discussed. The methods are illustrated using data from two observational studies.},
  langid = {english}
}

@book{rosenbaum_design_2020,
  title = {Design of {{Observational Studies}}},
  author = {Rosenbaum, Paul R.},
  year = 2020,
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-46405-9},
  isbn = {978-3-030-46404-2 978-3-030-46405-9},
  langid = {english}
}

@book{rosenbaum_design_2020,
  title = {Design of {{Observational Studies}}},
  author = {Rosenbaum, Paul R.},
  year = 2020,
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-46405-9},
  urldate = {2022-11-17},
  isbn = {978-3-030-46404-2 978-3-030-46405-9},
  langid = {english}
}

@book{rosenbaum_observational_2002,
  title = {Observational {{Studies}}},
  author = {Rosenbaum, Paul R.},
  year = 2002,
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-3692-2},
  isbn = {978-1-4419-3191-7 978-1-4757-3692-2},
  langid = {english}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = 1979,
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors}
}

@incollection{roth_lets_1994,
  title = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.: {{A Methodological Note}}},
  shorttitle = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.},
  booktitle = {Experimental {{Economics}}},
  author = {Roth, Alvin E.},
  editor = {Hey, John D.},
  year = 1994,
  series = {Studies in {{Empirical Economics}}},
  pages = {99--109},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-642-51179-0_6},
  abstract = {When Edward Learner (1983) wrote the well known critique of econometric practice whose title I have adapted and adopted, he was concerned that the credibility and utility of econometric research had suffered because of differences between the way econometric research was conducted and the way it was reported1. He wrote (p36--37): ``The econometric art as it is practiced at the computer terminal involves fitting many, perhaps thousands, of statistical models. One or several that the researcher finds pleasing are selected for reporting purposes. This searching for a model is often well intentioned, but there can be no doubt that such a specification search invalidates the traditional theories of inference. The concepts of unbiasedness, consistency, efficiency, maximum-likelihood estimation, in fact, all the concepts of traditional theory, utterly lose their meaning by the time an applied researcher pulls from the bramble of computer output the one thorn of a model he likes best, the one he chooses to portray as a rose. The consuming public is hardly fooled by this chicanery.''},
  isbn = {978-3-642-51179-0},
  langid = {english},
  keywords = {Inflated effects,To read}
}

@article{roth_pre-test_2022,
  title = {Pre-Test with {{Caution}}: {{Event-Study Estimates}} after {{Testing}} for {{Parallel Trends}}},
  shorttitle = {Pre-Test with {{Caution}}},
  author = {Roth, Jonathan},
  year = 2022,
  journal = {American Economic Review: Insights},
  doi = {10.1257/aeri.20210236},
  langid = {english},
  keywords = {Example}
}

@article{rubin_estimating_1974,
  title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.},
  author = {Rubin, Donald B.},
  year = 1974,
  journal = {Journal of Educational Psychology},
  volume = {66},
  number = {5},
  pages = {688--701},
  issn = {0022-0663},
  doi = {10.1037/h0037350},
  urldate = {2021-06-10},
  langid = {english}
}

@article{rubin_estimating_1974,
  title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.},
  author = {Rubin, Donald B.},
  year = 1974,
  month = oct,
  journal = {Journal of Educational Psychology},
  volume = {66},
  number = {5},
  pages = {688--701},
  issn = {1939-2176, 0022-0663},
  doi = {10.1037/h0037350},
  urldate = {2022-11-17},
  langid = {english}
}

@article{rubin_estimating_1974-1,
  title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.},
  author = {Rubin, Donald B.},
  year = 1974,
  journal = {Journal of Educational Psychology},
  volume = {66},
  number = {5},
  pages = {688--701},
  issn = {0022-0663},
  doi = {10.1037/h0037350},
  urldate = {2021-06-10},
  langid = {english}
}

@article{rubin_for_2008,
  title = {For Objective Causal Inference, Design Trumps Analysis},
  author = {Rubin, Donald B.},
  year = 2008,
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  issn = {1932-6157},
  doi = {10.1214/08-AOAS187},
  langid = {english}
}

@article{rubin_inference_1976,
  title = {Inference and Missing Data},
  author = {Rubin, Donald B.},
  year = 1976,
  month = dec,
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  publisher = {Oxford Academic},
  issn = {0006-3444},
  doi = {10.1093/biomet/63.3.581},
  urldate = {2020-07-03},
  abstract = {AbstractSUMMARY.  When making sampling distribution inferences about the parameter of the data, \texttheta, it is appropriate to ignore the process that causes missing d},
  langid = {english}
}

@article{rubin_meta-analysis_nodate,
  title = {Meta-{{Analysis}}: {{Literature Synthesis}} or {{Effect-Size Surface Estimation}}?},
  author = {Rubin, Donald B},
  pages = {12},
  langid = {english}
}

@article{rubin2001using,
  title = {Using Propensity Scores to Help Design Observational Studies: Application to the Tobacco Litigation},
  author = {Rubin, Donald B},
  year = 2001,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {2},
  number = {3},
  pages = {169--188},
  publisher = {Springer}
}

@article{sacks_environmental_2018,
  title = {The {{Environmental Benefits Mapping}} and {{Analysis Program}}~--~{{Community Edition}} ({{BenMAP}}--{{CE}}): {{A}} Tool to Estimate the Health and Economic Benefits of Reducing Air Pollution},
  shorttitle = {The {{Environmental Benefits Mapping}} and {{Analysis Program}}~--~{{Community Edition}} ({{BenMAP}}--{{CE}})},
  author = {Sacks, Jason D. and Lloyd, Jennifer M. and Zhu, Yun and Anderton, Jim and Jang, Carey J. and Hubbell, Bryan and Fann, Neal},
  year = 2018,
  month = jun,
  journal = {Environmental Modelling \& Software},
  volume = {104},
  pages = {118--129},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2018.02.009},
  urldate = {2026-01-29},
  abstract = {A number of software tools exist to estimate the health and economic impacts associated with air quality changes. Over the past 15 years, the U.S. Environmental Protection Agency and its partners invested substantial time and resources in developing the Environmental Benefits Mapping and Analysis Program -- Community Edition (BenMAP--CE). BenMAP--CE is a publicly available, PC-based open source software program that can be configured to conduct health impact assessments to inform air quality policies anywhere in the world. The developers coded the platform in C\# and made the source code available in GitHub, with the goal of building a collaborative relationship with programmers with expertise in other environmental modeling programs. The team recently improved the BenMAP--CE user experience and incorporated new features, while also building a cadre of analysts and BenMAP--CE training instructors in Latin America and Southeast Asia.},
  keywords = {Air pollution,Air quality,Benefits analysis,Health impact assessment,notion,Particulate matter,Policy analysis},
  file = {/Users/vincentbagilet/Zotero/storage/X2UCZZX4/Sacks et al. - 2018 - The Environmental Benefits Mapping and Analysis ProgramCommunity Edition (BenMAPCE) A tool to e.pdf;/Users/vincentbagilet/Zotero/storage/ILBWYJML/S1364815217311866.html}
}

@article{samet_national_2000,
  title = {The {{National Morbidity}}, {{Mortality}}, and {{Air Pollution Study}}. {{Part I}}: {{Methods}} and Methodologic Issues},
  shorttitle = {The {{National Morbidity}}, {{Mortality}}, and {{Air Pollution Study}}. {{Part I}}},
  author = {Samet, J. M. and Dominici, F. and Zeger, S. L. and Schwartz, J. and Dockery, D. W.},
  year = 2000,
  month = jun,
  journal = {Research Report (Health Effects Institute)},
  number = {94 Pt 1},
  pages = {5-14; discussion 75-84},
  issn = {1041-5505},
  abstract = {The Health Effects Institute, established in 1980, is an independent and unbiased source of information on the health effects of motor vehicle emissions. HEI supports research on all major pollutants, including regulated pollutants (such as carbon monoxide, ozone, nitrogen dioxide, and particulate matter) and unregulated pollutants (such as diesel engine exhaust, methanol, and aldehydes). To date, HEI has supported more than 200 projects at institutions in North America and Europe and has published over 100 research reports. Typically, HEI receives half its funds from the US Environmental Protection Agency and half from 28 manufacturers and marketers of motor vehicles and engines in the US. Occasionally, funds from other public and private organizations either support special projects or provide resources for a portion of an HEI study. Regardless of funding sources, HEI exercises complete autonomy in setting its research priorities and in reaching its conclusions. An independent Board of Directors governs HEI. The Institute's Research and Review Committees serve complementary scientific purposes and draw distinguished scientists as members. The results of HEI-funded studies are made available as Research Reports, which contain both the Investigators' Report and the Review Committee's evaluation of the work's scientific quality and regulatory relevance.},
  langid = {english},
  pmid = {11098531},
  keywords = {Air Pollutants,Air Pollution,Bayes Theorem,Humans,Mathematics,Models Statistical,Models Theoretical,Morbidity,Mortality,Regression Analysis,Research Design,Risk,United States,Urban Population}
}

@article{samet2000national,
  title = {The National Morbidity, Mortality, and Air Pollution Study},
  author = {Samet, Jonathan M and Zeger, Scott L and Dominici, Francesca and Curriero, Frank and Coursac, Ivan and Dockery, Douglas W and Schwartz, Joel and Zanobetti, Antonella},
  year = 2000,
  journal = {Part II: morbidity and mortality from air pollution in the United States Res Rep Health Eff Inst},
  volume = {94},
  number = {pt 2},
  pages = {5--79}
}

@article{samoli_what_2014,
  title = {What Is the Impact of Systematically Missing Exposure Data on Air Pollution Health Effect Estimates?},
  author = {Samoli, Evangelia and Peng, Roger D. and Ramsay, Tim and Touloumi, Giota and Dominici, Francesca and Atkinson, Richard W. and Zanobetti, Antonella and Le Tertre, Alain and Anderson, H. Ross and Schwartz, Joel and Cohen, Aaron and Krewski, Daniel and Samet, Jonathan M. and Katsouyanni, Klea},
  year = 2014,
  month = dec,
  journal = {Air Quality, Atmosphere \& Health},
  volume = {7},
  number = {4},
  pages = {415--420},
  issn = {1873-9326},
  doi = {10.1007/s11869-014-0250-2},
  urldate = {2020-07-29},
  abstract = {Time-series studies reporting associations between daily air pollution and health use pollution data from monitoring stations that vary in the frequency of recording. Within the Air Pollution and Health: A European and North American Approach (APHENA) project, we evaluated the impact of systematically missing daily measurements on the estimated effects of PM10 and ozone on daily mortality. For four cities with complete time-series data, we created patterns of systematically missing exposure measurements by deleting observations. Poisson regression-derived city-specific estimates were combined to produce overall effect estimates. Analyses based on incomplete time series gave considerably lower pooled PM10 and ozone health effects compared to those from complete data. City-specific estimates were generally lower although more variable. Systematically missing exposure data for air pollutants appears to lead to underestimation of associated health effects. Our findings indicate that the use of evidence from studies with incomplete exposure data may underestimate the impact of air pollution and highlight the advantage of having complete daily data in time-series studies.},
  langid = {english},
  keywords = {Air pollution,Estimate,Health,Missing data}
}

@techreport{schell_evaluating_2018,
  title = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}: {{A Simulation Study}}},
  shorttitle = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}},
  author = {Schell, Terry L. and Griffin, Beth Ann and Morral, Andrew R.},
  year = 2018,
  month = dec,
  institution = {RAND Corporation},
  abstract = {The authors use simulations to assess the performance of a wide range of statistical models commonly used in the gun policy literature to estimate the effects of state-level gun policies on firearm deaths and to identify the most-appropriate statistical methods for producing estimates. The results suggest substantial statistical problems with many of the methods used in this field. The authors identify the best method among those assessed.},
  langid = {english},
  keywords = {Event study}
}

@book{schell_evaluating_2018,
  title = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}: {{A Simulation Study}}},
  shorttitle = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}},
  author = {Schell, Terry and Griffin, Beth Ann and Morral, Andrew},
  year = 2018,
  publisher = {RAND Corporation},
  doi = {10.7249/RR2685},
  urldate = {2022-11-17},
  isbn = {978-1-9774-0155-7},
  langid = {english}
}

@article{schlenker_airports_2016,
  title = {Airports, {{Air Pollution}}, and {{Contemporaneous Health}}},
  author = {Schlenker, Wolfram and Walker, W. Reed},
  year = 2016,
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {83},
  number = {2},
  pages = {768--809},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdv043},
  urldate = {2020-11-05},
  abstract = {We link daily air pollution exposure to measures of contemporaneous health for communities surrounding the twelve largest airports in California. These airports are some of the largest sources of air pollution in the US, and they experience large changes in daily air pollution emissions depending on the amount of time planes spend idling on the tarmac. Excess airplane idling, measured as residual daily taxi time, is due to network delays originating in the Eastern US. This idiosyncratic variation in daily airplane taxi time significantly impacts the health of local residents, largely driven by increased levels of carbon monoxide (CO) exposure. We use this variation in daily airport congestion to estimate the population doseresponse of health outcomes to daily CO exposure, examining hospitalization rates for asthma, respiratory, and heart-related emergency room admissions. A one standard deviation increase in daily pollution levels leads to an additional \$540 thousand in hospitalization costs for respiratory and heart-related admissions for the 6 million individuals living within 10 km (6.2 miles) of the airports in California. These health effects occur at levels of CO exposure far below existing Environmental Protection Agency mandates, and our results suggest there may be sizable morbidity benefits from lowering the existing CO standard.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{schlenker_airports_2016,
  title = {Airports, {{Air Pollution}}, and {{Contemporaneous Health}}},
  author = {Schlenker, Wolfram and Walker, W. Reed},
  year = 2016,
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {83},
  number = {2},
  pages = {768--809},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdv043},
  urldate = {2022-11-17},
  abstract = {We link daily air pollution exposure to measures of contemporaneous health for communities surrounding the twelve largest airports in California. These airports are some of the largest sources of air pollution in the US, and they experience large changes in daily air pollution emissions depending on the amount of time planes spend idling on the tarmac. Excess airplane idling, measured as residual daily taxi time, is due to network delays originating in the Eastern US. This idiosyncratic variation in daily airplane taxi time significantly impacts the health of local residents, largely driven by increased levels of carbon monoxide (CO) exposure. We use this variation in daily airport congestion to estimate the population doseresponse of health outcomes to daily CO exposure, examining hospitalization rates for asthma, respiratory, and heart-related emergency room admissions. A one standard deviation increase in daily pollution levels leads to an additional \$540 thousand in hospitalization costs for respiratory and heart-related admissions for the 6 million individuals living within 10 km (6.2 miles) of the airports in California. These health effects occur at levels of CO exposure far below existing Environmental Protection Agency mandates, and our results suggest there may be sizable morbidity benefits from lowering the existing CO standard.},
  langid = {english}
}

@article{schlenker_airports_2016-1,
  title = {Airports, {{Air Pollution}}, and {{Contemporaneous Health}}},
  author = {Schlenker, Wolfram and Walker, W. Reed},
  year = 2016,
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {83},
  number = {2},
  pages = {768--809},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdv043},
  urldate = {2021-06-10},
  abstract = {We link daily air pollution exposure to measures of contemporaneous health for communities surrounding the twelve largest airports in California. These airports are some of the largest sources of air pollution in the US, and they experience large changes in daily air pollution emissions depending on the amount of time planes spend idling on the tarmac. Excess airplane idling, measured as residual daily taxi time, is due to network delays originating in the Eastern US. This idiosyncratic variation in daily airplane taxi time significantly impacts the health of local residents, largely driven by increased levels of carbon monoxide (CO) exposure. We use this variation in daily airport congestion to estimate the population doseresponse of health outcomes to daily CO exposure, examining hospitalization rates for asthma, respiratory, and heart-related emergency room admissions. A one standard deviation increase in daily pollution levels leads to an additional \$540 thousand in hospitalization costs for respiratory and heart-related admissions for the 6 million individuals living within 10 km (6.2 miles) of the airports in California. These health effects occur at levels of CO exposure far below existing Environmental Protection Agency mandates, and our results suggest there may be sizable morbidity benefits from lowering the existing CO standard.},
  langid = {english}
}

@article{schochet_statistical_2021,
  title = {Statistical {{Power}} for {{Estimating Treatment Effects Using Difference-in-Differences}} and {{Comparative Interrupted Time Series Designs}} with {{Variation}} in {{Treatment Timing}}},
  author = {Schochet, Peter Z.},
  year = 2021,
  month = oct,
  journal = {arXiv:2102.06770 [econ, stat]},
  eprint = {2102.06770},
  primaryclass = {econ, stat},
  abstract = {This article develops new closed-form variance expressions for power analyses for commonly used difference-in-differences (DID) and comparative interrupted time series (CITS) panel data estimators. The main contribution is to incorporate variation in treatment timing into the analysis. The power formulas also account for other key design features that arise in practice: autocorrelated errors, unequal measurement intervals, and clustering due to the unit of treatment assignment. We consider power formulas for both cross-sectional and longitudinal models and allow for covariates. An illustrative power analysis provides guidance on appropriate sample sizes. The key finding is that accounting for treatment timing increases required sample sizes. Further, DID estimators have considerably more power than standard CITS and ITS estimators. An available Shiny R dashboard performs the sample size calculations for the considered estimators.},
  archiveprefix = {arXiv},
  keywords = {DID}
}

@article{schwartz_estimating_2015,
  title = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}: {{Table}} 1.},
  shorttitle = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}},
  author = {Schwartz, Joel and Austin, Elena and Bind, Marie-Abele and Zanobetti, Antonella and Koutrakis, Petros},
  year = 2015,
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {7},
  pages = {644--650},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwv101},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{schwartz_estimating_2015,
  title = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}: {{Table}} 1.},
  shorttitle = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}},
  author = {Schwartz, Joel and Austin, Elena and Bind, Marie-Abele and Zanobetti, Antonella and Koutrakis, Petros},
  year = 2015,
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {7},
  pages = {644--650},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwv101},
  urldate = {2022-11-17},
  langid = {english}
}

@article{schwartz_estimating_2015-1,
  title = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}: {{Table}} 1.},
  shorttitle = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}},
  author = {Schwartz, Joel and Austin, Elena and Bind, Marie-Abele and Zanobetti, Antonella and Koutrakis, Petros},
  year = 2015,
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {7},
  pages = {644--650},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwv101},
  urldate = {2021-06-10},
  langid = {english}
}

@article{schwartz_estimating_2017,
  title = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}: {{Effect}} of {{Low Levels}}},
  shorttitle = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}},
  author = {Schwartz, Joel and Bind, Marie-Abele and Koutrakis, Petros},
  year = 2017,
  month = jan,
  journal = {Environmental Health Perspectives},
  volume = {125},
  number = {1},
  pages = {23--29},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/EHP232},
  urldate = {2020-11-05},
  abstract = {Background: Although many time-series studies have established associations of daily pollution variations with daily deaths, there are fewer at low concentrations, or focused on locally generated pollution, which is becoming more important as regulations reduce regional transport. Causal modeling approaches are also lacking. Objective: We used causal modeling to estimate the impact of local air pollution on mortality at low concentrations. Methods: Using an instrumental variable approach, we developed an instrument for variations in local pollution concentrations that is unlikely to be correlated with other causes of death, and examined its association with daily deaths in the Boston, Massachusetts, area. We combined height of the planetary boundary layer and wind speed, which affect concentrations of local emissions, to develop the instrument for particulate matter {$\leq$} 2.5 {$\mu$}m (PM2.5), black carbon (BC), or nitrogen dioxide (NO2) variations that were independent of year, month, and temperature. We also used Granger causality to assess whether omitted variable confounding existed. Results: We estimated that an interquartile range increase in the instrument for local PM2.5 was associated with a 0.90\% increase in daily deaths (95\% CI: 0.25, 1.56). A similar result was found for BC, and a weaker association with NO2. The Granger test found no evidence of omitted variable confounding for the instrument. A separate test confirmed the instrument was not associated with mortality independent of pollution. Furthermore, the association remained when all days with PM2.5 concentrations {$>$} 30 {$\mu$}g/m3 were excluded from the analysis (0.84\% increase in daily deaths; 95\% CI: 0.19, 1.50). Conclusions: We conclude that there is a causal association of local air pollution with daily deaths at concentrations below U.S. EPA standards. The estimated attributable risk in Boston exceeded 1,800 deaths during the study period, indicating that important public health benefits can follow from further control efforts.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{schwartz_estimating_2017,
  title = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}: {{Effect}} of {{Low Levels}}},
  shorttitle = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}},
  author = {Schwartz, Joel and Bind, Marie-Abele and Koutrakis, Petros},
  year = 2017,
  month = jan,
  journal = {Environmental Health Perspectives},
  volume = {125},
  number = {1},
  pages = {23--29},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/EHP232},
  urldate = {2022-11-17},
  abstract = {Background: Although many time-series studies have established associations of daily pollution variations with daily deaths, there are fewer at low concentrations, or focused on locally generated pollution, which is becoming more important as regulations reduce regional transport. Causal modeling approaches are also lacking. Objective: We used causal modeling to estimate the impact of local air pollution on mortality at low concentrations. Methods: Using an instrumental variable approach, we developed an instrument for variations in local pollution concentrations that is unlikely to be correlated with other causes of death, and examined its association with daily deaths in the Boston, Massachusetts, area. We combined height of the planetary boundary layer and wind speed, which affect concentrations of local emissions, to develop the instrument for particulate matter {$\leq$} 2.5 {$\mu$}m (PM2.5), black carbon (BC), or nitrogen dioxide (NO2) variations that were independent of year, month, and temperature. We also used Granger causality to assess whether omitted variable confounding existed. Results: We estimated that an interquartile range increase in the instrument for local PM2.5 was associated with a 0.90\% increase in daily deaths (95\% CI: 0.25, 1.56). A similar result was found for BC, and a weaker association with NO2. The Granger test found no evidence of omitted variable confounding for the instrument. A separate test confirmed the instrument was not associated with mortality independent of pollution. Furthermore, the association remained when all days with PM2.5 concentrations {$>$} 30 {$\mu$}g/m3 were excluded from the analysis (0.84\% increase in daily deaths; 95\% CI: 0.19, 1.50). Conclusions: We conclude that there is a causal association of local air pollution with daily deaths at concentrations below U.S. EPA standards. The estimated attributable risk in Boston exceeded 1,800 deaths during the study period, indicating that important public health benefits can follow from further control efforts.},
  langid = {english}
}

@article{schwartz_estimating_2017-1,
  title = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}: {{Effect}} of {{Low Levels}}},
  shorttitle = {Estimating {{Causal Effects}} of {{Local Air Pollution}} on {{Daily Deaths}}},
  author = {Schwartz, Joel and Bind, Marie-Abele and Koutrakis, Petros},
  year = 2017,
  month = jan,
  journal = {Environmental Health Perspectives},
  volume = {125},
  number = {1},
  pages = {23--29},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/EHP232},
  urldate = {2021-06-10},
  abstract = {Background: Although many time-series studies have established associations of daily pollution variations with daily deaths, there are fewer at low concentrations, or focused on locally generated pollution, which is becoming more important as regulations reduce regional transport. Causal modeling approaches are also lacking. Objective: We used causal modeling to estimate the impact of local air pollution on mortality at low concentrations. Methods: Using an instrumental variable approach, we developed an instrument for variations in local pollution concentrations that is unlikely to be correlated with other causes of death, and examined its association with daily deaths in the Boston, Massachusetts, area. We combined height of the planetary boundary layer and wind speed, which affect concentrations of local emissions, to develop the instrument for particulate matter {$\leq$} 2.5 {$\mu$}m (PM2.5), black carbon (BC), or nitrogen dioxide (NO2) variations that were independent of year, month, and temperature. We also used Granger causality to assess whether omitted variable confounding existed. Results: We estimated that an interquartile range increase in the instrument for local PM2.5 was associated with a 0.90\% increase in daily deaths (95\% CI: 0.25, 1.56). A similar result was found for BC, and a weaker association with NO2. The Granger test found no evidence of omitted variable confounding for the instrument. A separate test confirmed the instrument was not associated with mortality independent of pollution. Furthermore, the association remained when all days with PM2.5 concentrations {$>$} 30 {$\mu$}g/m3 were excluded from the analysis (0.84\% increase in daily deaths; 95\% CI: 0.19, 1.50). Conclusions: We conclude that there is a causal association of local air pollution with daily deaths at concentrations below U.S. EPA standards. The estimated attributable risk in Boston exceeded 1,800 deaths during the study period, indicating that important public health benefits can follow from further control efforts.},
  langid = {english}
}

@article{schwartz_national_2018,
  title = {A {{National Multicity Analysis}} of the {{Causal Effect}} of {{Local Pollution}}, {{NO2}}, and {{PM2}}.5 on {{Mortality}}},
  author = {Schwartz, Joel and Fong, Kelvin and Zanobetti, Antonella},
  year = 2018,
  month = aug,
  journal = {Environmental Health Perspectives},
  volume = {126},
  number = {8},
  pages = {087004},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/EHP2732},
  urldate = {2021-06-10},
  abstract = {BACKGROUND: Studies have long associated PM2:5 with daily mortality, but few applied causal-modeling methods, or at low exposures. Short-term exposure to NO2, a marker of local traffic, has also been associated with mortality but is less studied. We previously found a causal effect between local air pollution and mortality in Boston. OBJECTIVES: We aimed to estimate the causal effects of local pollution, PM2:5, and NO2 on mortality in 135 U.S. cities. METHODS: We used three methods which, under different assumptions, provide causal marginal estimates of effect: a marginal structural model, an instrumental variable analysis, and a negative exposure control. The instrumental approach used planetary boundary layer, wind speed, and air pressure as instruments for concentrations of local pollutants; the marginal structural model separated the effects of NO2 from the effects of PM2:5, and the negative exposure control provided protection against unmeasured confounders. RESULTS: In 7.3 million deaths, the instrumental approach estimated that mortality increased 1.5\% [95\% confidence interval (CI): 1.1\%, 2.0\%] per 10 lg=m3 increase in local pollution indexed as PM2:5. The negative control exposure was not associated with mortality. Restricting our analysis to days with PM2:5 below 25 lg=m3, we found a 1.70\% (95\% CI 1.11\%, 2.29\%) increase. With marginal structural models, we found positive significant increases in deaths with both PM2:5 and NO2. On days with PM2:5 below 25 lg=m3, we found a 0.83\% (95\% CI 0.39\%, 1.27\%) increase. Including negative exposure controls changed estimates minimally. CONCLUSIONS: Causal-modeling techniques, each subject to different assumptions, demonstrated causal effects of locally generated pollutants on daily deaths with effects at concentrations below the current EPA daily PM2:5 standard. https://doi.org/10.1289/EHP2732},
  langid = {english}
}

@article{schwartz_national_2018,
  title = {A {{National Multicity Analysis}} of the {{Causal Effect}} of {{Local Pollution}}, {{NO2}}, and {{PM2}}.5 on {{Mortality}}},
  author = {Schwartz, Joel and Fong, Kelvin and Zanobetti, Antonella},
  year = 2018,
  month = aug,
  journal = {Environmental Health Perspectives},
  volume = {126},
  number = {8},
  pages = {087004},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/EHP2732},
  urldate = {2022-11-17},
  abstract = {BACKGROUND: Studies have long associated PM2:5 with daily mortality, but few applied causal-modeling methods, or at low exposures. Short-term exposure to NO2, a marker of local traffic, has also been associated with mortality but is less studied. We previously found a causal effect between local air pollution and mortality in Boston. OBJECTIVES: We aimed to estimate the causal effects of local pollution, PM2:5, and NO2 on mortality in 135 U.S. cities. METHODS: We used three methods which, under different assumptions, provide causal marginal estimates of effect: a marginal structural model, an instrumental variable analysis, and a negative exposure control. The instrumental approach used planetary boundary layer, wind speed, and air pressure as instruments for concentrations of local pollutants; the marginal structural model separated the effects of NO2 from the effects of PM2:5, and the negative exposure control provided protection against unmeasured confounders. RESULTS: In 7.3 million deaths, the instrumental approach estimated that mortality increased 1.5\% [95\% confidence interval (CI): 1.1\%, 2.0\%] per 10 lg=m3 increase in local pollution indexed as PM2:5. The negative control exposure was not associated with mortality. Restricting our analysis to days with PM2:5 below 25 lg=m3, we found a 1.70\% (95\% CI 1.11\%, 2.29\%) increase. With marginal structural models, we found positive significant increases in deaths with both PM2:5 and NO2. On days with PM2:5 below 25 lg=m3, we found a 0.83\% (95\% CI 0.39\%, 1.27\%) increase. Including negative exposure controls changed estimates minimally. CONCLUSIONS: Causal-modeling techniques, each subject to different assumptions, demonstrated causal effects of locally generated pollutants on daily deaths with effects at concentrations below the current EPA daily PM2:5 standard. https://doi.org/10.1289/EHP2732},
  langid = {english}
}

@article{schwartz_what_1994,
  title = {What {{Are People Dying}} of on {{High Air Pollution Days}}?},
  author = {Schwartz, J.},
  year = 1994,
  month = jan,
  journal = {Environmental Research},
  volume = {64},
  number = {1},
  pages = {26--35},
  issn = {0013-9351},
  doi = {10.1006/enrs.1994.1004},
  urldate = {2026-01-28},
  abstract = {The air pollution disasters in London in 1952, the Meuse valley in 1930, and in Donoroa, Pennsylvania, in 1948 made it clear that extremely high levels of particulate-based smog could produce large increases in the daily mortality rate. Recent studies of fluctuations in daily air pollution and daily mortality have reported associations at much lower concentrations in London during the 1960s and in Philadelphia, Steubenville, Santa Clara, St. Louis, Utah valley, Detroit, and eastern Tennessee in the 1970s and 1980s. Whether these associations are causal or not is a matter of considerable public health concern. If the detailed pattern of the deaths at these lower concentrations appeared similar to the pattern in London, this would strengthen the argument for causality. To examine this issue, the death certificates from Philadelphia were examined on the 5\% of the days with the highest particulate air pollution and the 5\% of the days with the lowest particulate air pollution during the years 1973-1980. There was little difference in weather between the high and low pollution days, but total suspended particulate matter concentrations averaged 141 {$\mu$}g/m3 on the high pollution days versus 47 {$\mu$}g/m3 on the low pollution days. The relative risk of dying on the high pollution days was 1.08 P {$<$} 0.0001. The relative increase was higher for COPD (1.25) and pneumonia (1.13). Deaths were also elevated for heart disease and stroke; however, there was a substantial increase in the reports of respiratory factors as contributing causes for those underlying causes of death. Dead-on-arrival deaths and deaths outside of hospitals and clinics were also disproportionately increased. This paralleled the pattern seen in London in 1952. The age pattern of the relative risk of death was also similar. This adds to the evidence that the association is causal.}
}

@article{schwartz1994people,
  title = {What Are People Dying of on High Air Pollution Days?},
  author = {Schwartz, Joel},
  year = 1994,
  journal = {Environmental research},
  volume = {64},
  number = {1},
  pages = {26--35},
  publisher = {Elsevier}
}

@book{shadish_experimental_2002,
  title = {Experimental and {{Quasi-experimental Designs}} for {{Generalized Causal Inference}}},
  author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald Thomas},
  year = 2002,
  publisher = {Houghton Mifflin},
  abstract = {This long awaited successor of the original Cook/Campbell Quasi-Experimentation: Design and Analysis Issues for Field Settings represents updates in the field over the last two decades. The book covers four major topics in field experimentation:Theoretical matters: Experimentation, causation, and validityQuasi-experimental design: Regression discontinuity designs, interrupted time series designs, quasi-experimental designs that use both pretests and control groups, and other designsRandomized experiments: Logic and design issues, and practical problems involving ethics, recruitment, assignment, treatment implementation, and attritionGeneralized causal inference: A grounded theory of generalized causal inference, along with methods for implementing that theory in single and multiple studies},
  googlebooks = {o7jaAAAAMAAJ},
  isbn = {978-0-395-61556-0},
  langid = {english},
  keywords = {Philosophy / Epistemology,Psychology / Experimental Psychology,Psychology / General}
}

@article{shah_short_2015,
  title = {Short Term Exposure to Air Pollution and Stroke: Systematic Review and Meta-Analysis},
  shorttitle = {Short Term Exposure to Air Pollution and Stroke},
  author = {Shah, Anoop S. V. and Lee, Kuan Ken and McAllister, David A. and Hunter, Amanda and Nair, Harish and Whiteley, William and Langrish, Jeremy P. and Newby, David E. and Mills, Nicholas L.},
  year = 2015,
  month = mar,
  journal = {BMJ},
  volume = {350},
  pages = {h1295},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.h1295},
  urldate = {2023-11-07},
  abstract = {Objective To review the evidence for the short term association between air pollution and stroke. Design Systematic review and meta-analysis of observational studies Data sources Medline, Embase, Global Health, Cumulative Index to Nursing and Allied Health Literature (CINAHL), and Web of Science searched to January 2014 with no language restrictions. Eligibility criteria Studies investigating the short term associations (up to lag of seven days) between daily increases in gaseous pollutants (carbon monoxide, sulphur dioxide, nitrogen dioxide, ozone) and particulate matter ({$<$}2.5 \textmu m or {$<$}10 \textmu m diameter (PM2.5 and PM10)), and admission to hospital for stroke or mortality. Main outcome measures Admission to hospital and mortality from stroke. Results From 2748 articles, 238 were reviewed in depth with 103 satisfying our inclusion criteria and 94 contributing to our meta-estimates. This provided a total of 6.2 million events across 28 countries. Admission to hospital for stroke or mortality from stroke was associated with an increase in concentrations of carbon monoxide (relative risk 1.015 per 1 ppm, 95\% confidence interval 1.004 to 1.026), sulphur dioxide (1.019 per 10 ppb, 1.011 to 1.027), and nitrogen dioxide (1.014 per 10 ppb, 1.009 to 1.019). Increases in PM2.5 and PM10 concentration were also associated with admission and mortality (1.011 per 10 {$\mu$}g/m3 (1.011 to 1.012) and 1.003 per 10 \textmu g/m3 (1.002 to 1.004), respectively). The weakest association was seen with ozone (1.001 per 10 ppb, 1.000 to 1.002). Strongest associations were observed on the day of exposure with more persistent effects observed for PM2{$\cdot$}5. Conclusion Gaseous and particulate air pollutants have a marked and close temporal association with admissions to hospital for stroke or mortality from stroke. Public and environmental health policies to reduce air pollution could reduce the burden of stroke. Systematic review registration PROSPERO-CRD42014009225.},
  chapter = {Research},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions .  This is an Open Access article distributed in accordance with the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/ .},
  langid = {english},
  pmid = {25810496},
  keywords = {Air pollution,Literature review,Stroke}
}

@article{shah_short_2015,
  title = {Short Term Exposure to Air Pollution and Stroke: Systematic Review and Meta-Analysis},
  shorttitle = {Short Term Exposure to Air Pollution and Stroke},
  author = {Shah, Anoop S V and Lee, Kuan Ken and McAllister, David A and Hunter, Amanda and Nair, Harish and Whiteley, William and Langrish, Jeremy P and Newby, David E and Mills, Nicholas L},
  year = 2015,
  month = mar,
  journal = {BMJ (Clinical research ed.)},
  pages = {h1295},
  issn = {1756-1833},
  doi = {10.1136/bmj.h1295},
  urldate = {2022-11-17},
  abstract = {Objective To review the evidence for the short term association between air pollution and stroke. Design Systematic review and meta-analysis of observational studies Data sources Medline, Embase, Global Health, Cumulative Index to Nursing and Allied Health Literature (CINAHL), and Web of Science searched to January 2014 with no language restrictions. Eligibility criteria Studies investigating the short term associations (up to lag of seven days) between daily increases in gaseous pollutants (carbon monoxide, sulphur dioxide, nitrogen dioxide, ozone) and particulate matter ({$<$}2.5\>\textmu m or {$<$}10\>\textmu m diameter (PM2.5 and PM10)), and admission to hospital for stroke or mortality. Main outcome measures Admission to hospital and mortality from stroke. Results From 2748 articles, 238 were reviewed in depth with 103 satisfying our inclusion criteria and 94 contributing to our meta-estimates. This provided a total of 6.2 million events across 28 countries. Admission to hospital for stroke or mortality from stroke was associated with an increase in concentrations of carbon monoxide (relative risk 1.015 per 1 ppm, 95\% confidence interval 1.004 to 1.026), sulphur dioxide (1.019 per 10 ppb, 1.011 to 1.027), and nitrogen dioxide (1.014 per 10 ppb, 1.009 to 1.019). Increases in PM2.5 and PM10 concentration were also associated with admission and mortality (1.011 per 10 \^I{$\frac{1}{4}$}g/m3 (1.011 to 1.012) and 1.003 per 10\>\textmu g/m3 (1.002 to 1.004), respectively). The weakest association was seen with ozone (1.001 per 10 ppb, 1.000 to 1.002).},
  langid = {english}
}

@article{sheldon_impact_2017,
  title = {The {{Impact}} of {{Indonesian Forest Fires}} on {{Singaporean Pollution}} and {{Health}}},
  author = {Sheldon, Tamara L. and Sankaran, Chandini},
  year = 2017,
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {526--529},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171134},
  urldate = {2020-11-05},
  abstract = {Forest burning in Indonesia results in severe episodes of ``seasonal haze'' in neighboring Singapore. We offer the first causal analysis of the transboundary health effects of the Indonesian forest burning. Instrumenting for air pollution with satellite fire data, we estimate the impacts of the Indonesian fires on Singaporean polyclinic attendance for acute upper respiratory tract infections and acute conjunctivitis. We find that a one standard deviation increase in the Indonesian fire radiative power increases Singaporean pollution by 1.4 standard deviations and causes a 0.7 standard deviation increase in polyclinic attendance for each of the illnesses examined in this paper.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{sheldon_impact_2017,
  title = {The {{Impact}} of {{Indonesian Forest Fires}} on {{Singaporean Pollution}} and {{Health}}},
  author = {Sheldon, Tamara L. and Sankaran, Chandini},
  year = 2017,
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {526--529},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171134},
  urldate = {2022-11-17},
  abstract = {Forest burning in Indonesia results in severe episodes of ``seasonal haze'' in neighboring Singapore. We offer the first causal analysis of the transboundary health effects of the Indonesian forest burning. Instrumenting for air pollution with satellite fire data, we estimate the impacts of the Indonesian fires on Singaporean polyclinic attendance for acute upper respiratory tract infections and acute conjunctivitis. We find that a one standard deviation increase in the Indonesian fire radiative power increases Singaporean pollution by 1.4 standard deviations and causes a 0.7 standard deviation increase in polyclinic attendance for each of the illnesses examined in this paper.},
  langid = {english}
}

@article{sheldon_impact_2017-1,
  title = {The {{Impact}} of {{Indonesian Forest Fires}} on {{Singaporean Pollution}} and {{Health}}},
  author = {Sheldon, Tamara L. and Sankaran, Chandini},
  year = 2017,
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {526--529},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171134},
  urldate = {2021-06-10},
  abstract = {Forest burning in Indonesia results in severe episodes of ``seasonal haze'' in neighboring Singapore. We offer the first causal analysis of the transboundary health effects of the Indonesian forest burning. Instrumenting for air pollution with satellite fire data, we estimate the impacts of the Indonesian fires on Singaporean polyclinic attendance for acute upper respiratory tract infections and acute conjunctivitis. We find that a one standard deviation increase in the Indonesian fire radiative power increases Singaporean pollution by 1.4 standard deviations and causes a 0.7 standard deviation increase in polyclinic attendance for each of the illnesses examined in this paper.},
  langid = {english}
}

@article{shrader_implications_nodate,
  title = {Implications of Low Power for Estimation at Extreme Values},
  author = {Shrader, Jeffrey},
  pages = {7},
  langid = {english},
  keywords = {Extreme values,Power,Type S/M errors}
}

@article{simeonova_congestion_2019,
  title = {Congestion {{Pricing}}, {{Air Pollution}}, and {{Children}}'s {{Health}}},
  author = {Simeonova, Emilia and Currie, Janet and Nilsson, Peter and Walker, Reed},
  year = 2019,
  month = oct,
  journal = {Journal of Human Resources},
  pages = {0218-9363R2},
  issn = {0022-166X, 1548-8004},
  doi = {10.3368/jhr.56.4.0218-9363R2},
  urldate = {2020-11-05},
  abstract = {This study examines the effects of a congestion tax in central Stockholm on ambient air pollution and the health of local children. We demonstrate that the tax reduced ambient air pollution by 5--15 percent and the rate of acute asthma attacks among young children. We do not see corresponding changes in accidents or hospitalizations for nonrespiratory conditions. As the change in health was more gradual than the change in pollution, it may take time for the full health effects of changes in pollution to materialize if the mechanism is pollution. Hence, short-run estimates of pollution reduction programs may understate long-run health benefits.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{simeonova_congestion_2019-1,
  title = {Congestion {{Pricing}}, {{Air Pollution}}, and {{Children}}'s {{Health}}},
  author = {Simeonova, Emilia and Currie, Janet and Nilsson, Peter and Walker, Reed},
  year = 2019,
  month = oct,
  journal = {Journal of Human Resources},
  pages = {0218-9363R2},
  issn = {0022-166X, 1548-8004},
  doi = {10.3368/jhr.56.4.0218-9363R2},
  urldate = {2021-06-10},
  abstract = {This study examines the effects of a congestion tax in central Stockholm on ambient air pollution and the health of local children. We demonstrate that the tax reduced ambient air pollution by 5--15 percent and the rate of acute asthma attacks among young children. We do not see corresponding changes in accidents or hospitalizations for nonrespiratory conditions. As the change in health was more gradual than the change in pollution, it may take time for the full health effects of changes in pollution to materialize if the mechanism is pollution. Hence, short-run estimates of pollution reduction programs may understate long-run health benefits.},
  langid = {english}
}

@article{simeonova_congestion_2021,
  title = {Congestion {{Pricing}}, {{Air Pollution}}, and {{Children}}'s {{Health}}},
  author = {Simeonova, Emilia and Currie, Janet and Nilsson, Peter and Walker, Reed},
  year = 2021,
  journal = {Journal of Human Resources},
  volume = {56},
  number = {4},
  pages = {971--996},
  issn = {0022-166X, 1548-8004},
  doi = {10.3368/jhr.56.4.0218-9363R2},
  urldate = {2022-11-17},
  abstract = {This study examines the effects of a congestion tax in central Stockholm on ambient air pollution and the health of local children. We demonstrate that the tax reduced ambient air pollution by 5--15 percent and the rate of acute asthma attacks among young children. We do not see corresponding changes in accidents or hospitalizations for nonrespiratory conditions. As the change in health was more gradual than the change in pollution, it may take time for the full health effects of changes in pollution to materialize if the mechanism is pollution. Hence, short-run estimates of pollution reduction programs may understate long-run health benefits.},
  langid = {english}
}

@article{sinharay_respiratory_2018,
  title = {Respiratory and Cardiovascular Responses to Walking down a Traffic-Polluted Road Compared with Walking in a Traffic-Free Area in Participants Aged 60 Years and Older with Chronic Lung or Heart Disease and Age-Matched Healthy Controls: A Randomised, Crossover Study},
  shorttitle = {Respiratory and Cardiovascular Responses to Walking down a Traffic-Polluted Road Compared with Walking in a Traffic-Free Area in Participants Aged 60 Years and Older with Chronic Lung or Heart Disease and Age-Matched Healthy Controls},
  author = {Sinharay, Rudy and Gong, Jicheng and Barratt, Benjamin and {Ohman-Strickland}, Pamela and Ernst, Sabine and Kelly, Frank J and Zhang, Junfeng (Jim) and Collins, Peter and Cullinan, Paul and Chung, Kian Fan},
  year = 2018,
  month = jan,
  journal = {The Lancet},
  volume = {391},
  number = {10118},
  pages = {339--349},
  issn = {01406736},
  doi = {10.1016/S0140-6736(17)32643-0},
  urldate = {2020-03-12},
  abstract = {Background Long-term exposure to pollution can lead to an increase in the rate of decline of lung function, especially in older individuals and in those with chronic obstructive pulmonary disease (COPD), whereas shorter-term exposure at higher pollution levels has been implicated in causing excess deaths from ischaemic heart disease and exacerbations of COPD. We aimed to assess the effects on respiratory and cardiovascular responses of walking down a busy street with high levels of pollution compared with walking in a traffic-free area with lower pollution levels in older adults.},
  langid = {english}
}

@article{sinharay_respiratory_2018-1,
  title = {Respiratory and Cardiovascular Responses to Walking down a Traffic-Polluted Road Compared with Walking in a Traffic-Free Area in Participants Aged 60 Years and Older with Chronic Lung or Heart Disease and Age-Matched Healthy Controls: A Randomised, Crossover Study},
  shorttitle = {Respiratory and Cardiovascular Responses to Walking down a Traffic-Polluted Road Compared with Walking in a Traffic-Free Area in Participants Aged 60 Years and Older with Chronic Lung or Heart Disease and Age-Matched Healthy Controls},
  author = {Sinharay, Rudy and Gong, Jicheng and Barratt, Benjamin and {Ohman-Strickland}, Pamela and Ernst, Sabine and Kelly, Frank J and Zhang, Junfeng (Jim) and Collins, Peter and Cullinan, Paul and Chung, Kian Fan},
  year = 2018,
  month = jan,
  journal = {The Lancet},
  volume = {391},
  number = {10118},
  pages = {339--349},
  issn = {01406736},
  doi = {10.1016/S0140-6736(17)32643-0},
  urldate = {2020-03-12},
  abstract = {Background Long-term exposure to pollution can lead to an increase in the rate of decline of lung function, especially in older individuals and in those with chronic obstructive pulmonary disease (COPD), whereas shorter-term exposure at higher pollution levels has been implicated in causing excess deaths from ischaemic heart disease and exacerbations of COPD. We aimed to assess the effects on respiratory and cardiovascular responses of walking down a busy street with high levels of pollution compared with walking in a traffic-free area with lower pollution levels in older adults.},
  langid = {english}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = 2016,
  month = sep,
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  issn = {2054-5703},
  doi = {10.1098/rsos.160384},
  urldate = {2022-11-17},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  langid = {english}
}

@article{stekhoven_missforest--non-parametric_2012,
  title = {{{MissForest--non-parametric}} Missing Value Imputation for Mixed-Type Data},
  author = {Stekhoven, D. J. and Buhlmann, P.},
  year = 2012,
  month = jan,
  journal = {Bioinformatics},
  volume = {28},
  number = {1},
  pages = {112--118},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btr597},
  urldate = {2021-06-10},
  langid = {english}
}

@article{stommes_reliability_2021,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = 2021,
  month = sep,
  journal = {arXiv:2109.14526 [stat]},
  eprint = {2109.14526},
  primaryclass = {stat},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009--2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {RDD}
}

@techreport{stommes_reliability_2021,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = 2021,
  month = sep,
  number = {arXiv:2109.14526},
  institution = {arXiv},
  urldate = {2022-11-17},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009--2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious.},
  langid = {english},
  keywords = {Statistics - Methodology}
}

@article{stommes_reliability_2023,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = 2023,
  month = apr,
  journal = {Research \& Politics},
  volume = {10},
  number = {2},
  publisher = {SAGE Publications Ltd},
  issn = {2053-1680},
  doi = {10.1177/20531680231166457},
  urldate = {2024-05-16},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it a position as a standard method in modern political science research. But identification does not necessarily imply that causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation under the RD design involves serious statistical challenges and investigate how these challenges manifest themselves in the empirical literature in political science. We collect all RD-based findings published in top political science journals in the period 2009--2018. The distribution of published results exhibits pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher discretion is not a major driver of these features. However, researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design may be exaggerated.},
  langid = {english},
  keywords = {notion,Political Science,Power,Publication bias,RDD}
}

@misc{stommes2021reliability,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = 2021,
  eprint = {2109.14526},
  primaryclass = {stat.ME},
  archiveprefix = {arXiv}
}

@article{stommesReliability2023,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = 2023,
  month = apr,
  journal = {Research \& Politics},
  volume = {10},
  number = {2},
  publisher = {SAGE Publications Ltd},
  issn = {2053-1680},
  doi = {10.1177/20531680231166457},
  urldate = {2024-05-16},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it a position as a standard method in modern political science research. But identification does not necessarily imply that causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation under the RD design involves serious statistical challenges and investigate how these challenges manifest themselves in the empirical literature in political science. We collect all RD-based findings published in top political science journals in the period 2009--2018. The distribution of published results exhibits pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher discretion is not a major driver of these features. However, researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design may be exaggerated.},
  langid = {english},
  keywords = {Political Science,Power,Publication bias,RDD}
}

@article{sukhtankar_replications_2017,
  title = {Replications in {{Development Economics}}},
  author = {Sukhtankar, Sandip},
  year = 2017,
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {32--36},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171120},
  abstract = {I examine replications of empirical papers in development economics published in the top-5 and next-5 general interest journals between the years 2000 through 2015. Of the 1,138 empirical papers, 71 papers (6.2 percent) were replicated in another published paper or working paper. The majority (77.5 percent) of replications involved reanalysis of the data using different econometric specifications to assess robustness. The strongest predictor of whether a paper is replicated or not is the paper's Google Scholar citation count, followed by year of publication. Papers based on randomized control trials (RCTs) appear to be replicated at a higher rate (12.5 percent).},
  langid = {english},
  keywords = {Development economics,Replications}
}

@article{thistlethwaite_regression-discontinuity_1960,
  title = {Regression-Discontinuity Analysis: {{An}} Alternative to the Ex Post Facto Experiment},
  shorttitle = {Regression-Discontinuity Analysis},
  author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
  year = 1960,
  journal = {Journal of Educational Psychology},
  volume = {51},
  number = {6},
  pages = {309--317},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2176(Electronic),0022-0663(Print)},
  doi = {10.1037/h0044319},
  abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners---5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Award,Education,RDD}
}

@misc{timm_retrodesign_2019,
  title = {Retrodesign: {{Tools}} for {{Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Retrodesign},
  author = {Timm, Andrew and Gelman, Andrew and Carlin, John},
  year = 2019,
  month = mar,
  abstract = {Provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) {$<$}doi.org/10.1007/s001800000040{$>$} and Gelman \& Carlin (2014) {$<$}doi.org/10.1177/1745691614551642{$>$}. In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given "tolerances" for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) {$<$}doi.org/10.1111/bmsp.12132{$>$} are implemented. As of 1.0.0, this includes support only for simple research designs. See the package vignette for a fuller exposition on how Type S/M errors arise in research, and how to analyze them using the type of design analysis proposed in the above papers.},
  copyright = {MIT + file LICENSE}
}

@misc{timm_retrodesign_2019,
  title = {Retrodesign: {{Tools}} for {{Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  author = {Timm, Andrew},
  year = 2019,
  month = mar,
  abstract = {Provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) \&lt;doi.org/10.1007/s001800000040\&gt; and Gelman \&amp; Carlin (2014) \&lt;doi.org/10.1177/1745691614551642\&gt;. In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given "tolerances" for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) \&lt;doi.org/10.1111/bmsp.12132\&gt; are implemented. As of 1.0.0, this includes support only for simple research designs. See the package vignette for a fuller exposition on how Type S/M errors arise in research, and how to analyze them using the type of design analysis proposed in the above papers.},
  howpublished = {Comprehensive R Archive Network (CRAN)}
}

@misc{timm_retrodesign_2019a,
  title = {Retrodesign: {{Tools}} for {{Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  author = {Timm, Andrew},
  year = 2019,
  month = mar,
  urldate = {2021-06-10},
  abstract = {Provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) \&lt;doi.org/10.1007/s001800000040\&gt; and Gelman \&amp; Carlin (2014) \&lt;doi.org/10.1177/1745691614551642\&gt;. In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given "tolerances" for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) \&lt;doi.org/10.1111/bmsp.12132\&gt; are implemented. As of 1.0.0, this includes support only for simple research designs. See the package vignette for a fuller exposition on how Type S/M errors arise in research, and how to analyze them using the type of design analysis proposed in the above papers.},
  howpublished = {Comprehensive R Archive Network (CRAN)}
}

@article{trinh_temperature_2019,
  title = {Temperature Inversion and Air Pollution Relationship, and Its Effects on Human Health in {{Hanoi City}}, {{Vietnam}}},
  author = {Trinh, Thi Thuy and Trinh, Thi Tham and Le, Thi Trinh and Nguyen, The Duc Hanh and Tu, Binh Minh},
  year = 2019,
  month = apr,
  journal = {Environmental Geochemistry and Health},
  volume = {41},
  number = {2},
  pages = {929--937},
  issn = {0269-4042, 1573-2983},
  doi = {10.1007/s10653-018-0190-0},
  urldate = {2020-11-05},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@techreport{u.s.epa_environmental_2023,
  title = {Environmental {{Benefits Mapping}} and {{Analysis Program Community Edition}} - {{User}}'s {{Manual}}},
  author = {{U.S. EPA}},
  year = 2023,
  month = mar,
  institution = {U.S. Environmental Protection Agency},
  urldate = {2026-01-30},
  keywords = {notion},
  file = {/Users/vincentbagilet/Zotero/storage/XTWPQFL9/benmap-ce_user_manual_march_2015.pdf}
}

@techreport{u.s.epa_final_2024,
  title = {Final {{Regulatory Impact Analysis}} for the {{Reconsideration}} of the {{National Ambient Air Quality Standards}} for {{Particulate Matter}}},
  author = {{U.S. EPA}},
  year = 2024,
  month = jan,
  number = {EPA-452/R-24-006},
  institution = {U.S. Environmental Protection Agency},
  langid = {english},
  keywords = {notion},
  file = {/Users/vincentbagilet/Zotero/storage/2GMH4EUU/Final Regulatory Impact Analysis for the Reconsideration of the National Ambient Air Quality Standar.pdf}
}

@techreport{u.s.epa_guidelines_2024,
  title = {Guidelines for {{Preparing Economic Analyses}} (3rd Edition)},
  author = {{U.S. EPA}},
  year = 2024,
  number = {EPA-240-R-24-001},
  address = {Washington, DC},
  langid = {english},
  keywords = {notion},
  file = {/Users/vincentbagilet/Zotero/storage/UI4QDMS3/Guidelines for Preparing Economic Analyses - Third Edition.pdf}
}

@misc{u.s.epa_how_2014,
  type = {Overviews and {{Factsheets}}},
  title = {How {{BenMAP-CE Estimates}} the {{Health}} and {{Economic Effects}} of {{Air Pollution}}},
  author = {{U.S. EPA}},
  year = 2014,
  month = oct,
  urldate = {2026-01-28},
  abstract = {The BenMAP-CE tool estimates the number and economic value of health impacts resulting from changes in air quality - specifically, ground-level ozone and fine particles. Learn what data BenMAP-CE uses and how the estimates are calculated.},
  howpublished = {https://www.epa.gov/benmap/how-benmap-ce-estimates-health-and-economic-effects-air-pollution},
  langid = {english},
  keywords = {notion}
}

@techreport{u.s.epa_preamble_2015,
  type = {Reports and {{Assessments}}},
  title = {Preamble {{To The Integrated Science Assessments}} ({{ISA}})},
  author = {{U.S. EPA}},
  year = 2015,
  month = nov,
  number = {EPA/600/R-15/067},
  address = {Washington, DC},
  institution = {U.S. Environmental Protection Agency},
  urldate = {2026-01-29},
  abstract = {The {$<$}em{$>$}Preamble to the Integrated Science Assessments{$<$}/em{$>$}, or "Preamble", is an overview document},
  langid = {english},
  keywords = {notion},
  file = {/Users/vincentbagilet/Zotero/storage/KL3NZW7N/U.S. EPA - 2015 - Preamble To The Integrated Science Assessments (ISA).PDF}
}

@techreport{u.s.epa_us_2019,
  title = {U.{{S}}. {{EPA}}. {{Integrated Science Assessment}} ({{ISA}}) for {{Particulate Matter}} ({{Final Report}}, {{Dec}} 2019)},
  author = {{U.S. EPA}},
  year = 2019,
  number = {EPA/600/R-19/188, 2019},
  address = {Washington, DC},
  langid = {english},
  keywords = {notion}
}

@article{van_buuren_flexible_nodate,
  title = {Flexible {{Imputation}} of {{Missing Data}}, {{Second Edition}}},
  author = {{van Buuren}, Stef and Greenacre, Michael},
  pages = {444},
  langid = {english}
}

@article{vanderweele_sensitivity_2017,
  title = {Sensitivity {{Analysis}} in {{Observational Research}}: {{Introducing}} the {{E-Value}}},
  shorttitle = {Sensitivity {{Analysis}} in {{Observational Research}}},
  author = {VanderWeele, Tyler J. and Ding, Peng},
  year = 2017,
  month = aug,
  journal = {Annals of Internal Medicine},
  volume = {167},
  number = {4},
  pages = {268},
  issn = {0003-4819},
  doi = {10.7326/M16-2607},
  urldate = {2022-11-17},
  langid = {english}
}

@article{vasishth_how_2021,
  title = {How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis},
  author = {Vasishth, Shravan and Gelman, Andrew},
  year = 2021,
  month = sep,
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1311--1342},
  issn = {0024-3949, 1613-396X},
  doi = {10.1515/ling-2019-0051},
  abstract = {The use of statistical inference in linguistics and related areas like psychology typically involves a binary decision: either reject or accept some null hypothesis using statistical significance testing. When statistical power is low, this frequentist data-analytic approach breaks down: null results are uninformative, and effect size estimates associated with significant results are overestimated. Using an example from psycholinguistics, several alternative approaches are demonstrated for reporting inconsistencies between the data and a theoretical prediction. The key here is to focus on committing to a falsifiable prediction, on quantifying uncertainty statistically, and learning to accept the fact that -- in almost all practical data analysis situations -- we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not. A focus on uncertainty quantification is likely to lead to fewer excessively bold claims that, on closer investigation, may turn out to be not supported by the data.},
  langid = {english}
}

@article{vichit-vadakan_public_2008,
  title = {The {{Public Health}} and {{Air Pollution}} in {{Asia}} ({{PAPA}}) {{Project}}: Estimating the Mortality Effects of Particulate Matter in {{Bangkok}}, {{Thailand}}},
  shorttitle = {The {{Public Health}} and {{Air Pollution}} in {{Asia}} ({{PAPA}}) {{Project}}},
  author = {{Vichit-Vadakan}, Nuntavarn and Vajanapoom, Nitaya and Ostro, Bart},
  year = 2008,
  month = sep,
  journal = {Environmental Health Perspectives},
  volume = {116},
  number = {9},
  pages = {1179--1182},
  issn = {0091-6765},
  doi = {10.1289/ehp.10849},
  abstract = {BACKGROUND: Air pollution data in Bangkok, Thailand, indicate that levels of particulate matter with aerodynamic diameter {$<$} or = 10 microm (PM(10)) are significantly higher than in most cities in North America and Western Europe, where the health effects of PM(10) are well documented. However, the pollution mix, seasonality, and demographics are different from those in developed Western countries. It is important, therefore, to determine whether the large metropolitan area of Bangkok is subject to similar effects of PM(10). OBJECTIVES: This study was designed to investigate the mortality risk from air pollution in Bangkok, Thailand. METHODS: The study period extended from 1999 to 2003, for which the Ministry of Public Health provided the mortality data. Measures of air pollution were derived from air monitoring stations, and information on temperature and relative humidity was obtained from the weather station in central Bangkok. The statistical analysis followed the common protocol for the multicity PAPA (Public Health and Air Pollution Project in Asia) project in using a natural cubic spline model with smooths of time and weather. RESULTS: The excess risk for non-accidental mortality was 1.3\% [95\% confidence interval (CI), 0.8-1.7] per 10 microg/m(3) of PM(10), with higher excess risks for cardiovascular and above age 65 mortality of 1.9\% (95\% CI, 0.8-3.0) and 1.5\% (95\% CI, 0.9-2.1), respectively. In addition, the effects from PM(10) appear to be consistent in multipollutant models. CONCLUSIONS: The results suggest strong associations between several different mortality outcomes and PM(10). In many cases, the effect estimates were higher than those typically reported in Western industrialized nations.},
  langid = {english},
  pmcid = {PMC2535619},
  pmid = {18795160},
  keywords = {Adolescent,Adult,Aged,air pollution,Air Pollution,Asia,Bangkok,Child,Child Preschool,Female,Humans,Male,Middle Aged,mortality,Mortality,PM10,Public Health,Risk Factors,Thailand,time series}
}

@article{vichit-vadakan_public_2008,
  title = {The {{Public Health}} and {{Air Pollution}} in {{Asia}} ({{PAPA}}) {{Project}}: {{Estimating}} the {{Mortality Effects}} of {{Particulate Matter}} in {{Bangkok}}, {{Thailand}}},
  shorttitle = {The {{Public Health}} and {{Air Pollution}} in {{Asia}} ({{PAPA}}) {{Project}}},
  author = {{Vichit-Vadakan}, Nuntavarn and Vajanapoom, Nitaya and Ostro, Bart},
  year = 2008,
  month = sep,
  journal = {Environmental Health Perspectives},
  volume = {116},
  number = {9},
  pages = {1179--1182},
  issn = {0091-6765, 1552-9924},
  doi = {10.1289/ehp.10849},
  urldate = {2022-11-17},
  abstract = {BACKGROUND: Air pollution data in Bangkok, Thailand, indicate that levels of particulate matter with aerodynamic diameter {$\leq$} 10 \textmu m (PM10) are significantly higher than in most cities in North America and Western Europe, where the health effects of PM10 are well documented. However, the pollution mix, seasonality, and demographics are different from those in developed Western countries. It is important, therefore, to determine whether the large metropolitan area of Bangkok is subject to similar effects of PM10. OBJECTIVES: This study was designed to investigate the mortality risk from air pollution in Bangkok, Thailand. METHODS: The study period extended from 1999 to 2003, for which the Ministry of Public Health provided the mortality data. Measures of air pollution were derived from air monitoring stations, and information on temperature and relative humidity was obtained from the weather station in central Bangkok. The statistical analysis followed the common protocol for the multicity PAPA (Public Health and Air Pollution Project in Asia) project in using a natural cubic spline model with smooths of time and weather. RESULTS: The excess risk for non-accidental mortality was 1.3\% [95\% confidence interval (CI), 0.8--1.7] per 10 \textmu g/m3 of PM10, with higher excess risks for cardiovascular and above age 65 mortality of 1.9\% (95\% CI, 0.8--3.0) and 1.5\% (95\% CI, 0.9--2.1), respectively. In addition, the effects from PM10 appear to be consistent in multipollutant models. CONCLUSIONS: The results suggest strong associations between several different mortality outcomes and PM10. In many cases, the effect estimates were higher than those typically reported in Western industrialized nations.},
  langid = {english}
}

@article{vivalt_specification_2019,
  title = {Specification {{Searching}} and {{Significance Inflation Across Time}}, {{Methods}} and {{Disciplines}}},
  author = {Vivalt, Eva},
  year = 2019,
  journal = {Oxford Bulletin of Economics and Statistics},
  volume = {81},
  number = {4},
  pages = {797--816},
  issn = {1468-0084},
  doi = {10.1111/obes.12289},
  urldate = {2023-09-22},
  abstract = {This paper examines how significance inflation has varied across time, methods and disciplines. Leveraging a unique data set of impact evaluations on 20 kinds of development programmes, I find that results from randomized controlled trials exhibit less significance inflation than results from studies using other methods. Further, randomized controlled trials have exhibited less significance inflation over time, but quasi-experimental studies have not. There is no robust difference between results from researchers affiliated with economics departments and those from researchers affiliated with other predominantly health-related departments. Overall, the biases found appear much smaller than those previously observed in other social sciences.},
  copyright = {\copyright{} 2019 The Department of Economics, University of Oxford and John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {Publication bias}
}

@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on {\emph{p}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {\emph{p}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = 2016,
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  langid = {english}
}

@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on {{{\emph{p}}}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {{{\emph{p}}}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = 2016,
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2022-11-17},
  langid = {english}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = 2019,
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  langid = {english}
}

@article{williams_short-term_2019,
  title = {Short-Term Impact of {{PM}} {\textsubscript{2.5}} on Contemporaneous Asthma Medication Use: {{Behavior}} and the Value of Pollution Reductions},
  shorttitle = {Short-Term Impact of {{PM}} {\textsubscript{2.5}} on Contemporaneous Asthma Medication Use},
  author = {Williams, Austin M. and Phaneuf, Daniel J. and Barrett, Meredith A. and Su, Jason G.},
  year = 2019,
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {12},
  pages = {5246--5253},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1805647115},
  urldate = {2021-06-10},
  abstract = {Statistics. Table 1 presents summary statistics for the study population. Mean rescue events per day were 0.45 per person. To put this usage in context, we examined data from the 2016 Medical Expenditure Panel Survey and found that users of albuterol, a common SABA medication, fill on average 2.8 prescriptions annually. If these inhalers average enough medication for 100 rescue events, then an upper bound on mean daily events for a nationally representative user would be 0.77. This is an upper bound since filled prescriptions need not imply full usage. Nonetheless, our mean of 0.45 suggests that the intensity of inhaler use among participants in our study is not abnormally high. Mean PM2.5 exposure was 8.7 {$\mu$}g/m3, and median distance of a heartbeat or rescue event to a PM2.5 monitor was 16 km. Participants in the sample who were prescribed controller medications (n = 1,275) took on average 1.3 daily puffs, representing 41\% of the number of daily puffs prescribed.},
  langid = {english}
}

@article{winquist_power_2012,
  title = {Power Estimation Using Simulations for Air Pollution Time-Series Studies},
  author = {Winquist, Andrea and Klein, Mitchel and Tolbert, Paige and Sarnat, Stefanie Ebelt},
  year = 2012,
  month = dec,
  journal = {Environmental Health},
  volume = {11},
  number = {1},
  pages = {68},
  issn = {1476-069X},
  doi = {10.1186/1476-069X-11-68},
  urldate = {2022-11-17},
  abstract = {Background: Estimation of power to assess associations of interest can be challenging for time-series studies of the acute health effects of air pollution because there are two dimensions of sample size (time-series length and daily outcome counts), and because these studies often use generalized linear models to control for complex patterns of covariation between pollutants and time trends, meteorology and possibly other pollutants. In general, statistical software packages for power estimation rely on simplifying assumptions that may not adequately capture this complexity. Here we examine the impact of various factors affecting power using simulations, with comparison of power estimates obtained from simulations with those obtained using statistical software. Methods: Power was estimated for various analyses within a time-series study of air pollution and emergency department visits using simulations for specified scenarios. Mean daily emergency department visit counts, model parameter value estimates and daily values for air pollution and meteorological variables from actual data (8/1/98 to 7/31/99 in Atlanta) were used to generate simulated daily outcome counts with specified temporal associations with air pollutants and randomly generated error based on a Poisson distribution. Power was estimated by conducting analyses of the association between simulated daily outcome counts and air pollution in 2000 data sets for each scenario. Power estimates from simulations and statistical software (G*Power and PASS) were compared. Results: In the simulation results, increasing time-series length and average daily outcome counts both increased power to a similar extent. Our results also illustrate the low power that can result from using outcomes with low daily counts or short time series, and the reduction in power that can accompany use of multipollutant models. Power estimates obtained using standard statistical software were very similar to those from the simulations when properly implemented; implementation, however, was not straightforward. Conclusions: These analyses demonstrate the similar impact on power of increasing time-series length versus increasing daily outcome counts, which has not previously been reported. Implementation of power software for these studies is discussed and guidance is provided.},
  langid = {english}
}

@article{winquist2012power,
  title = {Power Estimation Using Simulations for Air Pollution Time-Series Studies},
  author = {Winquist, Andrea and Klein, Mitchel and Tolbert, Paige and Sarnat, Stefanie Ebelt},
  year = 2012,
  journal = {Environmental Health},
  volume = {11},
  number = {1},
  pages = {1--12},
  publisher = {Springer}
}

@article{wolff_keep_2014,
  title = {Keep {{Your Clunker}} in the {{Suburb}}: {{Low}}-{{Emission Zones}} and {{Adoption}} of {{Green Vehicles}}},
  shorttitle = {Keep {{Your Clunker}} in the {{Suburb}}},
  author = {Wolff, Hendrik},
  year = 2014,
  month = aug,
  journal = {The Economic Journal},
  volume = {124},
  number = {578},
  pages = {F481-F512},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/ecoj.12091},
  urldate = {2020-06-05},
  langid = {english}
}

@article{xia_short-term_2022,
  title = {The Short-Term Impact of Air Pollution on Medical Expenditures: {{Evidence}} from {{Beijing}}},
  shorttitle = {The Short-Term Impact of Air Pollution on Medical Expenditures},
  author = {Xia, Fan and Xing, Jianwei and Xu, Jintao and Pan, Xiaochuan},
  year = 2022,
  month = jul,
  journal = {Journal of Environmental Economics and Management},
  volume = {114},
  pages = {102680},
  issn = {00950696},
  doi = {10.1016/j.jeem.2022.102680},
  urldate = {2022-11-17},
  abstract = {We identify the short-term effects of PM2.5 concentrations on medical costs in Beijing by analyzing two datasets: one detailing daily air quality indexes over a four-year period and the other containing individual-level records of all health care visits and medical transactions that occurred under a government insurance program that covers most city residents. We find that both higher levels of air pollution and longer-lasting pollution episodes significantly increase health care visits and medical expenditures. An analysis of multiple-day pollution episodes shows that marginal health care visits and marginal health costs start to increase as the pollution event lasts for consecutive days. Omitting the variation in the magnitude of the marginal effects of pollution exposure over the course of a pollution episode would lead to the underestimation of the total health costs of air pollution. Our findings provide empirical evidence that both the intensity and the duration of pollution episodes are critical considerations when designing policies to reduce the health costs of air pollution.},
  langid = {english}
}

@article{young_channelling_nodate,
  title = {{{CHANNELLING FISHER}}: {{RANDOMIZATION TESTS AND THE STATISTICAL INSIGNIFICANCE OF SEEMINGLY SIGNIFICANT EXPERIMENTAL RESULTS}}},
  author = {Young, Alwyn},
  pages = {47},
  langid = {english}
}

@article{young_consistency_2022,
  title = {Consistency without {{Inference}}: {{Instrumental Variables}} in {{Practical Application}}},
  shorttitle = {Consistency without {{Inference}}},
  author = {Young, Alwyn},
  year = 2022,
  month = aug,
  journal = {European Economic Review},
  volume = {147},
  pages = {104112},
  issn = {0014-2921},
  doi = {10.1016/j.euroecorev.2022.104112},
  urldate = {2024-03-26},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1309 instrumental variables regressions in 30 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias and mean squared error of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  keywords = {Bootstrap,IV,Jackknife,Literature review,Monte-Carlo,Stephane Bonhomme}
}

@article{young_consistency_2022,
  title = {Consistency without {{Inference}}: {{Instrumental Variables}} in {{Practical Application}}},
  shorttitle = {Consistency without {{Inference}}},
  author = {Young, Alwyn},
  year = 2022,
  month = aug,
  journal = {European Economic Review},
  volume = {147},
  pages = {104112},
  issn = {00142921},
  doi = {10.1016/j.euroecorev.2022.104112},
  urldate = {2022-11-17},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1309 instrumental variables regressions in 30 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias and mean squared error of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  langid = {english}
}

@article{young_leverage_2021,
  title = {Leverage, {{Heteroskedasticity}} and {{Instrumental Variables}} in {{Practical Application}}},
  author = {Young, Alwyn},
  year = 2021,
  month = jun,
  pages = {43},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1359 instrumental variables regressions in 31 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers, statistically significant IV results often depend upon only one or two observations or clusters, IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  langid = {english}
}

@article{yu_missing_2020,
  title = {Missing {{Air Pollution Data Recovery Based}} on {{Long-Short Term Context Encoder}}},
  author = {Yu, Yangwen and Li, Victor O. K. and Lam, Jacqueline C. K.},
  year = 2020,
  journal = {IEEE Transactions on Big Data},
  pages = {1--1},
  issn = {2332-7790},
  doi = {10.1109/TBDATA.2020.2979443},
  abstract = {Air pollution has become a global challenge, and obtaining real-time air quality information is urgently needed. Although the governments have been trying their best in delivering accurate air quality reports, missing air pollution data remains a key challenge. Based on the temporal-spatial correlation of the data, we propose a novel long-short term context encoder (LSCE) structure for recovering missing air pollution data. The original context encoder approach based on image completion focuses on reconstructing rectangular missing regions. Differing from traditional methods, our fully convolutional neural network architecture enjoys the following novelties. First, LSCE can recover irregular missing data patterns. Second, we devise two data pre-processing strategies to produce two types of context encoders, namely, the long-short term cutting context encoder (LSCCE) and the long-short term sliding context encoder (LSSCE). Compared with LSCCE, LSSCE increases the number of training data matrixes. Finally, we investigate the significance of adaptive training in addressing different types of missing data. Our simulation results have demonstrated that our approach, especially, LSSCE, can outperform existing missing data recovery methods. Besides, our techniques can be widely applicable for recovering other temporally and spatially correlated missing data, such as vehicular traffic or meteorology data.},
  keywords = {Air pollution,Comparison imputation methods,Deep learning,Honk hong,Imputation,Imputation method,Method}
}

@article{zhong_superstitious_nodate,
  title = {Superstitious {{Driving Restriction}}: {{Traffic Congestion}}, {{Ambient Air Pollution}}, and {{Health}} in {{Beijing}}},
  author = {Zhong, Nan},
  pages = {51},
  abstract = {Vehicles have recently overtaken coal to become the largest source of air pollution in urban China. Research on mobile sources of pollution has foundered due both to inaccessibility of Chinese data on health outcomes and strong identifying assumptions. To address these, I collect daily ambulance call data from the Beijing Emergency Medical Center and combine them with an idiosyncratic feature of a driving restriction policy in Beijing that references the last digit of vehicles' license plate numbers. Because the number 4 is considered unlucky by many in China, it tends to be avoided on license plates. As a result, days on which the policy restricts license plates ending in 4 unintentionally allow more vehicles in Beijing. Leveraging this variation, I find that traffic congestion is indeed 20\% higher on days banning 4 and that 24-hour average concentration of N O2 is 12\% higher. Correspondingly, these short term increases in pollution increase ambulance calls by 12\% and 3\% for fever and heart related symptoms, while no effects are found for injuries. These estimates are largely unchanged when day of the week fixed effect, weather, and lagged pollution are included. These findings suggest that traffic congestion has substantial health externalities in China but that they are also responsive to policy.},
  langid = {english}
}

@article{zhong_traffic_2017,
  title = {Traffic {{Congestion}}, {{Ambient Air Pollution}}, and {{Health}}: {{Evidence}} from {{Driving Restrictions}} in {{Beijing}}},
  shorttitle = {Traffic {{Congestion}}, {{Ambient Air Pollution}}, and {{Health}}},
  author = {Zhong, Nan and Cao, Jing and Wang, Yuzhu},
  year = 2017,
  month = sep,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {4},
  number = {3},
  pages = {821--856},
  issn = {2333-5955, 2333-5963},
  doi = {10.1086/692115},
  urldate = {2020-11-05},
  abstract = {Vehicles have recently overtaken coal to become the largest source of air pollution in urban China. Research on mobile sources of pollution has foundered due both to inaccessibility of Chinese data on health outcomes and strong identifying assumptions. To address these, we collect daily ambulance call data from the Beijing Emergency Medical Center and combine them with an idiosyncratic feature of a driving restriction policy in Beijing that references the last digit of vehicles' license plate numbers. Because the number 4 is considered unlucky by many in China, it tends to be avoided on license plates. As a result, days on which the policy restricts license plates ending in 4 unintentionally allow more vehicles in Beijing. Leveraging this variation, we find that traffic congestion is {$\ast$}This paper is supported by National Science Foundation of China (project codes: 71422013 and 71173130), Harvard Global Institute, Hang Lung Center for Real Estate, Tsinghua University Research Center for Green Economy and Sustainable Development, Tsinghua University Initiative Scientific Research Program, and Fundamental Research Funds for the Central Universities. We would like to thank Douglas Almond, Geoffrey Heal, Wolfram Schlenker, and two anonymous referees for comments that greatly improved the manuscript. We are also grateful to seminar and conference participants at Columbia University, Chinese University of Hong Kong, Xiamen University, and the 21st Annual Conference of the European Association of Environmental and Resource Economists. All errors are our own.},
  langid = {english},
  keywords = {Lit review air pollution health effects}
}

@article{zhong_traffic_2017,
  title = {Traffic {{Congestion}}, {{Ambient Air Pollution}}, and {{Health}}: {{Evidence}} from {{Driving Restrictions}} in {{Beijing}}},
  shorttitle = {Traffic {{Congestion}}, {{Ambient Air Pollution}}, and {{Health}}},
  author = {Zhong, Nan and Cao, Jing and Wang, Yuzhu},
  year = 2017,
  month = sep,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {4},
  number = {3},
  pages = {821--856},
  issn = {2333-5955, 2333-5963},
  doi = {10.1086/692115},
  urldate = {2022-11-17},
  abstract = {Vehicles have recently overtaken coal to become the largest source of air pollution in urban China. Research on mobile sources of pollution has foundered due both to inaccessibility of Chinese data on health outcomes and strong identifying assumptions. To address these, we collect daily ambulance call data from the Beijing Emergency Medical Center and combine them with an idiosyncratic feature of a driving restriction policy in Beijing that references the last digit of vehicles' license plate numbers. Because the number 4 is considered unlucky by many in China, it tends to be avoided on license plates. As a result, days on which the policy restricts license plates ending in 4 unintentionally allow more vehicles in Beijing. Leveraging this variation, we find that traffic congestion is {$\ast$}This paper is supported by National Science Foundation of China (project codes: 71422013 and 71173130), Harvard Global Institute, Hang Lung Center for Real Estate, Tsinghua University Research Center for Green Economy and Sustainable Development, Tsinghua University Initiative Scientific Research Program, and Fundamental Research Funds for the Central Universities. We would like to thank Douglas Almond, Geoffrey Heal, Wolfram Schlenker, and two anonymous referees for comments that greatly improved the manuscript. We are also grateful to seminar and conference participants at Columbia University, Chinese University of Hong Kong, Xiamen University, and the 21st Annual Conference of the European Association of Environmental and Resource Economists. All errors are our own.},
  langid = {english}
}

@book{ziliak_cult_2008,
  title = {The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives},
  shorttitle = {The Cult of Statistical Significance},
  author = {Ziliak, Stephen Thomas and McCloskey, Deirdre N.},
  year = 2008,
  series = {Economics, Cognition, and Society},
  publisher = {University of Michigan Press},
  address = {Ann Arbor},
  isbn = {978-0-472-07007-7 978-0-472-05007-9},
  langid = {english},
  lccn = {HB137 .Z55 2008},
  keywords = {Statistical power (Leo)},
  annotation = {OCLC: ocn168717577}
}

@book{ziliak_cult_2008,
  title = {The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives},
  shorttitle = {The Cult of Statistical Significance},
  author = {Ziliak, Stephen Thomas and McCloskey, Deirdre N.},
  year = 2008,
  series = {Economics, Cognition, and Society},
  publisher = {University of Michigan Press},
  address = {Ann Arbor},
  isbn = {978-0-472-07007-7 978-0-472-05007-9},
  langid = {english},
  keywords = {Economics,Social aspects,Statistical hypothesis testing,Statistical methods,Statistics}
}

@book{ziliak_cult_2008-1,
  title = {The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives},
  shorttitle = {The Cult of Statistical Significance},
  author = {Ziliak, Stephen Thomas and McCloskey, Deirdre N.},
  year = 2008,
  series = {Economics, Cognition, and Society},
  publisher = {University of Michigan Press},
  address = {Ann Arbor},
  isbn = {978-0-472-07007-7 978-0-472-05007-9},
  langid = {english},
  lccn = {HB137 .Z55 2008},
  keywords = {Economics,Social aspects,Statistical hypothesis testing,Statistical methods,Statistics},
  annotation = {OCLC: ocn168717577}
}

@article{zwet_proposal_2021,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {Zwet, Erik and Gelman, Andrew},
  year = 2021,
  month = jul,
  journal = {The American Statistician},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english}
}

@article{zwet_proposal_2022,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {{van Zwet}, Erik and Gelman, Andrew},
  year = 2022,
  month = jan,
  journal = {The American Statistician},
  volume = {76},
  number = {1},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english}
}

@article{zwet_significance_2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Zwet, Erik W. and Cator, Eric A.},
  year = 2021,
  month = apr,
  journal = {Statistica Neerlandica},
  pages = {stan.12241},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  urldate = {2021-06-10},
  langid = {english}
}

@article{zwet_significance_2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Zwet, Erik W. and Cator, Eric A.},
  year = 2021,
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {75},
  number = {4},
  pages = {437--452},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  langid = {english}
}

@article{zwet_significance_2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Zwet, Erik W. and Cator, Eric A.},
  year = 2021,
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {75},
  number = {4},
  pages = {437--452},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  urldate = {2022-11-17},
  langid = {english}
}

@article{zwet_statistical_2021,
  title = {The Statistical Properties of {{RCTs}} and a Proposal for Shrinkage},
  author = {Zwet, Erik and Schwab, Simon and Senn, Stephen},
  year = 2021,
  month = nov,
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6107--6117},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.9173},
  abstract = {We abstract the concept of a randomized controlled trial as a triple (B, b, s), where B is the primary efficacy parameter, b the estimate, and s the standard error (s {$>$} 0). If the parameter B is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z = b/s and the signal-to-noise ratio SNR = B/s from a sample of pairs (bi, si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on (B, b, s) only through the pair (z, SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of B is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  langid = {english}
}
